{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Y1tHj3DNqhX"
   },
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Entrenamiento de redes *Feed-Forward* vı́a GD y variantes (SGD, mini-*batches*), *momentum*, regularización y tasa de aprendizaje adaptiva.\n",
    "* Evaluación de redes *Feed-Forward* vı́a validación cruzada (cross-validation).\n",
    "* Rol de capas ocultas y mayor profundidad (*Deep Learning*).\n",
    "* Identificar el gradiente desvaneciente.\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas (*cada uno debe estar en condiciones de realizar una presentación y discutir sobre cada punto del trabajo realizado*)\n",
    "* Se debe preparar una presentación de 20 minutos. Presentador será elegido aleatoriamente.\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega y discusión: 26 de Octubre.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<margarita.bugueno.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<cvalle@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea1-INF395-II-2018]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "#### Paquetes instalación\n",
    "\n",
    "Para poder trabajar en el curso se necesitará instalar librerías para Python, por lo que se recomienda instalarlas a través de anaconda (para Windows y sistemas Unix) en un entorno virtual, donde podrán elegir su versión de Python. Se instalarán librerías como sklearn, una librería simple y de facil acceso para data science, keras en su versión con GPU (para cálculo acelerado a través de la tarjeta gráfica), además de que ésta utiliza como backend TensorFlow o Theano, por lo que habrá que instalar alguno de éstos, además de las librerías básicas de computer science como *numpy, matplotlib, pandas,* además de claramente *jupyter*.\n",
    "\n",
    "* Descargar anacona\n",
    "* Luego de instalar Anaconda y tenerla en el path de su computador crear un entorno virtual:\n",
    "```\n",
    "conda create -n redesneuronales python=version\n",
    "```\n",
    "con version, la version de Python que desea utilizar. Si está en Windows, se recomienda Python 3 debido a dependencias con una de las librerías a utilziar.\n",
    "\n",
    "* Acceder al ambiente creado\n",
    "```\n",
    "source activate redesneuronales\n",
    "```\n",
    "\n",
    "* Instalar los paquetes a utilizar\n",
    "```\n",
    "conda install jupyter sklearn numpy pandas matplotlib keras-gpu tensorflow-gpu\n",
    "```\n",
    "\n",
    "*  Para salir del entorno\n",
    "```\n",
    "source deactivate redesneuronales\n",
    "```\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Predicción de Entalpía de Atomización  \n",
    "[2.](#segundo) *Deep Networks*  \n",
    "[3.](#tercero) Entendimiento de imágenes de personas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Predicción de Entalpía de Atomización\n",
    "\n",
    "\n",
    "Las simulaciones de propiedades moleculares son computacionalmente costosas y requieren de un arduo trabajo científico. El objetivo de esta sección corresponde a la utilización de métodos de aprendizaje automático supervisado (Redes Neuronales Artificiales) para predecir propiedades moleculares, en este caso la Energía de Atomización o Entalpía de Atomización, a partir de una base de datos de simulaciones obtenida mediante __[Quantum Espresso](http://www.quantum-espresso.org/)__. Si esto se lograse hacer con gran precisión, se abrirían muchas posibilidades en el diseño computacional y el descubrimiento de nuevas moléculas, compuestos y fármacos.\n",
    "\n",
    "<img src=\"https://pubs.rsc.org/services/images/RSCpubs.ePlatform.Service.FreeContent.ImageService.svc/ImageService/Articleimage/2012/NR/c2nr11543c/c2nr11543c-f4.gif\" title=\"Title text\" width=\"40%\"/>\n",
    "\n",
    "\n",
    "La **entalpía de atomización** es la cantidad de variación de entalpía cuando los enlaces de un compuesto se rompen y los componentes se reducen a átomos individuales. Tal como se ha indicado, su tarea es la de predecir dicho nivel a partir de los atributos enunciados en el dataset puesto a vuestra disposición en *moodle*.\n",
    "\n",
    "> a) Construya un *dataframe* con los datos a analizar y descríbalo brevemete. Además, realice la división de éste en los conjuntos de entrenamiento, validación y testeo correspondientes. Comente por qué se deben eliminar ciertas columnas.\n",
    "```python\n",
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()\n",
    "...\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante\n",
    "```\n",
    ">> a.1) Una buena práctica es la de normalizar los datos antes de trabajar con el modelo. **Explique por qué se aconseja dicho preprocesamiento**\n",
    ">```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "X_val_scaled =  pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "X_test_scaled =  pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "...\n",
    "y_train = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val.pop('Eat').values.reshape(-1,1)\n",
    "...\n",
    "X_train_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_val_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_test_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "```\n",
    "\n",
    " \n",
    ">b) Muestre en un gráfico el error cuadrático (MSE) para el conjunto de entrenamiento y de pruebas vs número de *epochs* de entrenamiento, para una red *feedforward* de 3 capas, con 256 unidades ocultas y función de activación sigmoidal. Entrene la red usando gradiente descendente estocástico con tasa de aprendizaje (learning rate) 0.01 y 250 epochs de entrenamiento, en el conjunto de entrenamiento y de validación. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "```\n",
    "> c) Repita el paso anterior, utilizado ’**ReLU**’ como función de activación y compare con lo obtenido en b).  \n",
    "\n",
    "> d) Repita b) y c) variando la tasa de aprendizaje (*learning rate*) en un rango sensible. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento.\n",
    "```python\n",
    "import numpy as np\n",
    "n_lr = 20\n",
    "lear_rate = np.linspace(0,1,n_lr)\n",
    "```\n",
    "\n",
    "> e) Entrene los modelos considerados en b) y c) usando *progressive decay*. Compare y comente.\n",
    "```python\n",
    "n_decay = 10\n",
    "lear_decay = np.logspace(-6,0,n_decay)\n",
    "sgd = SGD(lr=0.2, decay=1e-6)\n",
    "```\n",
    "\n",
    "> f) Entrene los modelos considerados en b) y c) utilizando SGD en mini-*batches*. Experimente con diferentes tamaños del *batch*. Comente.\n",
    "```python\n",
    "n_batches = 21\n",
    "batch_sizes = np.round(np.linspace(1,X_train_scaled.shape[0],n_batches))\n",
    "model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "```\n",
    "> g) Entrene los modelos obtenidos en b) y c) utilizando estrategias modernas para adaptar la tasa de aprendizaje. Compare los desempeños de adagrad, adadelta, RMSprop y adam. ¿Se observa en algún caso un mejor resultado final? ¿Se observa en algún caso una mayor velocidad de convergencia sobre el dataset de entrenamiento? ¿Sobre el dataset de validación?\n",
    "```python\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "moptimizer = Adagrad(lr=0.01)\n",
    "model.compile(optimizer=moptimizer)\n",
    "model.fit(X_train_scaled,y_train,batch_size=bs,epochs=250,validation_data=(X_val_scaled, y_val))\n",
    "```\n",
    "\n",
    "> h) Entrene los modelos obtenidos en b) y c) utilizando regularizadores $l_1$ y $l_2$ (*weight decay*). Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente. Además evalúe el efecto de regularizar solo la primera capa *vs* la segunda, comente.\n",
    "```python\n",
    "model = Sequential()\n",
    "...#la regularization se debe incorporar a cada capa separadamente\n",
    "idim=X_train_scaled.shape[1]\n",
    "model.add(Dense(256,input_dim=idim,kernel_initializer='uniform',W_regularizer=l2(0.01)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',W_regularizer=l2(0.01)))\n",
    "model.add(Activation('linear'))\n",
    "```\n",
    "\n",
    "> i) Entrene los modelos obtenidos en b) y c) utilizando *Dropout*. Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente.\n",
    "```python\n",
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "...\n",
    "model.add(Dense(256,kernel_initializer='uniform'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "...\n",
    "```\n",
    "\n",
    "> j) Fijando todos los demás hiper-parámetros del modelo definido en b) y en c), utilice validación cruzada con un número de *folds* igual a *K* = 5 y *K*=10 para determinar el mejor valor correspondiente a un parámetro que usted elija (tasa de aprendizaje, número de neuronas, parámetro de regularización, etc) ¿El mejor parámetro para la red con sigmoidal es distinto que para ReLU? ¿Porqué sucede? Además mida el error real del modelo sobre el conjunto de pruebas, compare y concluya.\n",
    "```python\n",
    "from sklearn import cross_validation\n",
    "Xm = X_train_scaled.values\n",
    "ym = y_train\n",
    "kfold = cross_validation.KFold(len(Xm), 10)\n",
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    ...# create model\n",
    "    model = #model with hiperparam\n",
    "    ...# Compile model\n",
    "    model.compile(optimizer=,loss='mean_squared_error')\n",
    "    ...# Fit the model\n",
    "    model.fit(Xm[train], ym[train], epochs=250)\n",
    "    ...# evaluate the model\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    cvscores.append(scores)\n",
    "mse_cv = np.mean(cvscores)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1267</th>\n",
       "      <th>1268</th>\n",
       "      <th>1269</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>pubchem_id</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8139.041805</td>\n",
       "      <td>115.715266</td>\n",
       "      <td>22.445723</td>\n",
       "      <td>20.474191</td>\n",
       "      <td>18.529573</td>\n",
       "      <td>17.169350</td>\n",
       "      <td>15.816888</td>\n",
       "      <td>15.133152</td>\n",
       "      <td>14.471534</td>\n",
       "      <td>13.960759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>33107.484300</td>\n",
       "      <td>-11.178969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4698.182820</td>\n",
       "      <td>113.198503</td>\n",
       "      <td>8.659586</td>\n",
       "      <td>7.670481</td>\n",
       "      <td>6.485777</td>\n",
       "      <td>5.512560</td>\n",
       "      <td>4.179691</td>\n",
       "      <td>3.885091</td>\n",
       "      <td>3.503075</td>\n",
       "      <td>3.357136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.043869</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.024472</td>\n",
       "      <td>23456.785147</td>\n",
       "      <td>3.659133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>2.906146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-23.245373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4068.250000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.969345</td>\n",
       "      <td>16.228071</td>\n",
       "      <td>15.165862</td>\n",
       "      <td>13.744092</td>\n",
       "      <td>13.653146</td>\n",
       "      <td>13.637784</td>\n",
       "      <td>12.759519</td>\n",
       "      <td>12.587359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12298.250000</td>\n",
       "      <td>-13.475805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8142.500000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.662511</td>\n",
       "      <td>18.631287</td>\n",
       "      <td>17.690729</td>\n",
       "      <td>16.020040</td>\n",
       "      <td>15.156646</td>\n",
       "      <td>13.848274</td>\n",
       "      <td>13.659233</td>\n",
       "      <td>13.652832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27731.500000</td>\n",
       "      <td>-10.835211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12207.750000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>21.132432</td>\n",
       "      <td>20.739496</td>\n",
       "      <td>18.712895</td>\n",
       "      <td>18.297501</td>\n",
       "      <td>17.639688</td>\n",
       "      <td>16.154918</td>\n",
       "      <td>15.499474</td>\n",
       "      <td>14.900585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55020.750000</td>\n",
       "      <td>-8.623903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16272.000000</td>\n",
       "      <td>388.023441</td>\n",
       "      <td>73.563510</td>\n",
       "      <td>66.269180</td>\n",
       "      <td>66.268891</td>\n",
       "      <td>66.268756</td>\n",
       "      <td>66.268196</td>\n",
       "      <td>66.264158</td>\n",
       "      <td>66.258487</td>\n",
       "      <td>66.258177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062225</td>\n",
       "      <td>0.061999</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.061534</td>\n",
       "      <td>0.059760</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.057834</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>74980.000000</td>\n",
       "      <td>-0.789513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0             0             1             2             3  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean    8139.041805    115.715266     22.445723     20.474191     18.529573   \n",
       "std     4698.182820    113.198503      8.659586      7.670481      6.485777   \n",
       "min        0.000000     36.858105      2.906146      0.000000      0.000000   \n",
       "25%     4068.250000     73.516695     17.969345     16.228071     15.165862   \n",
       "50%     8142.500000     73.516695     20.662511     18.631287     17.690729   \n",
       "75%    12207.750000     73.516695     21.132432     20.739496     18.712895   \n",
       "max    16272.000000    388.023441     73.563510     66.269180     66.268891   \n",
       "\n",
       "                  4             5             6             7             8  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean      17.169350     15.816888     15.133152     14.471534     13.960759   \n",
       "std        5.512560      4.179691      3.885091      3.503075      3.357136   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       13.744092     13.653146     13.637784     12.759519     12.587359   \n",
       "50%       16.020040     15.156646     13.848274     13.659233     13.652832   \n",
       "75%       18.297501     17.639688     16.154918     15.499474     14.900585   \n",
       "max       66.268756     66.268196     66.264158     66.258487     66.258177   \n",
       "\n",
       "           ...               1267          1268          1269          1270  \\\n",
       "count      ...       16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean       ...           0.000134      0.000133      0.003879      0.000131   \n",
       "std        ...           0.002728      0.002705      0.043869      0.002676   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "max        ...           0.062225      0.061999      0.500000      0.061534   \n",
       "\n",
       "               1271          1272          1273          1274    pubchem_id  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean       0.000129      0.002155      0.000127      0.001201  33107.484300   \n",
       "std        0.002633      0.032755      0.002594      0.024472  23456.785147   \n",
       "min        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000  12298.250000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000  27731.500000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000  55020.750000   \n",
       "max        0.059760      0.500000      0.057834      0.500000  74980.000000   \n",
       "\n",
       "                Eat  \n",
       "count  16242.000000  \n",
       "mean     -11.178969  \n",
       "std        3.659133  \n",
       "min      -23.245373  \n",
       "25%      -13.475805  \n",
       "50%      -10.835211  \n",
       "75%       -8.623903  \n",
       "max       -0.789513  \n",
       "\n",
       "[8 rows x 1278 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primera columna son los indices, la penultima es un public chemical id? no es necesario en este contexto\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "X_val_scaled =  pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "\n",
    "X_test_scaled =  pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "y_train = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val.pop('Eat').values.reshape(-1,1)\n",
    "\n",
    "X_train_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_val_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_test_scaled.drop(columns=['Eat'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 3s 286us/step - loss: 1.5044 - val_loss: 0.5708\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.6075 - val_loss: 0.4474\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.5255 - val_loss: 0.4506\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.4441 - val_loss: 0.3482\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.3841 - val_loss: 0.2987\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.3385 - val_loss: 0.3092\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2965 - val_loss: 0.2494\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.2578 - val_loss: 0.2494\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.2306 - val_loss: 0.2139\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.2081 - val_loss: 0.2031\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1830 - val_loss: 0.2073\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1677 - val_loss: 0.1685\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.1480 - val_loss: 0.1451\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.1364 - val_loss: 0.1931\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1240 - val_loss: 0.1512\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1159 - val_loss: 0.1265\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1056 - val_loss: 0.1157\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0992 - val_loss: 0.1875\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0947 - val_loss: 0.0974\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0881 - val_loss: 0.1296\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0842 - val_loss: 0.0921\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0799 - val_loss: 0.0882\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0763 - val_loss: 0.1282\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0743 - val_loss: 0.0790\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0711 - val_loss: 0.1123\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0675 - val_loss: 0.0787\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0660 - val_loss: 0.0827\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0620 - val_loss: 0.0801\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0619 - val_loss: 0.0686\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0588 - val_loss: 0.0922\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0571 - val_loss: 0.0676\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0566 - val_loss: 0.1003\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0567 - val_loss: 0.0701\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0527 - val_loss: 0.0704\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0519 - val_loss: 0.0641\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0506 - val_loss: 0.0617\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0516 - val_loss: 0.0598\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0495 - val_loss: 0.0715\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0490 - val_loss: 0.0577\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0470 - val_loss: 0.0602\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0466 - val_loss: 0.0697\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.0458 - val_loss: 0.0559\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0456 - val_loss: 0.0655\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 227us/step - loss: 0.0445 - val_loss: 0.0558\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0436 - val_loss: 0.0642\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0424 - val_loss: 0.0600\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 239us/step - loss: 0.0428 - val_loss: 0.0622\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0412 - val_loss: 0.0579\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0412 - val_loss: 0.0739\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0396 - val_loss: 0.0547\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0397 - val_loss: 0.0638\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0375 - val_loss: 0.0591\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0384 - val_loss: 0.0829\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0382 - val_loss: 0.0592\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0386 - val_loss: 0.0569\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0367 - val_loss: 0.0493\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0361 - val_loss: 0.0501\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0362 - val_loss: 0.0486\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0362 - val_loss: 0.0591\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0351 - val_loss: 0.0459\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0347 - val_loss: 0.0506\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0352 - val_loss: 0.0532\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0348 - val_loss: 0.0480\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0332 - val_loss: 0.0492\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0332 - val_loss: 0.0466\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0332 - val_loss: 0.0478\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0328 - val_loss: 0.0498\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0327 - val_loss: 0.0497\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0335 - val_loss: 0.0462\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0329 - val_loss: 0.0477\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0328 - val_loss: 0.0478\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0307 - val_loss: 0.0464\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0310 - val_loss: 0.0504\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0321 - val_loss: 0.0464\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0311 - val_loss: 0.0488\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0310 - val_loss: 0.0554\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0307 - val_loss: 0.0476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0306 - val_loss: 0.0526\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0303 - val_loss: 0.0473\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0294 - val_loss: 0.0501\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0292 - val_loss: 0.0469\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0290 - val_loss: 0.0434\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0295 - val_loss: 0.0415\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0284 - val_loss: 0.0434\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0290 - val_loss: 0.0422\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0303 - val_loss: 0.0428\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0284 - val_loss: 0.0452\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0277 - val_loss: 0.0764\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0282 - val_loss: 0.0501\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0278 - val_loss: 0.1107\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0277 - val_loss: 0.0595\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0269 - val_loss: 0.0463\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0270 - val_loss: 0.0406\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0290 - val_loss: 0.0476\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0258 - val_loss: 0.0436\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 232us/step - loss: 0.0264 - val_loss: 0.0498\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 226us/step - loss: 0.0258 - val_loss: 0.0460\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0262 - val_loss: 0.0494\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0255 - val_loss: 0.0410\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0262 - val_loss: 0.0399\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0254 - val_loss: 0.0461\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0254 - val_loss: 0.0427\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0248 - val_loss: 0.0470\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0250 - val_loss: 0.0424\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0254 - val_loss: 0.0398\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0249 - val_loss: 0.0789\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0246 - val_loss: 0.0437\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0249 - val_loss: 0.0410\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0247 - val_loss: 0.0396\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0245 - val_loss: 0.0435\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0242 - val_loss: 0.0406\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0239 - val_loss: 0.0422\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0233 - val_loss: 0.0395\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0234 - val_loss: 0.0386\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0234 - val_loss: 0.0410\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0227 - val_loss: 0.0398\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0241 - val_loss: 0.0479\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0237 - val_loss: 0.0437\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0223 - val_loss: 0.0423\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0233 - val_loss: 0.0375\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0226 - val_loss: 0.0372\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0224 - val_loss: 0.0519\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0232 - val_loss: 0.0414\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0224 - val_loss: 0.0403\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0226 - val_loss: 0.0373\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0221 - val_loss: 0.0416\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0223 - val_loss: 0.0609\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0226 - val_loss: 0.0402\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0223 - val_loss: 0.0409\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0224 - val_loss: 0.0402\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0219 - val_loss: 0.0472\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0213 - val_loss: 0.0364\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0217 - val_loss: 0.0438\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0223 - val_loss: 0.0366\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0209 - val_loss: 0.0525\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0214 - val_loss: 0.0506\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0210 - val_loss: 0.0367\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0208 - val_loss: 0.0431\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0211 - val_loss: 0.0440\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0214 - val_loss: 0.0379\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0208 - val_loss: 0.0412\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0218 - val_loss: 0.0366\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0205 - val_loss: 0.0374\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0206 - val_loss: 0.0376\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0210 - val_loss: 0.0427\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0200 - val_loss: 0.0491\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0196 - val_loss: 0.0369\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0204 - val_loss: 0.0365\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0198 - val_loss: 0.0493\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0201 - val_loss: 0.0355\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0204 - val_loss: 0.0369\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0200 - val_loss: 0.0357\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 1s 137us/step - loss: 0.0196 - val_loss: 0.0366\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 1s 134us/step - loss: 0.0197 - val_loss: 0.0356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0198 - val_loss: 0.0385\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0198 - val_loss: 0.0360\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0195 - val_loss: 0.0375\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0193 - val_loss: 0.0394\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0196 - val_loss: 0.0353\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0191 - val_loss: 0.0371\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 1s 140us/step - loss: 0.0193 - val_loss: 0.0382\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 1s 136us/step - loss: 0.0191 - val_loss: 0.0349\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0191 - val_loss: 0.0348\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0192 - val_loss: 0.0441\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0190 - val_loss: 0.0346\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0180 - val_loss: 0.0361\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0190 - val_loss: 0.0344\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0186 - val_loss: 0.0368\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0190 - val_loss: 0.0343\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0185 - val_loss: 0.0383\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0189 - val_loss: 0.0377\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 1s 137us/step - loss: 0.0185 - val_loss: 0.0331\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0184 - val_loss: 0.0357\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 1s 146us/step - loss: 0.0187 - val_loss: 0.0345\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0181 - val_loss: 0.0587\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0184 - val_loss: 0.0342\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0186 - val_loss: 0.0393\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0186 - val_loss: 0.0417\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 229us/step - loss: 0.0184 - val_loss: 0.0382\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0179 - val_loss: 0.0387\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0177 - val_loss: 0.0358\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0181 - val_loss: 0.0350\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0181 - val_loss: 0.0393\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0179 - val_loss: 0.0356\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0178 - val_loss: 0.0344\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0174 - val_loss: 0.0346\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0180 - val_loss: 0.0394\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0171 - val_loss: 0.0403\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0173 - val_loss: 0.0332\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 1s 111us/step - loss: 0.0173 - val_loss: 0.0372\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 1s 114us/step - loss: 0.0172 - val_loss: 0.0327\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 1s 107us/step - loss: 0.0169 - val_loss: 0.0342\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 1s 111us/step - loss: 0.0166 - val_loss: 0.0346\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 1s 137us/step - loss: 0.0174 - val_loss: 0.0380\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 1s 134us/step - loss: 0.0174 - val_loss: 0.0344\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 1s 130us/step - loss: 0.0177 - val_loss: 0.0471\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 1s 135us/step - loss: 0.0170 - val_loss: 0.0354\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0171 - val_loss: 0.0350\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0164 - val_loss: 0.0348\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 1s 121us/step - loss: 0.0164 - val_loss: 0.0340\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0171 - val_loss: 0.0320\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 1s 109us/step - loss: 0.0169 - val_loss: 0.0356\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 1s 118us/step - loss: 0.0169 - val_loss: 0.0319\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0162 - val_loss: 0.0337\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0165 - val_loss: 0.0344\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 1s 129us/step - loss: 0.0169 - val_loss: 0.0465\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0169 - val_loss: 0.0331\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 1s 142us/step - loss: 0.0164 - val_loss: 0.0376\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0164 - val_loss: 0.0343\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 1s 144us/step - loss: 0.0166 - val_loss: 0.0366\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0165 - val_loss: 0.0341\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0167 - val_loss: 0.0336\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0158 - val_loss: 0.0381\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0172 - val_loss: 0.0340\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 1s 131us/step - loss: 0.0162 - val_loss: 0.0499\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 1s 132us/step - loss: 0.0168 - val_loss: 0.0360\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0163 - val_loss: 0.0351\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 1s 111us/step - loss: 0.0159 - val_loss: 0.0322\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 1s 109us/step - loss: 0.0161 - val_loss: 0.0323\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 1s 111us/step - loss: 0.0163 - val_loss: 0.0337\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 1s 112us/step - loss: 0.0159 - val_loss: 0.0342\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 1s 136us/step - loss: 0.0157 - val_loss: 0.0324\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0157 - val_loss: 0.0346\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 1s 134us/step - loss: 0.0154 - val_loss: 0.0325\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0159 - val_loss: 0.0341\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 1s 135us/step - loss: 0.0157 - val_loss: 0.0328\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0165 - val_loss: 0.0366\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 1s 129us/step - loss: 0.0160 - val_loss: 0.0326\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0157 - val_loss: 0.0413\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0160 - val_loss: 0.0323\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0147 - val_loss: 0.0335\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 1s 138us/step - loss: 0.0152 - val_loss: 0.0333\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0150 - val_loss: 0.0337\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 1s 131us/step - loss: 0.0151 - val_loss: 0.0343\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 1s 108us/step - loss: 0.0151 - val_loss: 0.0322\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0151 - val_loss: 0.0315\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0151 - val_loss: 0.0393\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0152 - val_loss: 0.0311\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0155 - val_loss: 0.0349\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0148 - val_loss: 0.0323\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0148 - val_loss: 0.0347\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0149 - val_loss: 0.0337\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0151 - val_loss: 0.0321\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0151 - val_loss: 0.0354\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0148 - val_loss: 0.0572\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0150 - val_loss: 0.0322\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0150 - val_loss: 0.0340\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0150 - val_loss: 0.0315\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0150 - val_loss: 0.0323\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0147 - val_loss: 0.0318\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5d451a8ed6b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.loss)\n",
    "plt.hold()\n",
    "plt.plot(history.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-776800f5dbdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2748\u001b[0m     return gca().plot(\n\u001b[0;32m-> 2749\u001b[0;31m         *args, scalex=scalex, scaley=scaley, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   2750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2751\u001b[0m \u001b[0;31m# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1785\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1883\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_line%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1905\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m         \"\"\"\n\u001b[0;32m-> 1907\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1908\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mget_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \"\"\"\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mrecache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0myconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFtCUBp9Nqhb"
   },
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Deep Networks\n",
    "Las *deep network*, o lo que hoy en día se conoce como *deep learning*, hace referencia a modelos de redes neuronales estructurados con muchas capas, es decir, el cómputo de la función final es la composición una gran cantidad de funciones ( $f^{(n)} = f^{(n-1)} \\circ f^{(n-2)} \\circ \\cdots \\circ f^{(2)} \\circ f^{(1)} $ con $n \\gg 0$ ).  \n",
    "Este tipo de redes neuronales tienen una gran cantidad de parámetros, creciendo exponencialmente por capa con las redes *feed forward*, siendo bastante dificiles de entrenar comparadas con una red poco profunda, esto es debido a que requieren una gran cantidad de datos para ajustar correctamente todos esos parámetros. Pero entonces ¿Cuál es el beneficio que tienen este tipo de redes? ¿Qué ganancias trae el añadir capas a una arquitectura de una red neuronal?  \n",
    "\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz36.png\" title=\"Title text\" width=\"80%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "\n",
    "En esta sección se estudiará la complejidad de entrenar redes neuronales profundas, mediante la visualización de los gradientes de los pesos en cada capa, el cómo varía mientras se hace el *backpropagation* hacia las primeras capas de la red. \n",
    "\n",
    "> a) Se trabajará con las etiquetas escaladas uniformemente, es decir, $\\mu=0$ y $\\sigma=1$, ajuste sobre el conjunto de entrenamiento y transforme éstas además de las de validación y pruebas.\n",
    "```python\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "y_train_scaled = X_train_scaled.pop('Eat').values.reshape(-1,1)\n",
    "...#transform val and test\n",
    "```\n",
    "\n",
    "> b) Para el mismo problema definido anteriormente ([sección 1](#primero)) se entrenarán diferentes redes. En esta primera instancia se trabajará con la misma red de la pregunta b), inicializada con pesos uniforme. Visualice el gradiente de la función de pérdida (*loss*) para el conjunto de entrenamiento (promedio del gradiente de cada dato) respecto a los pesos en las distintas capas, para esto se le pedirá el cálculo del gradiente para una capa mediante la función de *gradients* (__[link](https://www.tensorflow.org/api_docs/python/tf/keras/backend/gradients)__) en el *backend* de Keras. Deberá generar un **histograma** para todos los pesos de cada capa antes y despues del entrenamiento con 250 *epochs*. Comente.\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "- ###calculate gradients\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "listOfVariableTensors = model.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "```\n",
    "\n",
    "> c) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento pero ahora entrenando una red mucho más profunda de 6 capas, 5 capas escondidas y 1 de salida. Utilice el inicializador de pesos *uniform* el cual inicializa mediante una distribución uniforme entre $-1/\\sqrt{N}$ y $1/\\sqrt{N}$ para cada capa, con $N$ el número de neuronas de la capa anterior. Por simplicidad visualice las 3-4 primeras capas de la red. Comente si observa el efecto del *gradiente desvaneciente* antes y/o después de entrenar.\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "> d) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento, pero ahora entrenando la red profunda con el inicializador de Glorot [[1]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/(N_{in}+N_{out})}$  y $\\sqrt{6/(N_{in}+N_{out})}$ . Por simplicidad visualice las 3-4 primeras capas de la red. Comente si el efecto del *gradiente desvaneciente* se amortigua antes y/o después de entrenar.\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256,  kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='glorot_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "> e) Vuelva a repetir la experimentación ahora cambiando la función de activación por ReLU, es decir, deberá visualizar los gradientes de los pesos de cada capa antes y después del entrenamiento, con inicialización *uniform* y comparar con la inicialización de He [[2]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/N_{in}}$ y $\\sqrt{6/N_{in}} $. Comente si ocurre el mismo fenómeno anterior (para función sigmoidal) sobre el efecto del *gradiente desvaneciente* para la función ReLU. Explique la importancia de la inicialización de los pesos dependiendo de la arquitectura.\n",
    "```python\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='uniform',activation='relu')) #uniform\n",
    "...\n",
    "or\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='he_uniform',activation='relu')) #he\n",
    "...\n",
    "```\n",
    "> f) ¿Qué es lo que sucede con la red más profunda? ¿El modelo logra convergencia en su entrenamiento? Modifique aspectos estructurales (funciones de activación, inicializadores, regularización, *momentum*, variación de tasa de aprendizaje, entre otros) de la red profunda de 6 capas definida anteriormente (no modifique la profundidad ni el número de neuronas) para lograr un error cuadrático medio (*mse*) similar o menor al de una red no profunda, como la definida en b) en esta sección, sobre el conjunto de pruebas.\n",
    "\n",
    "> g) Experimente con la utilización de una función activación auxiliar (debido a que aproxima) a '**ReLU**' y que es continua derivable (**softplus**) ¿Cuál es el beneficio de ésta con respecto ReLU? Comente.\n",
    "```python\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='he_uniform',activation='softplus')) #softplus\n",
    "...\n",
    "```\n",
    "\n",
    "> h) Pruebe con utilizar una red *shallow* (poco profunda), es decir, sitúe todas las neuronas en una única capa ¿Qué sucede con la convergencia del algoritmo? ¿Por qué sucede este fenómeno?\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=X_train_scaled.shape[1], kernel_initializer='choose',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='choose',activation='linear'))\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "model.fit(X_train_scaled.values, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled.values, y_val_scaled))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLGc9XcDNqhi"
   },
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Entendimiento de imágenes de personas\n",
    "\n",
    "El problema de inferir ciertas características de una persona a través de una foto de ella puede resultar bastante dificil incluso para nosotros, como por ejemplo de qué país es, la emoción que expresa, la edad que tiene, o el género. La automatización de este proceso para que máquinas logren identificar ciertas características de una persona puede ser algo crucial para el futuro desarrollo de Inteligencia Artificial.\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/6B072GE.jpg\" width=\"60%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "En esta actividad trabajaremos con unos datos (imágenes) con la tarea de predecir la **edad** (*target value*) de la persona en la imagen. Los datos con corresponden a 3640 imágenes de Flickr de rostros de personas, pero, debido a que trabajamos con redes *feed forward*, se trabajará con representaciones de características extraídas. Para ésto necesitará descargar los datos del siguiente __[link](http://chenlab.ece.cornell.edu/people/Andy/ImagesOfGroups.html)__ en el extracto de *ageGenderClassification* o a través de la consola Unix.\n",
    "```\n",
    "wget http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/ageGenderClassification.zip\n",
    "```\n",
    "\n",
    "Se trabajará con archivos *.mat* que pueden ser cargados de la siguiente manera:\n",
    "```python\n",
    "import scipy.io as sio\n",
    "sio.loadmat(\"file.mat\")\n",
    "```\n",
    "\n",
    "Para descripción sobre las columnas están en el archivo readme a través del siguiente __[link](http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/README.txt)__ o a través de la consola Unix:\n",
    "```\n",
    "wget http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/README.txt\n",
    "```\n",
    "\n",
    "\n",
    "> a) Cargue los datos dos dataset de entrenamiento y de pruebas ¿Cuántos datos hay en cada conjunto?\n",
    "```python\n",
    "import scipy.io as sio\n",
    "mat_train = sio.loadmat(\"./eventrain.mat\")\n",
    "mat_test = sio.loadmat(\"./eventest.mat\")\n",
    "data_train= mat_train[\"trcoll\"][0][0]\n",
    "data_test= mat_test[\"tecoll\"][0][0]\n",
    "```\n",
    "\n",
    "> b) Eliga cuál representación utilizará para trabajar los datos y entregárselos como *input* al modelo neuronal denso. Además extraiga las etiquetas del problema. Describa los datos utilziados.\n",
    "```python\n",
    "genFeat = data[0]  #it can be used as representation: contextual features\n",
    "ageClass = data[1] #target\n",
    "ffcoefs = data[3]   #it can be used as representation: fisherface space\n",
    "faceGist = data[4]  #it can be used as representation\n",
    "```\n",
    "\n",
    "> c) Defina y entrene una modelo de red neuronal *feed forward* para la inferencia de la edad de la persona a través de la representación escogida. Intente llegar a un *mse* menor a 100 en el conjunto de pruebas. Recuerde que **NO** puede seleccionar modelos a través del conjunto de pruebas. Visualice sus resultados si estima conveniente.\n",
    "\n",
    "\n",
    "*Nota: Puede notar que la cantidad de edades presentes en el problema son pocas (1,  5, 10, 16, 28, 51 o 75 años), por lo que puede tratar al problema así como de regresión o clasificación (considerando cada edad como una clase)*\n",
    "\n",
    "\n",
    "#### Ayuda:\n",
    "> Para problemas de clasificación de múltiples clases es necesario transformar las etiquetas categóricas en *one hot vector*, donde cada columna del vector representará una categoría. Por ejemplo, si existen tres categorías (perro, gato, ratón), la categoría perro puede ser codificada como [1,0,0], y la categoría ratón puede ser codificada como [0,0,1]. Para ésto la librería *keras* nos ayuda:\n",
    "\n",
    "```python\n",
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train,num_classes=edades_distintas)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YAYlvfT-Nqhw"
   },
   "source": [
    "<a id=\"refs\"></a>\n",
    "## Referencias\n",
    "[1] Glorot, X., & Bengio, Y. (2010, March). *Understanding the difficulty of training deep feedforward neural networks*. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256).    \n",
    "[2]  He, K., Zhang, X., Ren, S., & Sun, J. (2015). *Delving deep into rectifiers: Surpassing human-level performance on imagenet classification*. In Proceedings of the IEEE international conference on computer vision (pp. 1026-1034).  \n",
    "[3] Gallagher, A. C., & Chen, T. (2009, June). *Understanding images of groups of people*. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (pp. 256-263). IEEE."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Enunciado.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
