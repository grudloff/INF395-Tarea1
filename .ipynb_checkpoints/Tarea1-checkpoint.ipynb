{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Y1tHj3DNqhX"
   },
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1 - Redes Neuronales y *Deep Learning* </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "[1.](#primero) Predicción de Entalpía de Atomización  \n",
    "[2.](#segundo) *Deep Networks*  \n",
    "[3.](#tercero) Entendimiento de imágenes de personas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Predicción de Entalpía de Atomización\n",
    "\n",
    "\n",
    "Las simulaciones de propiedades moleculares son computacionalmente costosas y requieren de un arduo trabajo científico. El objetivo de esta sección corresponde a la utilización de métodos de aprendizaje automático supervisado (Redes Neuronales Artificiales) para predecir propiedades moleculares, en este caso la Energía de Atomización o Entalpía de Atomización, a partir de una base de datos de simulaciones obtenida mediante __[Quantum Espresso](http://www.quantum-espresso.org/)__. Si esto se lograse hacer con gran precisión, se abrirían muchas posibilidades en el diseño computacional y el descubrimiento de nuevas moléculas, compuestos y fármacos.\n",
    "\n",
    "<img src=\"https://pubs.rsc.org/services/images/RSCpubs.ePlatform.Service.FreeContent.ImageService.svc/ImageService/Articleimage/2012/NR/c2nr11543c/c2nr11543c-f4.gif\" title=\"Title text\" width=\"40%\"/>\n",
    "\n",
    "\n",
    "La **entalpía de atomización** es la cantidad de variación de entalpía cuando los enlaces de un compuesto se rompen y los componentes se reducen a átomos individuales. Tal como se ha indicado, su tarea es la de predecir dicho nivel a partir de los atributos enunciados en el dataset puesto a vuestra disposición en *moodle*.\n",
    "\n",
    "> a) Construya un *dataframe* con los datos a analizar y descríbalo brevemete. Además, realice la división de éste en los conjuntos de entrenamiento, validación y testeo correspondientes. Comente por qué se deben eliminar ciertas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1267</th>\n",
       "      <th>1268</th>\n",
       "      <th>1269</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>pubchem_id</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8139.041805</td>\n",
       "      <td>115.715266</td>\n",
       "      <td>22.445723</td>\n",
       "      <td>20.474191</td>\n",
       "      <td>18.529573</td>\n",
       "      <td>17.169350</td>\n",
       "      <td>15.816888</td>\n",
       "      <td>15.133152</td>\n",
       "      <td>14.471534</td>\n",
       "      <td>13.960759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>33107.484300</td>\n",
       "      <td>-11.178969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4698.182820</td>\n",
       "      <td>113.198503</td>\n",
       "      <td>8.659586</td>\n",
       "      <td>7.670481</td>\n",
       "      <td>6.485777</td>\n",
       "      <td>5.512560</td>\n",
       "      <td>4.179691</td>\n",
       "      <td>3.885091</td>\n",
       "      <td>3.503075</td>\n",
       "      <td>3.357136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.043869</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.024472</td>\n",
       "      <td>23456.785147</td>\n",
       "      <td>3.659133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>2.906146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-23.245373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4068.250000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.969345</td>\n",
       "      <td>16.228071</td>\n",
       "      <td>15.165862</td>\n",
       "      <td>13.744092</td>\n",
       "      <td>13.653146</td>\n",
       "      <td>13.637784</td>\n",
       "      <td>12.759519</td>\n",
       "      <td>12.587359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12298.250000</td>\n",
       "      <td>-13.475805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8142.500000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.662511</td>\n",
       "      <td>18.631287</td>\n",
       "      <td>17.690729</td>\n",
       "      <td>16.020040</td>\n",
       "      <td>15.156646</td>\n",
       "      <td>13.848274</td>\n",
       "      <td>13.659233</td>\n",
       "      <td>13.652832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27731.500000</td>\n",
       "      <td>-10.835211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12207.750000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>21.132432</td>\n",
       "      <td>20.739496</td>\n",
       "      <td>18.712895</td>\n",
       "      <td>18.297501</td>\n",
       "      <td>17.639688</td>\n",
       "      <td>16.154918</td>\n",
       "      <td>15.499474</td>\n",
       "      <td>14.900585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55020.750000</td>\n",
       "      <td>-8.623903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16272.000000</td>\n",
       "      <td>388.023441</td>\n",
       "      <td>73.563510</td>\n",
       "      <td>66.269180</td>\n",
       "      <td>66.268891</td>\n",
       "      <td>66.268756</td>\n",
       "      <td>66.268196</td>\n",
       "      <td>66.264158</td>\n",
       "      <td>66.258487</td>\n",
       "      <td>66.258177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062225</td>\n",
       "      <td>0.061999</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.061534</td>\n",
       "      <td>0.059760</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.057834</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>74980.000000</td>\n",
       "      <td>-0.789513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0             0             1             2             3  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean    8139.041805    115.715266     22.445723     20.474191     18.529573   \n",
       "std     4698.182820    113.198503      8.659586      7.670481      6.485777   \n",
       "min        0.000000     36.858105      2.906146      0.000000      0.000000   \n",
       "25%     4068.250000     73.516695     17.969345     16.228071     15.165862   \n",
       "50%     8142.500000     73.516695     20.662511     18.631287     17.690729   \n",
       "75%    12207.750000     73.516695     21.132432     20.739496     18.712895   \n",
       "max    16272.000000    388.023441     73.563510     66.269180     66.268891   \n",
       "\n",
       "                  4             5             6             7             8  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean      17.169350     15.816888     15.133152     14.471534     13.960759   \n",
       "std        5.512560      4.179691      3.885091      3.503075      3.357136   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       13.744092     13.653146     13.637784     12.759519     12.587359   \n",
       "50%       16.020040     15.156646     13.848274     13.659233     13.652832   \n",
       "75%       18.297501     17.639688     16.154918     15.499474     14.900585   \n",
       "max       66.268756     66.268196     66.264158     66.258487     66.258177   \n",
       "\n",
       "           ...               1267          1268          1269          1270  \\\n",
       "count      ...       16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean       ...           0.000134      0.000133      0.003879      0.000131   \n",
       "std        ...           0.002728      0.002705      0.043869      0.002676   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "max        ...           0.062225      0.061999      0.500000      0.061534   \n",
       "\n",
       "               1271          1272          1273          1274    pubchem_id  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean       0.000129      0.002155      0.000127      0.001201  33107.484300   \n",
       "std        0.002633      0.032755      0.002594      0.024472  23456.785147   \n",
       "min        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000  12298.250000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000  27731.500000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000  55020.750000   \n",
       "max        0.059760      0.500000      0.057834      0.500000  74980.000000   \n",
       "\n",
       "                Eat  \n",
       "count  16242.000000  \n",
       "mean     -11.178969  \n",
       "std        3.659133  \n",
       "min      -23.245373  \n",
       "25%      -13.475805  \n",
       "50%      -10.835211  \n",
       "75%       -8.623903  \n",
       "max       -0.789513  \n",
       "\n",
       "[8 rows x 1278 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "datos= pd.read_csv(\"EnergyMolecule/roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "#primera columna son los indices, la penultima es un public chemical id? no es necesario en este contexto\n",
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    ">a.1) Una buena práctica es la de normalizar los datos antes de trabajar con el modelo. **Explique por qué se aconseja dicho preprocesamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "X_val_scaled =  pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "X_test_scaled =  pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "\n",
    "y_train = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val.pop('Eat').values.reshape(-1,1)\n",
    "\n",
    "X_train_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_val_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_test_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    ">b) Muestre en un gráfico el error cuadrático (MSE) para el conjunto de entrenamiento y de pruebas vs número de *epochs* de entrenamiento, para una red *feedforward* de 3 capas, con 256 unidades ocultas y función de activación sigmoidal. Entrene la red usando gradiente descendente estocástico con tasa de aprendizaje (learning rate) 0.01 y 250 epochs de entrenamiento, en el conjunto de entrenamiento y de validación. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 238us/step - loss: 1.4477 - val_loss: 0.5873\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 1s 121us/step - loss: 0.6115 - val_loss: 0.4698\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 1s 120us/step - loss: 0.5188 - val_loss: 0.4067\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 1s 119us/step - loss: 0.4376 - val_loss: 0.3776\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 1s 134us/step - loss: 0.3834 - val_loss: 0.3820\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.3360 - val_loss: 0.2871\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 1s 118us/step - loss: 0.2948 - val_loss: 0.3452\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 1s 120us/step - loss: 0.2657 - val_loss: 0.2303\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 1s 115us/step - loss: 0.2370 - val_loss: 0.2243\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.2100 - val_loss: 0.1966\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 1s 119us/step - loss: 0.1894 - val_loss: 0.2009\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.1708 - val_loss: 0.1733\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 1s 121us/step - loss: 0.1562 - val_loss: 0.1487\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.1419 - val_loss: 0.1644\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 1s 120us/step - loss: 0.1282 - val_loss: 0.1358\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 1s 119us/step - loss: 0.1182 - val_loss: 0.1202\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 1s 120us/step - loss: 0.1082 - val_loss: 0.1231\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.1021 - val_loss: 0.1149\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 1s 121us/step - loss: 0.0956 - val_loss: 0.1285\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 1s 110us/step - loss: 0.0928 - val_loss: 0.1109\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 1s 109us/step - loss: 0.0850 - val_loss: 0.0947\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 1s 114us/step - loss: 0.0821 - val_loss: 0.0910\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 1s 114us/step - loss: 0.0796 - val_loss: 0.1031\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 1s 114us/step - loss: 0.0743 - val_loss: 0.0891\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 1s 132us/step - loss: 0.0702 - val_loss: 0.1205\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0695 - val_loss: 0.0840\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0660 - val_loss: 0.0758\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 1s 111us/step - loss: 0.0649 - val_loss: 0.0779\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 1s 114us/step - loss: 0.0623 - val_loss: 0.0706\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 1s 109us/step - loss: 0.0608 - val_loss: 0.0776\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 1s 111us/step - loss: 0.0608 - val_loss: 0.0689\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 1s 122us/step - loss: 0.0578 - val_loss: 0.0672\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 1s 127us/step - loss: 0.0553 - val_loss: 0.0711\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 1s 145us/step - loss: 0.0542 - val_loss: 0.0653\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0529 - val_loss: 0.0918\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0513 - val_loss: 0.0813\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 1s 143us/step - loss: 0.0506 - val_loss: 0.0616\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 1s 139us/step - loss: 0.0492 - val_loss: 0.1130\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 1s 136us/step - loss: 0.0483 - val_loss: 0.0644\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 1s 119us/step - loss: 0.0483 - val_loss: 0.0628\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 1s 116us/step - loss: 0.0465 - val_loss: 0.0733\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 1s 117us/step - loss: 0.0463 - val_loss: 0.0635\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0448 - val_loss: 0.0588\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 1s 116us/step - loss: 0.0447 - val_loss: 0.0552\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 1s 109us/step - loss: 0.0428 - val_loss: 0.0667\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0446 - val_loss: 0.0539\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0426 - val_loss: 0.0546\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 1s 129us/step - loss: 0.0413 - val_loss: 0.0544\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 1s 110us/step - loss: 0.0426 - val_loss: 0.0537\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0404 - val_loss: 0.0702\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0406 - val_loss: 0.0527\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 1s 115us/step - loss: 0.0395 - val_loss: 0.0572\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 1s 112us/step - loss: 0.0397 - val_loss: 0.0731\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 1s 108us/step - loss: 0.0390 - val_loss: 0.0627\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 1s 113us/step - loss: 0.0375 - val_loss: 0.0574\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 1s 108us/step - loss: 0.0363 - val_loss: 0.0484\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.0365 - val_loss: 0.0547\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 1s 105us/step - loss: 0.0361 - val_loss: 0.0564\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.0354 - val_loss: 0.0471\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 1s 135us/step - loss: 0.0358 - val_loss: 0.0499\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 4s 398us/step - loss: 0.0351 - val_loss: 0.0566\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 5s 479us/step - loss: 0.0349 - val_loss: 0.0477\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 5s 521us/step - loss: 0.0358 - val_loss: 0.0502\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 5s 500us/step - loss: 0.0336 - val_loss: 0.0548\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 5s 496us/step - loss: 0.0331 - val_loss: 0.0468\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 5s 501us/step - loss: 0.0330 - val_loss: 0.0899\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 0.0336 - val_loss: 0.0593\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 5s 466us/step - loss: 0.0333 - val_loss: 0.0473\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 4s 460us/step - loss: 0.0319 - val_loss: 0.0469\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 4s 437us/step - loss: 0.0318 - val_loss: 0.0520\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0318 - val_loss: 0.0447\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 5s 484us/step - loss: 0.0307 - val_loss: 0.0458\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 4s 436us/step - loss: 0.0317 - val_loss: 0.0666\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 4s 401us/step - loss: 0.0312 - val_loss: 0.0451\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0314 - val_loss: 0.0465\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 0.0300 - val_loss: 0.0510\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0304 - val_loss: 0.0515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0300 - val_loss: 0.0526\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0297 - val_loss: 0.1163\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0292 - val_loss: 0.0432\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0287 - val_loss: 0.0585\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 4s 375us/step - loss: 0.0282 - val_loss: 0.0436\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0290 - val_loss: 0.0467\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0286 - val_loss: 0.0479\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0287 - val_loss: 0.0454\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 4s 382us/step - loss: 0.0278 - val_loss: 0.0416\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0281 - val_loss: 0.0742\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.0278 - val_loss: 0.0440\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 4s 373us/step - loss: 0.0273 - val_loss: 0.0449\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0274 - val_loss: 0.0407\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: 0.0274 - val_loss: 0.0479\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 4s 409us/step - loss: 0.0261 - val_loss: 0.0453\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 4s 388us/step - loss: 0.0262 - val_loss: 0.0590\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 4s 376us/step - loss: 0.0264 - val_loss: 0.0475\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0264 - val_loss: 0.0595\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0262 - val_loss: 0.0434\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 4s 388us/step - loss: 0.0261 - val_loss: 0.0515\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0260 - val_loss: 0.0448\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: 0.0262 - val_loss: 0.0408\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0259 - val_loss: 0.0458\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0255 - val_loss: 0.0425\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: 0.0252 - val_loss: 0.0466\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0249 - val_loss: 0.0470\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 4s 392us/step - loss: 0.0254 - val_loss: 0.0488\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.0246 - val_loss: 0.0474\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0245 - val_loss: 0.0397\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 344us/step - loss: 0.0248 - val_loss: 0.0416\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 4s 359us/step - loss: 0.0238 - val_loss: 0.0437\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0242 - val_loss: 0.0514\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0240 - val_loss: 0.0441\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0242 - val_loss: 0.0454\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0235 - val_loss: 0.0410\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: 0.0238 - val_loss: 0.0393\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: 0.0238 - val_loss: 0.0412\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 4s 386us/step - loss: 0.0230 - val_loss: 0.0384\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 4s 391us/step - loss: 0.0232 - val_loss: 0.0404\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 4s 415us/step - loss: 0.0228 - val_loss: 0.0396\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 4s 391us/step - loss: 0.0234 - val_loss: 0.0382\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 5s 505us/step - loss: 0.0226 - val_loss: 0.0385\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0229 - val_loss: 0.0413\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 0.0222 - val_loss: 0.0453\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 0.0220 - val_loss: 0.0508\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0222 - val_loss: 0.0471\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0226 - val_loss: 0.0474\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.0227 - val_loss: 0.0383\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0224 - val_loss: 0.0374\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0224 - val_loss: 0.0385\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0217 - val_loss: 0.1022\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 4s 365us/step - loss: 0.0217 - val_loss: 0.0430\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 4s 378us/step - loss: 0.0217 - val_loss: 0.0423\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0215 - val_loss: 0.0364\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0213 - val_loss: 0.0375\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.0216 - val_loss: 0.0435\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.0213 - val_loss: 0.0701\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 4s 404us/step - loss: 0.0221 - val_loss: 0.0384\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: 0.0214 - val_loss: 0.0376\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 5s 480us/step - loss: 0.0206 - val_loss: 0.0473\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 4s 385us/step - loss: 0.0211 - val_loss: 0.0374\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 4s 404us/step - loss: 0.0215 - val_loss: 0.0388\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 4s 378us/step - loss: 0.0213 - val_loss: 0.0392\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: 0.0211 - val_loss: 0.0392\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 3s 354us/step - loss: 0.0207 - val_loss: 0.0400\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: 0.0202 - val_loss: 0.0395\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0206 - val_loss: 0.0447\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0204 - val_loss: 0.0370\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 4s 370us/step - loss: 0.0200 - val_loss: 0.0366\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0202 - val_loss: 0.0383\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.0194 - val_loss: 0.0405\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 4s 402us/step - loss: 0.0199 - val_loss: 0.0376\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0196 - val_loss: 0.0415\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0193 - val_loss: 0.0382\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0193 - val_loss: 0.0400\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0199 - val_loss: 0.0419\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0196 - val_loss: 0.0503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 3s 348us/step - loss: 0.0201 - val_loss: 0.0407\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0193 - val_loss: 0.0380\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0189 - val_loss: 0.0358\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0199 - val_loss: 0.0398\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 4s 362us/step - loss: 0.0196 - val_loss: 0.0371\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 346us/step - loss: 0.0184 - val_loss: 0.0524\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 345us/step - loss: 0.0191 - val_loss: 0.0369\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0193 - val_loss: 0.0404\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 349us/step - loss: 0.0187 - val_loss: 0.0344\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0187 - val_loss: 0.0389\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 3s 353us/step - loss: 0.0184 - val_loss: 0.0421\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 3s 352us/step - loss: 0.0187 - val_loss: 0.0397\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 0.0187 - val_loss: 0.0356\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 4s 360us/step - loss: 0.0182 - val_loss: 0.0446\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 3s 351us/step - loss: 0.0184 - val_loss: 0.0390\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 4s 368us/step - loss: 0.0182 - val_loss: 0.0355\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 0.0181 - val_loss: 0.0388\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 4s 392us/step - loss: 0.0183 - val_loss: 0.0348\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 4s 410us/step - loss: 0.0186 - val_loss: 0.0480\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 0.0183 - val_loss: 0.0415\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 4s 434us/step - loss: 0.0183 - val_loss: 0.0395\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 4s 441us/step - loss: 0.0184 - val_loss: 0.0354\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 4s 456us/step - loss: 0.0174 - val_loss: 0.0344\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 4s 457us/step - loss: 0.0175 - val_loss: 0.0364\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: 0.0180 - val_loss: 0.0374\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 4s 392us/step - loss: 0.0184 - val_loss: 0.0341\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 4s 419us/step - loss: 0.0175 - val_loss: 0.0362\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 4s 389us/step - loss: 0.0171 - val_loss: 0.0360\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 4s 414us/step - loss: 0.0173 - val_loss: 0.0343\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 4s 382us/step - loss: 0.0176 - val_loss: 0.0346\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 4s 376us/step - loss: 0.0170 - val_loss: 0.0413\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 4s 377us/step - loss: 0.0170 - val_loss: 0.0375\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0170 - val_loss: 0.0364\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 355us/step - loss: 0.0172 - val_loss: 0.0338\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: 0.0172 - val_loss: 0.0345\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0173 - val_loss: 0.0357\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 4s 366us/step - loss: 0.0173 - val_loss: 0.0504\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 4s 367us/step - loss: 0.0175 - val_loss: 0.0371\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: 0.0170 - val_loss: 0.0365\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 5s 502us/step - loss: 0.0170 - val_loss: 0.0401\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 5s 462us/step - loss: 0.0166 - val_loss: 0.0345\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 4s 420us/step - loss: 0.0170 - val_loss: 0.0347\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 4s 446us/step - loss: 0.0169 - val_loss: 0.0397\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 4s 450us/step - loss: 0.0168 - val_loss: 0.0366\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 4s 369us/step - loss: 0.0169 - val_loss: 0.0349\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 4s 375us/step - loss: 0.0164 - val_loss: 0.0342\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 4s 385us/step - loss: 0.0166 - val_loss: 0.0610\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 3s 358us/step - loss: 0.0170 - val_loss: 0.0344\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 3s 350us/step - loss: 0.0160 - val_loss: 0.0344\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 4s 361us/step - loss: 0.0164 - val_loss: 0.0366\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 4s 383us/step - loss: 0.0163 - val_loss: 0.0334\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 4s 363us/step - loss: 0.0164 - val_loss: 0.0367\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 3s 357us/step - loss: 0.0167 - val_loss: 0.0337\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 4s 381us/step - loss: 0.0158 - val_loss: 0.0333\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 4s 398us/step - loss: 0.0159 - val_loss: 0.0374\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 4s 433us/step - loss: 0.0163 - val_loss: 0.0363\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 4s 407us/step - loss: 0.0157 - val_loss: 0.0352\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.0164 - val_loss: 0.0376\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 4s 388us/step - loss: 0.0160 - val_loss: 0.0374\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 4s 387us/step - loss: 0.0164 - val_loss: 0.0353\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 4s 408us/step - loss: 0.0156 - val_loss: 0.0337\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 4s 448us/step - loss: 0.0160 - val_loss: 0.0349\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 4s 412us/step - loss: 0.0157 - val_loss: 0.0459\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 3s 356us/step - loss: 0.0157 - val_loss: 0.0344\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 3s 338us/step - loss: 0.0161 - val_loss: 0.0347\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 3s 347us/step - loss: 0.0153 - val_loss: 0.0376\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 0.0158 - val_loss: 0.0327\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0154 - val_loss: 0.0332\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 1s 94us/step - loss: 0.0156 - val_loss: 0.0346\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0154 - val_loss: 0.0370\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 1s 88us/step - loss: 0.0161 - val_loss: 0.0372\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0153 - val_loss: 0.0331\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 1s 87us/step - loss: 0.0153 - val_loss: 0.0337\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 1s 85us/step - loss: 0.0152 - val_loss: 0.0380\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 1s 86us/step - loss: 0.0151 - val_loss: 0.0363\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 1s 89us/step - loss: 0.0152 - val_loss: 0.0362\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 1s 88us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 1s 85us/step - loss: 0.0153 - val_loss: 0.0361\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 1s 86us/step - loss: 0.0152 - val_loss: 0.0379\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 1s 85us/step - loss: 0.0153 - val_loss: 0.0327\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 1s 86us/step - loss: 0.0151 - val_loss: 0.0445\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 1s 86us/step - loss: 0.0155 - val_loss: 0.0409\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 1s 87us/step - loss: 0.0146 - val_loss: 0.0354\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 1s 88us/step - loss: 0.0151 - val_loss: 0.0394\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 1s 87us/step - loss: 0.0149 - val_loss: 0.0332\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 1s 86us/step - loss: 0.0151 - val_loss: 0.0397\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 1s 85us/step - loss: 0.0147 - val_loss: 0.0448\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 1s 85us/step - loss: 0.0150 - val_loss: 0.0415\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 1s 85us/step - loss: 0.0150 - val_loss: 0.0341\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 1s 86us/step - loss: 0.0150 - val_loss: 0.0340\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 1s 86us/step - loss: 0.0146 - val_loss: 0.0346\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 1s 87us/step - loss: 0.0146 - val_loss: 0.0341\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 1s 86us/step - loss: 0.0146 - val_loss: 0.0432\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 1s 86us/step - loss: 0.0144 - val_loss: 0.0324\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 1s 86us/step - loss: 0.0144 - val_loss: 0.0323\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 1s 87us/step - loss: 0.0145 - val_loss: 0.0326\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title('Sigmoide')\n",
    "plt.plot(history.history['loss'],label='train');\n",
    "plt.plot(history.history['val_loss'],label='validation');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "> c) Repita el paso anterior, utilizado ’**ReLU**’ como función de activación y compare con lo obtenido en b).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario disminuir el learning rate en un factor de 10, de lo contrario el error es demasiado grande con lo que el algoritmo diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 9.3825 - val_loss: 4.4363\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 2.1793 - val_loss: 2.7727\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.8572 - val_loss: 1.3909\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.6973 - val_loss: 1.8484\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.6084 - val_loss: 1.3405\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.4565 - val_loss: 1.2550\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.4517 - val_loss: 1.5995\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.3956 - val_loss: 0.9165\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 1s 108us/step - loss: 0.3446 - val_loss: 0.9730\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 1s 105us/step - loss: 0.4942 - val_loss: 0.8994\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 1s 117us/step - loss: 0.3662 - val_loss: 0.7103\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 1s 104us/step - loss: 0.2542 - val_loss: 0.7892\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.2511 - val_loss: 0.7020\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 1s 94us/step - loss: 0.2181 - val_loss: 0.6475\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 1s 105us/step - loss: 0.2172 - val_loss: 0.5378\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.1952 - val_loss: 0.6271\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.1791 - val_loss: 0.5451\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 1s 104us/step - loss: 0.1704 - val_loss: 0.5743\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 1s 108us/step - loss: 0.1518 - val_loss: 0.4852\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 1s 103us/step - loss: 0.1536 - val_loss: 0.5501\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 1s 118us/step - loss: 0.1747 - val_loss: 0.4707\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 1s 112us/step - loss: 0.1404 - val_loss: 0.5656\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 1s 103us/step - loss: 0.1299 - val_loss: 0.4348\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 1s 112us/step - loss: 0.1258 - val_loss: 0.4430\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 1s 107us/step - loss: 0.1359 - val_loss: 0.4052\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.1384 - val_loss: 0.3934\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.1233 - val_loss: 0.3676\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.1278 - val_loss: 0.3631\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.1115 - val_loss: 0.4034\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.1183 - val_loss: 0.3597\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 1s 99us/step - loss: 0.1912 - val_loss: 0.4534\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.1278 - val_loss: 0.3935\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.1013 - val_loss: 0.7247\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.1250 - val_loss: 0.3959\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.0998 - val_loss: 0.3668\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 1s 106us/step - loss: 0.0971 - val_loss: 0.4358\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 1s 105us/step - loss: 0.0943 - val_loss: 0.3537\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 1s 102us/step - loss: 0.1046 - val_loss: 0.3438\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 1s 106us/step - loss: 0.1050 - val_loss: 0.4190\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 1s 111us/step - loss: 0.0997 - val_loss: 0.3297\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 1s 105us/step - loss: 0.0841 - val_loss: 0.3230\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 1s 102us/step - loss: 0.0892 - val_loss: 0.3578\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0805 - val_loss: 0.3670\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 1s 103us/step - loss: 0.0830 - val_loss: 0.3691\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 1s 109us/step - loss: 0.0797 - val_loss: 0.3274\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 1s 118us/step - loss: 0.0778 - val_loss: 0.3284\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.0765 - val_loss: 0.3136\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 1s 99us/step - loss: 0.0733 - val_loss: 0.3777\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.0855 - val_loss: 0.2945\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 1s 99us/step - loss: 0.0766 - val_loss: 0.3432\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 1s 94us/step - loss: 0.0816 - val_loss: 0.3412\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 1s 94us/step - loss: 0.0825 - val_loss: 0.3145\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0732 - val_loss: 0.2985\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0854 - val_loss: 0.3102\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0684 - val_loss: 0.2954\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0694 - val_loss: 0.3119\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0670 - val_loss: 0.2984\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0677 - val_loss: 0.2927\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0656 - val_loss: 0.3080\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 1s 99us/step - loss: 0.0646 - val_loss: 0.2821\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0592 - val_loss: 0.2986\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 1s 112us/step - loss: 0.0652 - val_loss: 0.2719\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 1s 114us/step - loss: 0.0662 - val_loss: 0.5329\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 1s 119us/step - loss: 0.0666 - val_loss: 0.4028\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 1s 115us/step - loss: 0.0614 - val_loss: 0.2679\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0599 - val_loss: 0.2738\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0569 - val_loss: 0.2502\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 1s 126us/step - loss: 0.0602 - val_loss: 0.2754\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0575 - val_loss: 0.2685\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 1s 125us/step - loss: 0.0574 - val_loss: 0.2877\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 1s 120us/step - loss: 0.0587 - val_loss: 0.2956\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 1s 108us/step - loss: 0.0528 - val_loss: 0.2811\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 1s 118us/step - loss: 0.0572 - val_loss: 0.2951\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 1s 113us/step - loss: 0.0551 - val_loss: 0.2654\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0483 - val_loss: 0.3932\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0520 - val_loss: 0.2957\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.0534 - val_loss: 0.2653\n",
      "Epoch 78/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0631 - val_loss: 0.2702\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.0512 - val_loss: 0.3205\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0495 - val_loss: 0.2783\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 1s 104us/step - loss: 0.0512 - val_loss: 0.2612\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 1s 110us/step - loss: 0.0626 - val_loss: 0.2794\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 1s 114us/step - loss: 0.0598 - val_loss: 0.3001\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0598 - val_loss: 0.2733\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0496 - val_loss: 0.2822\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0469 - val_loss: 0.3660\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0464 - val_loss: 0.2923\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 1s 109us/step - loss: 0.0475 - val_loss: 0.3447\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0451 - val_loss: 0.3429\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.0431 - val_loss: 0.3072\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 1s 109us/step - loss: 0.0438 - val_loss: 0.2779\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 1s 105us/step - loss: 0.0447 - val_loss: 0.2807\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 1s 102us/step - loss: 0.0437 - val_loss: 0.2613\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0412 - val_loss: 0.3176\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0427 - val_loss: 0.2819\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.0435 - val_loss: 0.2756\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 1s 110us/step - loss: 0.0435 - val_loss: 0.7013\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 1s 110us/step - loss: 0.0453 - val_loss: 0.2950\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 1s 114us/step - loss: 0.0454 - val_loss: 0.2696\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 1s 130us/step - loss: 0.0414 - val_loss: 0.2837\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 1s 124us/step - loss: 0.0426 - val_loss: 0.2550\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0419 - val_loss: 0.2468\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 1s 121us/step - loss: 0.0405 - val_loss: 0.2803\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 1s 123us/step - loss: 0.0425 - val_loss: 0.5704\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 1s 118us/step - loss: 0.0475 - val_loss: 0.2647\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 1s 115us/step - loss: 0.0395 - val_loss: 0.2711\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 1s 113us/step - loss: 0.0417 - val_loss: 0.2606\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 1s 109us/step - loss: 0.0389 - val_loss: 0.2724\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 1s 112us/step - loss: 0.0427 - val_loss: 0.2738\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 1s 103us/step - loss: 0.0406 - val_loss: 0.3085\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.0425 - val_loss: 0.2667\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0374 - val_loss: 0.2732\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0371 - val_loss: 0.2557\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0389 - val_loss: 0.2768\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0372 - val_loss: 0.2671\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0380 - val_loss: 0.2772\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0357 - val_loss: 0.2627\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0427 - val_loss: 0.2939\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0372 - val_loss: 0.2720\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 1s 89us/step - loss: 0.0342 - val_loss: 0.2594\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0353 - val_loss: 0.2806\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0353 - val_loss: 0.2692\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0341 - val_loss: 0.2815\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0338 - val_loss: 0.2822\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0368 - val_loss: 0.2601\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0357 - val_loss: 0.2676\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0329 - val_loss: 0.2722\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0346 - val_loss: 0.2735\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0365 - val_loss: 0.2828\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0405 - val_loss: 0.2889\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0565 - val_loss: 0.2775\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0356 - val_loss: 0.2767\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0404 - val_loss: 0.4446\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0367 - val_loss: 0.2789\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0335 - val_loss: 0.2761\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0326 - val_loss: 0.3063\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0352 - val_loss: 0.2983\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0335 - val_loss: 0.2800\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 1s 89us/step - loss: 0.0311 - val_loss: 0.2740\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0377 - val_loss: 0.3021\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 1s 94us/step - loss: 0.0329 - val_loss: 0.2863\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.0337 - val_loss: 0.2949\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0308 - val_loss: 0.2715\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0335 - val_loss: 0.2656\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.0298 - val_loss: 0.2718\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 1s 103us/step - loss: 0.0302 - val_loss: 0.2963\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 1s 99us/step - loss: 0.0339 - val_loss: 0.2867\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0335 - val_loss: 0.3886\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0332 - val_loss: 0.2847\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0369 - val_loss: 0.2872\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0304 - val_loss: 2.8154\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0650 - val_loss: 0.2978\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0343 - val_loss: 0.3347\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0304 - val_loss: 0.2856\n",
      "Epoch 155/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0321 - val_loss: 0.3071\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0349 - val_loss: 0.4042\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 1s 94us/step - loss: 0.0409 - val_loss: 0.2783\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0297 - val_loss: 0.3168\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 1s 94us/step - loss: 0.0295 - val_loss: 0.2980\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0304 - val_loss: 0.3386\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0301 - val_loss: 0.2857\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0291 - val_loss: 0.2978\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0297 - val_loss: 0.2808\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0333 - val_loss: 0.2905\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0360 - val_loss: 0.2919\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0288 - val_loss: 0.3044\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0276 - val_loss: 0.3001\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0321 - val_loss: 0.2980\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0296 - val_loss: 0.3124\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0302 - val_loss: 0.3061\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 1s 89us/step - loss: 0.0287 - val_loss: 0.2973\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0334 - val_loss: 0.3246\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0271 - val_loss: 0.3018\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0298 - val_loss: 0.2963\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0297 - val_loss: 0.2934\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0271 - val_loss: 0.2843\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 1s 89us/step - loss: 0.0262 - val_loss: 0.2969\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0302 - val_loss: 0.2881\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 1s 89us/step - loss: 0.0294 - val_loss: 0.3030\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0269 - val_loss: 0.2851\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0260 - val_loss: 0.2853\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0264 - val_loss: 0.2977\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0254 - val_loss: 0.3052\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0267 - val_loss: 0.2858\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.0275 - val_loss: 0.2992\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.0257 - val_loss: 0.2893\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.0275 - val_loss: 0.3146\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 1s 109us/step - loss: 0.0247 - val_loss: 0.3468\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 1s 102us/step - loss: 0.0259 - val_loss: 0.3199\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 1s 103us/step - loss: 0.0270 - val_loss: 0.3079\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 1s 128us/step - loss: 0.0246 - val_loss: 0.3015\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 1s 105us/step - loss: 0.0265 - val_loss: 0.3062\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0281 - val_loss: 0.2961\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0243 - val_loss: 0.3154\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0244 - val_loss: 0.3021\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0261 - val_loss: 0.3094\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0256 - val_loss: 0.2942\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0304 - val_loss: 0.2974\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.0259 - val_loss: 0.3191\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0296 - val_loss: 0.3019\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0253 - val_loss: 0.3096\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0262 - val_loss: 0.3101\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0249 - val_loss: 0.3095\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0254 - val_loss: 0.3083\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0244 - val_loss: 0.3229\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0249 - val_loss: 0.3061\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0256 - val_loss: 0.3007\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0237 - val_loss: 0.3044\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0302 - val_loss: 0.3406\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0263 - val_loss: 0.3128\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0284 - val_loss: 0.3276\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0279 - val_loss: 0.3195\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0278 - val_loss: 0.3022\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0270 - val_loss: 0.3232\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0240 - val_loss: 0.3209\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0231 - val_loss: 0.3174\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0223 - val_loss: 0.3066\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0212 - val_loss: 0.3277\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0239 - val_loss: 0.3079\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0260 - val_loss: 0.3276\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0329 - val_loss: 0.3131\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 1s 92us/step - loss: 0.0243 - val_loss: 0.3314\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0237 - val_loss: 0.3221\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 1s 90us/step - loss: 0.0234 - val_loss: 0.3370\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 1s 94us/step - loss: 0.0230 - val_loss: 0.3023\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0217 - val_loss: 0.3219\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 1s 93us/step - loss: 0.0218 - val_loss: 0.3132\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0223 - val_loss: 0.3202\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 1s 100us/step - loss: 0.0217 - val_loss: 0.3485\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.0220 - val_loss: 0.3034\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0229 - val_loss: 0.3149\n",
      "Epoch 232/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0218 - val_loss: 0.3372\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 1s 101us/step - loss: 0.0213 - val_loss: 0.3515\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 1s 94us/step - loss: 0.0218 - val_loss: 0.3234\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0233 - val_loss: 0.3421\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0215 - val_loss: 0.3193\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0219 - val_loss: 0.3295\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0236 - val_loss: 0.3179\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0226 - val_loss: 0.3146\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0228 - val_loss: 0.3151\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0213 - val_loss: 0.3308\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0221 - val_loss: 0.3287\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0217 - val_loss: 0.3091\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 1s 97us/step - loss: 0.0210 - val_loss: 0.3147\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0209 - val_loss: 0.3454\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0211 - val_loss: 0.3129\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 1s 98us/step - loss: 0.0209 - val_loss: 0.3309\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 1s 96us/step - loss: 0.0223 - val_loss: 0.3296\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 1s 95us/step - loss: 0.0216 - val_loss: 0.3318\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 1s 91us/step - loss: 0.0209 - val_loss: 0.3290\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "history_relu = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title('ReLU')\n",
    "plt.plot(history_relu.history['loss'],label='train');\n",
    "plt.plot(history_relu.history['val_loss'],label='validation');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el learning rate es menor se tiene una baja en la velocidad de convergencia y  en 250 epoch logra un desempeño menor en comparación con la función de activación sigmoide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "> d) Repita b) y c) variando la tasa de aprendizaje (*learning rate*) en un rango sensible. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 496us/step - loss: 3.8711 - val_loss: 1.1181\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 1.1497 - val_loss: 0.9000\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.9720 - val_loss: 0.7660\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.8641 - val_loss: 0.6836\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.7966 - val_loss: 0.6426\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.7482 - val_loss: 0.5950\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 224us/step - loss: 0.7124 - val_loss: 0.5719\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.6831 - val_loss: 0.5603\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.6600 - val_loss: 0.5342\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.6391 - val_loss: 0.5218\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.6214 - val_loss: 0.5141\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.6059 - val_loss: 0.4972\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.5925 - val_loss: 0.4858\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.5795 - val_loss: 0.4732\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.5672 - val_loss: 0.4668\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.5563 - val_loss: 0.4627\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.5458 - val_loss: 0.4648\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.5366 - val_loss: 0.4534\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.5268 - val_loss: 0.4429\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.5181 - val_loss: 0.4306\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.5093 - val_loss: 0.4244\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.5018 - val_loss: 0.4216\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.4927 - val_loss: 0.4104\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.4853 - val_loss: 0.4096\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.4784 - val_loss: 0.4032\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.4704 - val_loss: 0.3966\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.4635 - val_loss: 0.3970\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.4566 - val_loss: 0.3932\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.4498 - val_loss: 0.3833\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.4432 - val_loss: 0.3815\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.4359 - val_loss: 0.3776\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.4301 - val_loss: 0.3715\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.4238 - val_loss: 0.3649\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.4176 - val_loss: 0.3771\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.4122 - val_loss: 0.3537\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.4052 - val_loss: 0.3488\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.4003 - val_loss: 0.3438\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.3941 - val_loss: 0.3401\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.3874 - val_loss: 0.3375\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.3828 - val_loss: 0.3301\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.3774 - val_loss: 0.3316\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.3715 - val_loss: 0.3245\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.3661 - val_loss: 0.3315\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.3618 - val_loss: 0.3159\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.3564 - val_loss: 0.3125\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.3514 - val_loss: 0.3087\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.3468 - val_loss: 0.3095\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.3420 - val_loss: 0.3029\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.3370 - val_loss: 0.2972\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.3326 - val_loss: 0.2945\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.3284 - val_loss: 0.2916\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.3233 - val_loss: 0.2921\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.3189 - val_loss: 0.2892\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.3145 - val_loss: 0.2821\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.3104 - val_loss: 0.2806\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.3062 - val_loss: 0.2760\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.3020 - val_loss: 0.2721\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.2981 - val_loss: 0.2689\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.2943 - val_loss: 0.2658\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2902 - val_loss: 0.2623\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.2861 - val_loss: 0.2616\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.2828 - val_loss: 0.2595\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2791 - val_loss: 0.2547\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.2756 - val_loss: 0.2522\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.2721 - val_loss: 0.2506\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.2685 - val_loss: 0.2471\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.2647 - val_loss: 0.2441\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.2611 - val_loss: 0.2439\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.2582 - val_loss: 0.2403\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.2548 - val_loss: 0.2437\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.2513 - val_loss: 0.2373\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.2480 - val_loss: 0.2390\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.2451 - val_loss: 0.2305\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.2414 - val_loss: 0.2283\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2384 - val_loss: 0.2277\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.2357 - val_loss: 0.2266\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2323 - val_loss: 0.2222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.2296 - val_loss: 0.2219\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.2264 - val_loss: 0.2166\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.2237 - val_loss: 0.2150\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.2206 - val_loss: 0.2168\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2177 - val_loss: 0.2096\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.2152 - val_loss: 0.2086\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2121 - val_loss: 0.2065\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.2097 - val_loss: 0.2048\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.2067 - val_loss: 0.2017\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.2037 - val_loss: 0.2015\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2017 - val_loss: 0.1983\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1986 - val_loss: 0.1959\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1964 - val_loss: 0.1949\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1937 - val_loss: 0.1924\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1913 - val_loss: 0.1938\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1889 - val_loss: 0.1904\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1864 - val_loss: 0.1866\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.1842 - val_loss: 0.1857\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1816 - val_loss: 0.1840\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1794 - val_loss: 0.1814\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1771 - val_loss: 0.1801\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1748 - val_loss: 0.1795\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1728 - val_loss: 0.1767\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.1707 - val_loss: 0.1766\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1682 - val_loss: 0.1746\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.1664 - val_loss: 0.1722\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1642 - val_loss: 0.1705\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1622 - val_loss: 0.1690\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.1601 - val_loss: 0.1668\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.1583 - val_loss: 0.1649\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1564 - val_loss: 0.1640\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1548 - val_loss: 0.1619\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1527 - val_loss: 0.1615\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1506 - val_loss: 0.1612\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.1493 - val_loss: 0.1579\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1473 - val_loss: 0.1580\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1456 - val_loss: 0.1553\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.1441 - val_loss: 0.1543\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1425 - val_loss: 0.1538\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.1410 - val_loss: 0.1512\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1392 - val_loss: 0.1501\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1376 - val_loss: 0.1488\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1362 - val_loss: 0.1503\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1346 - val_loss: 0.1469\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1331 - val_loss: 0.1452\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1317 - val_loss: 0.1443\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1304 - val_loss: 0.1432\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1287 - val_loss: 0.1417\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.1275 - val_loss: 0.1408\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1265 - val_loss: 0.1393\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1250 - val_loss: 0.1383\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1237 - val_loss: 0.1371\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1224 - val_loss: 0.1361\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1211 - val_loss: 0.1352\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.1201 - val_loss: 0.1363\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.1189 - val_loss: 0.1330\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1179 - val_loss: 0.1330\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1167 - val_loss: 0.1320\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.1153 - val_loss: 0.1314\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1144 - val_loss: 0.1303\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.1133 - val_loss: 0.1285\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.1122 - val_loss: 0.1286\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1113 - val_loss: 0.1267\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1100 - val_loss: 0.1261\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1093 - val_loss: 0.1247\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.1082 - val_loss: 0.1248\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1072 - val_loss: 0.1233\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1063 - val_loss: 0.1219\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1054 - val_loss: 0.1211\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1044 - val_loss: 0.1206\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.1035 - val_loss: 0.1197\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.1027 - val_loss: 0.1203\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1019 - val_loss: 0.1188\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.1009 - val_loss: 0.1178\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1003 - val_loss: 0.1167\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0994 - val_loss: 0.1159\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0986 - val_loss: 0.1151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0977 - val_loss: 0.1140\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0971 - val_loss: 0.1136\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0963 - val_loss: 0.1126\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0955 - val_loss: 0.1127\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0948 - val_loss: 0.1119\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0941 - val_loss: 0.1108\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0933 - val_loss: 0.1099\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0926 - val_loss: 0.1098\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0919 - val_loss: 0.1089\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0913 - val_loss: 0.1088\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0906 - val_loss: 0.1080\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0899 - val_loss: 0.1070\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0892 - val_loss: 0.1061\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0885 - val_loss: 0.1057\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0882 - val_loss: 0.1050\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0875 - val_loss: 0.1043\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0869 - val_loss: 0.1036\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0861 - val_loss: 0.1032\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0856 - val_loss: 0.1026\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0852 - val_loss: 0.1021\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0845 - val_loss: 0.1025\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0839 - val_loss: 0.1010\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0832 - val_loss: 0.1005\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0830 - val_loss: 0.1000\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0824 - val_loss: 0.0993\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0819 - val_loss: 0.0988\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0811 - val_loss: 0.0986\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0807 - val_loss: 0.0977\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0803 - val_loss: 0.0972\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0799 - val_loss: 0.0974\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0795 - val_loss: 0.0961\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0787 - val_loss: 0.0988\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0785 - val_loss: 0.0955\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0779 - val_loss: 0.0950\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0776 - val_loss: 0.0944\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0769 - val_loss: 0.0949\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0765 - val_loss: 0.0933\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0759 - val_loss: 0.0934\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0759 - val_loss: 0.0927\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0753 - val_loss: 0.0923\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0750 - val_loss: 0.0919\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0745 - val_loss: 0.0916\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0741 - val_loss: 0.0920\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0737 - val_loss: 0.0902\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0732 - val_loss: 0.0902\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0727 - val_loss: 0.0898\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0725 - val_loss: 0.0890\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0721 - val_loss: 0.0891\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0718 - val_loss: 0.0882\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0713 - val_loss: 0.0882\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0711 - val_loss: 0.0879\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0707 - val_loss: 0.0872\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0702 - val_loss: 0.0868\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0701 - val_loss: 0.0863\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0697 - val_loss: 0.0867\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0694 - val_loss: 0.0855\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0688 - val_loss: 0.0866\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0686 - val_loss: 0.0850\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0681 - val_loss: 0.0855\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0680 - val_loss: 0.0850\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0675 - val_loss: 0.0839\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0672 - val_loss: 0.0837\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0670 - val_loss: 0.0837\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0667 - val_loss: 0.0828\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0663 - val_loss: 0.0825\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0661 - val_loss: 0.0828\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0657 - val_loss: 0.0821\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0653 - val_loss: 0.0841\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0652 - val_loss: 0.0813\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0648 - val_loss: 0.0808\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0646 - val_loss: 0.0806\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0644 - val_loss: 0.0802\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0640 - val_loss: 0.0801\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0637 - val_loss: 0.0796\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0635 - val_loss: 0.0802\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0632 - val_loss: 0.0787\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0629 - val_loss: 0.0789\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0628 - val_loss: 0.0784\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0623 - val_loss: 0.0785\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0620 - val_loss: 0.0780\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0618 - val_loss: 0.0783\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0616 - val_loss: 0.0776\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0613 - val_loss: 0.0769\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0610 - val_loss: 0.0772\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0608 - val_loss: 0.0763\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0605 - val_loss: 0.0765\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0604 - val_loss: 0.0765\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0600 - val_loss: 0.0756\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0598 - val_loss: 0.0762\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0596 - val_loss: 0.0749\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0595 - val_loss: 0.0749\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0592 - val_loss: 0.0744\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0589 - val_loss: 0.0746\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0587 - val_loss: 0.0740\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0584 - val_loss: 0.0739\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0582 - val_loss: 0.0739\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 480us/step - loss: 2.5643 - val_loss: 0.8792\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.9360 - val_loss: 0.6860\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.7831 - val_loss: 0.6137\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.7067 - val_loss: 0.6462\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.6570 - val_loss: 0.5303\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.6203 - val_loss: 0.4993\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.5886 - val_loss: 0.4738\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.5674 - val_loss: 0.4639\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.5471 - val_loss: 0.4548\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.5250 - val_loss: 0.4387\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.5080 - val_loss: 0.4229\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.4922 - val_loss: 0.4118\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.4761 - val_loss: 0.3959\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.4610 - val_loss: 0.3870\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.4463 - val_loss: 0.4074\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.4314 - val_loss: 0.3826\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.4176 - val_loss: 0.3560\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.4053 - val_loss: 0.3404\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.3923 - val_loss: 0.3326\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.3803 - val_loss: 0.3267\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.3688 - val_loss: 0.3240\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.3586 - val_loss: 0.3102\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.3466 - val_loss: 0.3076\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.3379 - val_loss: 0.2921\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.3273 - val_loss: 0.2908\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.3187 - val_loss: 0.2813\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.3091 - val_loss: 0.2755\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.3012 - val_loss: 0.2700\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.2929 - val_loss: 0.2671\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.2850 - val_loss: 0.2554\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.2766 - val_loss: 0.2520\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.2703 - val_loss: 0.2456\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.2633 - val_loss: 0.2405\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.2554 - val_loss: 0.2406\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.2504 - val_loss: 0.2354\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2433 - val_loss: 0.2296\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.2366 - val_loss: 0.2242\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2304 - val_loss: 0.2195\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.2249 - val_loss: 0.2188\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.2194 - val_loss: 0.2111\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.2141 - val_loss: 0.2043\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.2082 - val_loss: 0.2018\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.2032 - val_loss: 0.2212\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1981 - val_loss: 0.2011\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.1934 - val_loss: 0.1968\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1883 - val_loss: 0.1870\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.1835 - val_loss: 0.1859\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.1788 - val_loss: 0.1791\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1747 - val_loss: 0.1760\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1708 - val_loss: 0.1749\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.1664 - val_loss: 0.1696\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1620 - val_loss: 0.1670\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.1577 - val_loss: 0.1643\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.1545 - val_loss: 0.1627\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1511 - val_loss: 0.1601\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1473 - val_loss: 0.1631\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.1439 - val_loss: 0.1525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.1407 - val_loss: 0.1512\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1372 - val_loss: 0.1479\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1342 - val_loss: 0.1537\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1312 - val_loss: 0.1451\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.1284 - val_loss: 0.1408\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1259 - val_loss: 0.1385\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1241 - val_loss: 0.1363\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.1212 - val_loss: 0.1351\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.1189 - val_loss: 0.1323\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.1163 - val_loss: 0.1302\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.1145 - val_loss: 0.1296\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1121 - val_loss: 0.1261\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1097 - val_loss: 0.1246\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.1082 - val_loss: 0.1232\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1058 - val_loss: 0.1227\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.1042 - val_loss: 0.1220\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1026 - val_loss: 0.1195\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1007 - val_loss: 0.1175\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0990 - val_loss: 0.1145\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0976 - val_loss: 0.1133\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0961 - val_loss: 0.1131\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0945 - val_loss: 0.1106\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0931 - val_loss: 0.1136\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0919 - val_loss: 0.1096\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0906 - val_loss: 0.1078\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0891 - val_loss: 0.1056\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0878 - val_loss: 0.1053\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0866 - val_loss: 0.1039\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0857 - val_loss: 0.1036\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0843 - val_loss: 0.0999\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0836 - val_loss: 0.1007\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0821 - val_loss: 0.0990\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0811 - val_loss: 0.0980\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0803 - val_loss: 0.0985\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0792 - val_loss: 0.0955\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0783 - val_loss: 0.0939\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0775 - val_loss: 0.0943\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0766 - val_loss: 0.0922\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0756 - val_loss: 0.0915\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0747 - val_loss: 0.0904\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0737 - val_loss: 0.0902\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0730 - val_loss: 0.0899\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0726 - val_loss: 0.0881\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0716 - val_loss: 0.0872\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0708 - val_loss: 0.0868\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0699 - val_loss: 0.0866\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0694 - val_loss: 0.0862\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0683 - val_loss: 0.0859\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0679 - val_loss: 0.0847\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0672 - val_loss: 0.0832\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0665 - val_loss: 0.0829\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0659 - val_loss: 0.0819\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0652 - val_loss: 0.0810\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0649 - val_loss: 0.0810\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0640 - val_loss: 0.0806\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0633 - val_loss: 0.0791\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0630 - val_loss: 0.0794\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0624 - val_loss: 0.0785\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0617 - val_loss: 0.0773\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0612 - val_loss: 0.0772\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0609 - val_loss: 0.0764\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0602 - val_loss: 0.0760\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0597 - val_loss: 0.0749\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0594 - val_loss: 0.0751\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0589 - val_loss: 0.0747\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0584 - val_loss: 0.0743\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0580 - val_loss: 0.0738\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0575 - val_loss: 0.0736\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0569 - val_loss: 0.0733\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0567 - val_loss: 0.0733\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0561 - val_loss: 0.0717\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0556 - val_loss: 0.0713\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0551 - val_loss: 0.0715\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0548 - val_loss: 0.0719\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0545 - val_loss: 0.0716\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0539 - val_loss: 0.0718\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0536 - val_loss: 0.0701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0532 - val_loss: 0.0700\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0530 - val_loss: 0.0685\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0525 - val_loss: 0.0684\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0520 - val_loss: 0.0681\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0518 - val_loss: 0.0675\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0514 - val_loss: 0.0691\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0509 - val_loss: 0.0676\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0507 - val_loss: 0.0674\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0504 - val_loss: 0.0663\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0502 - val_loss: 0.0654\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0498 - val_loss: 0.0653\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0495 - val_loss: 0.0649\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0492 - val_loss: 0.0654\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0488 - val_loss: 0.0642\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0486 - val_loss: 0.0638\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0482 - val_loss: 0.0636\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0479 - val_loss: 0.0635\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0476 - val_loss: 0.0632\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0473 - val_loss: 0.0629\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0472 - val_loss: 0.0625\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0469 - val_loss: 0.0622\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0465 - val_loss: 0.0629\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0464 - val_loss: 0.0626\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0462 - val_loss: 0.0618\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0458 - val_loss: 0.0614\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0454 - val_loss: 0.0610\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0453 - val_loss: 0.0615\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0450 - val_loss: 0.0626\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0448 - val_loss: 0.0640\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0445 - val_loss: 0.0609\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0444 - val_loss: 0.0603\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0441 - val_loss: 0.0593\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0439 - val_loss: 0.0597\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0436 - val_loss: 0.0602\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0436 - val_loss: 0.0592\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0432 - val_loss: 0.0592\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0429 - val_loss: 0.0592\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0428 - val_loss: 0.0581\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0424 - val_loss: 0.0579\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0423 - val_loss: 0.0580\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0421 - val_loss: 0.0577\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0420 - val_loss: 0.0569\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0418 - val_loss: 0.0570\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0415 - val_loss: 0.0575\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0414 - val_loss: 0.0567\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0412 - val_loss: 0.0576\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0410 - val_loss: 0.0578\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0406 - val_loss: 0.0571\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0406 - val_loss: 0.0571\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0404 - val_loss: 0.0564\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0402 - val_loss: 0.0558\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0398 - val_loss: 0.0559\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0397 - val_loss: 0.0566\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0398 - val_loss: 0.0547\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0394 - val_loss: 0.0544\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0393 - val_loss: 0.0559\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0393 - val_loss: 0.0550\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0391 - val_loss: 0.0551\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0389 - val_loss: 0.0552\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0386 - val_loss: 0.0546\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0385 - val_loss: 0.0538\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0383 - val_loss: 0.0542\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0382 - val_loss: 0.0538\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0380 - val_loss: 0.0539\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0378 - val_loss: 0.0538\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0377 - val_loss: 0.0532\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0374 - val_loss: 0.0537\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0375 - val_loss: 0.0536\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0372 - val_loss: 0.0526\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0370 - val_loss: 0.0532\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0369 - val_loss: 0.0530\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0368 - val_loss: 0.0527\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0367 - val_loss: 0.0525\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0366 - val_loss: 0.0521\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0363 - val_loss: 0.0528\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0362 - val_loss: 0.0520\n",
      "Epoch 211/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0361 - val_loss: 0.0520\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0361 - val_loss: 0.0515\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0359 - val_loss: 0.0518\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0358 - val_loss: 0.0518\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0358 - val_loss: 0.0520\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0354 - val_loss: 0.0523\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0354 - val_loss: 0.0515\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0352 - val_loss: 0.0512\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0352 - val_loss: 0.0509\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0348 - val_loss: 0.0519\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0349 - val_loss: 0.0511\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0347 - val_loss: 0.0502\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0348 - val_loss: 0.0510\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0344 - val_loss: 0.0529\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0342 - val_loss: 0.0500\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0343 - val_loss: 0.0499\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0341 - val_loss: 0.0499\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0340 - val_loss: 0.0509\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0339 - val_loss: 0.0510\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0339 - val_loss: 0.0496\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0337 - val_loss: 0.0503\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0336 - val_loss: 0.0529\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0334 - val_loss: 0.0498\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0334 - val_loss: 0.0499\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0332 - val_loss: 0.0497\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0332 - val_loss: 0.0492\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0330 - val_loss: 0.0495\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0329 - val_loss: 0.0487\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0328 - val_loss: 0.0487\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0327 - val_loss: 0.0513\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0326 - val_loss: 0.0492\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0325 - val_loss: 0.0500\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0323 - val_loss: 0.0488\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0323 - val_loss: 0.0482\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0320 - val_loss: 0.0493\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0321 - val_loss: 0.0482\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0321 - val_loss: 0.0477\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0320 - val_loss: 0.0476\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0319 - val_loss: 0.0486\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0317 - val_loss: 0.0476\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 488us/step - loss: 2.0807 - val_loss: 0.7441\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.8027 - val_loss: 0.6062\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.6839 - val_loss: 0.5159\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.6226 - val_loss: 0.4958\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.5774 - val_loss: 0.4731\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.5433 - val_loss: 0.4324\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.5127 - val_loss: 0.4183\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.4862 - val_loss: 0.4191\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.4624 - val_loss: 0.3916\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.4400 - val_loss: 0.3609\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.4196 - val_loss: 0.3598\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.4000 - val_loss: 0.3439\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.3825 - val_loss: 0.3508\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.3636 - val_loss: 0.3106\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.3484 - val_loss: 0.3091\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.3325 - val_loss: 0.2959\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.3186 - val_loss: 0.2772\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.3055 - val_loss: 0.2737\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2919 - val_loss: 0.2763\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.2806 - val_loss: 0.2612\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2686 - val_loss: 0.2405\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.2578 - val_loss: 0.2331\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2478 - val_loss: 0.2264\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2379 - val_loss: 0.2217\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.2289 - val_loss: 0.2164\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.2202 - val_loss: 0.2073\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.2106 - val_loss: 0.1988\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.2033 - val_loss: 0.1994\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1949 - val_loss: 0.1930\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1876 - val_loss: 0.1848\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1804 - val_loss: 0.1824\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1741 - val_loss: 0.1740\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1680 - val_loss: 0.1687\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1616 - val_loss: 0.1629\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1555 - val_loss: 0.1615\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.1502 - val_loss: 0.1579\n",
      "Epoch 37/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1456 - val_loss: 0.1525\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.1406 - val_loss: 0.1459\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.1352 - val_loss: 0.1420\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1303 - val_loss: 0.1462\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1270 - val_loss: 0.1372\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1227 - val_loss: 0.1347\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.1195 - val_loss: 0.1285\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1156 - val_loss: 0.1287\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.1125 - val_loss: 0.1339\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1090 - val_loss: 0.1202\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.1060 - val_loss: 0.1207\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1034 - val_loss: 0.1151\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1006 - val_loss: 0.1157\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0980 - val_loss: 0.1113\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0961 - val_loss: 0.1116\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0938 - val_loss: 0.1090\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0915 - val_loss: 0.1046\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0896 - val_loss: 0.1036\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0874 - val_loss: 0.1027\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0851 - val_loss: 0.1018\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0839 - val_loss: 0.0979\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0824 - val_loss: 0.0996\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0809 - val_loss: 0.1011\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0791 - val_loss: 0.0936\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0777 - val_loss: 0.0999\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0765 - val_loss: 0.0914\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0751 - val_loss: 0.0924\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0741 - val_loss: 0.0893\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0727 - val_loss: 0.0957\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0713 - val_loss: 0.0880\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0705 - val_loss: 0.0884\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0694 - val_loss: 0.0865\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0681 - val_loss: 0.0852\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0673 - val_loss: 0.0836\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0659 - val_loss: 0.0820\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0652 - val_loss: 0.0810\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0645 - val_loss: 0.0805\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0639 - val_loss: 0.0800\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0627 - val_loss: 0.0788\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0624 - val_loss: 0.0778\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0613 - val_loss: 0.0781\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0606 - val_loss: 0.0767\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0596 - val_loss: 0.0768\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0593 - val_loss: 0.0751\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0584 - val_loss: 0.0756\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0578 - val_loss: 0.0738\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0572 - val_loss: 0.0786\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0567 - val_loss: 0.0729\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0561 - val_loss: 0.0721\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0554 - val_loss: 0.0704\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0548 - val_loss: 0.0714\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0542 - val_loss: 0.0770\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0536 - val_loss: 0.0694\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0529 - val_loss: 0.0680\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0527 - val_loss: 0.0682\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0524 - val_loss: 0.0680\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0519 - val_loss: 0.0674\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0514 - val_loss: 0.0720\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0505 - val_loss: 0.0668\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0504 - val_loss: 0.0672\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0501 - val_loss: 0.0656\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0494 - val_loss: 0.0650\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0492 - val_loss: 0.0661\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 215us/step - loss: 0.0488 - val_loss: 0.0658\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0481 - val_loss: 0.0649\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0478 - val_loss: 0.0656\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0476 - val_loss: 0.0637\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0476 - val_loss: 0.0633\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0470 - val_loss: 0.0644\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0465 - val_loss: 0.0628\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0462 - val_loss: 0.0615\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0458 - val_loss: 0.0616\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0455 - val_loss: 0.0627\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0450 - val_loss: 0.0603\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0447 - val_loss: 0.0618\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0445 - val_loss: 0.0608\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0443 - val_loss: 0.0609\n",
      "Epoch 114/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0438 - val_loss: 0.0609\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0438 - val_loss: 0.0605\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0435 - val_loss: 0.0590\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0430 - val_loss: 0.0596\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0428 - val_loss: 0.0587\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0424 - val_loss: 0.0579\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0422 - val_loss: 0.0601\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0420 - val_loss: 0.0578\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0417 - val_loss: 0.0576\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0415 - val_loss: 0.0584\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0413 - val_loss: 0.0597\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0408 - val_loss: 0.0575\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0408 - val_loss: 0.0574\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0403 - val_loss: 0.0560\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0402 - val_loss: 0.0584\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0398 - val_loss: 0.0594\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0397 - val_loss: 0.0554\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0396 - val_loss: 0.0573\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0393 - val_loss: 0.0569\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.0392 - val_loss: 0.0566\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0389 - val_loss: 0.0562\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0386 - val_loss: 0.0575\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0384 - val_loss: 0.0566\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 221us/step - loss: 0.0382 - val_loss: 0.0542\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0382 - val_loss: 0.0568\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0380 - val_loss: 0.0538\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0375 - val_loss: 0.0555\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0375 - val_loss: 0.0555\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0373 - val_loss: 0.0542\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0371 - val_loss: 0.0533\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0371 - val_loss: 0.0537\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0368 - val_loss: 0.0527\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0368 - val_loss: 0.0528\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0363 - val_loss: 0.0562\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0362 - val_loss: 0.0527\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0360 - val_loss: 0.0536\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0359 - val_loss: 0.0522\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0358 - val_loss: 0.0535\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0355 - val_loss: 0.0529\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0352 - val_loss: 0.0515\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0352 - val_loss: 0.0533\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0350 - val_loss: 0.0520\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0348 - val_loss: 0.0555\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0347 - val_loss: 0.0515\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0346 - val_loss: 0.0506\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0343 - val_loss: 0.0512\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0342 - val_loss: 0.0522\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0341 - val_loss: 0.0505\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0342 - val_loss: 0.0527\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0337 - val_loss: 0.0529\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0336 - val_loss: 0.0498\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0334 - val_loss: 0.0514\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0334 - val_loss: 0.0500\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0332 - val_loss: 0.0503\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0329 - val_loss: 0.0506\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0331 - val_loss: 0.0505\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0328 - val_loss: 0.0495\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0327 - val_loss: 0.0510\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0325 - val_loss: 0.0509\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0322 - val_loss: 0.0488\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0323 - val_loss: 0.0507\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0320 - val_loss: 0.0501\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0321 - val_loss: 0.0489\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0316 - val_loss: 0.0491\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0319 - val_loss: 0.0492\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0316 - val_loss: 0.0497\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0314 - val_loss: 0.0483\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0312 - val_loss: 0.0480\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0313 - val_loss: 0.0485\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0311 - val_loss: 0.0486\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0310 - val_loss: 0.0499\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0308 - val_loss: 0.0481\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0306 - val_loss: 0.0477\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0306 - val_loss: 0.0499\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0306 - val_loss: 0.0496\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0305 - val_loss: 0.0474\n",
      "Epoch 190/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0302 - val_loss: 0.0478\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0303 - val_loss: 0.0475\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0301 - val_loss: 0.0494\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0300 - val_loss: 0.0468\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0299 - val_loss: 0.0473\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0298 - val_loss: 0.0478\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0296 - val_loss: 0.0474\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0297 - val_loss: 0.0483\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0294 - val_loss: 0.0473\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0294 - val_loss: 0.0467\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0292 - val_loss: 0.0472\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0290 - val_loss: 0.0465\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0291 - val_loss: 0.0496\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0291 - val_loss: 0.0461\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0287 - val_loss: 0.0471\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0287 - val_loss: 0.0466\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0287 - val_loss: 0.0508\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0285 - val_loss: 0.0487\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 218us/step - loss: 0.0286 - val_loss: 0.0473\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0285 - val_loss: 0.0460\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0283 - val_loss: 0.0459\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0284 - val_loss: 0.0455\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0281 - val_loss: 0.0490\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0281 - val_loss: 0.0469\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0276 - val_loss: 0.0463\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0278 - val_loss: 0.0452\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0278 - val_loss: 0.0453\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0277 - val_loss: 0.0452\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0276 - val_loss: 0.0491\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0276 - val_loss: 0.0453\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0273 - val_loss: 0.0448\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0273 - val_loss: 0.0448\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0272 - val_loss: 0.0446\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0272 - val_loss: 0.0462\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 219us/step - loss: 0.0270 - val_loss: 0.0447\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0270 - val_loss: 0.0461\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0270 - val_loss: 0.0446\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0268 - val_loss: 0.0483\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0268 - val_loss: 0.0450\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0267 - val_loss: 0.0454\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0265 - val_loss: 0.0446\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0266 - val_loss: 0.0463\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0264 - val_loss: 0.0463\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0263 - val_loss: 0.0462\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0262 - val_loss: 0.0453\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0262 - val_loss: 0.0447\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0262 - val_loss: 0.0453\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0260 - val_loss: 0.0434\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0260 - val_loss: 0.0528\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0260 - val_loss: 0.0443\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0259 - val_loss: 0.0432\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0257 - val_loss: 0.0446\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0255 - val_loss: 0.0452\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0256 - val_loss: 0.0443\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0255 - val_loss: 0.0436\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0255 - val_loss: 0.0430\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0254 - val_loss: 0.0436\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0253 - val_loss: 0.0431\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0252 - val_loss: 0.0461\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0251 - val_loss: 0.0431\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0251 - val_loss: 0.0438\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 482us/step - loss: 1.8164 - val_loss: 0.7015\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.7504 - val_loss: 0.5567\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.6465 - val_loss: 0.5355\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.5863 - val_loss: 0.4688\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.5444 - val_loss: 0.4421\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.5085 - val_loss: 0.4051\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.4761 - val_loss: 0.4336\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.4472 - val_loss: 0.3767\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.4192 - val_loss: 0.3502\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.3940 - val_loss: 0.3309\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.3701 - val_loss: 0.3093\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.3514 - val_loss: 0.2957\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.3277 - val_loss: 0.2849\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.3110 - val_loss: 0.3118\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2936 - val_loss: 0.2712\n",
      "Epoch 16/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.2769 - val_loss: 0.2637\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.2637 - val_loss: 0.2377\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.2485 - val_loss: 0.2300\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.2376 - val_loss: 0.2180\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.2244 - val_loss: 0.2085\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.2133 - val_loss: 0.2006\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.2031 - val_loss: 0.1925\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.1923 - val_loss: 0.1887\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1835 - val_loss: 0.1787\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.1750 - val_loss: 0.1777\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.1654 - val_loss: 0.1751\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1583 - val_loss: 0.1731\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1519 - val_loss: 0.1584\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.1446 - val_loss: 0.1557\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.1386 - val_loss: 0.1522\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.1323 - val_loss: 0.1471\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1270 - val_loss: 0.1390\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.1224 - val_loss: 0.1322\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.1171 - val_loss: 0.1309\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.1137 - val_loss: 0.1438\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1092 - val_loss: 0.1233\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1055 - val_loss: 0.1178\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.1015 - val_loss: 0.1178\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0988 - val_loss: 0.1123\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0958 - val_loss: 0.1132\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0925 - val_loss: 0.1072\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0901 - val_loss: 0.1063\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0871 - val_loss: 0.1067\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0857 - val_loss: 0.1005\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0834 - val_loss: 0.1056\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0811 - val_loss: 0.1016\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0794 - val_loss: 0.0959\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0768 - val_loss: 0.0935\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0756 - val_loss: 0.0905\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0742 - val_loss: 0.0912\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0720 - val_loss: 0.0889\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0710 - val_loss: 0.0872\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0696 - val_loss: 0.0916\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0680 - val_loss: 0.0844\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0673 - val_loss: 0.0848\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0656 - val_loss: 0.0838\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0649 - val_loss: 0.0791\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.0639 - val_loss: 0.0793\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0623 - val_loss: 0.0826\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0618 - val_loss: 0.0787\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0607 - val_loss: 0.0833\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0596 - val_loss: 0.0816\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0590 - val_loss: 0.0750\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0581 - val_loss: 0.0753\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0572 - val_loss: 0.0751\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0562 - val_loss: 0.0715\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0554 - val_loss: 0.0706\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0546 - val_loss: 0.0741\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0544 - val_loss: 0.0810\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0537 - val_loss: 0.0694\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 225us/step - loss: 0.0528 - val_loss: 0.0691\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0525 - val_loss: 0.0762\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0513 - val_loss: 0.0684\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0508 - val_loss: 0.0699\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0506 - val_loss: 0.0734\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 228us/step - loss: 0.0497 - val_loss: 0.0652\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 220us/step - loss: 0.0492 - val_loss: 0.0649\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0485 - val_loss: 0.0642\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0485 - val_loss: 0.0650\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0476 - val_loss: 0.0667\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0474 - val_loss: 0.0644\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0469 - val_loss: 0.0686\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0464 - val_loss: 0.0619\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0454 - val_loss: 0.0672\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0454 - val_loss: 0.0628\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0450 - val_loss: 0.0623\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0449 - val_loss: 0.0685\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0444 - val_loss: 0.0617\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0438 - val_loss: 0.0612\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0431 - val_loss: 0.0592\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0427 - val_loss: 0.0610\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0426 - val_loss: 0.0582\n",
      "Epoch 93/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0422 - val_loss: 0.0611\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0420 - val_loss: 0.0585\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0416 - val_loss: 0.0600\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0411 - val_loss: 0.0593\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0409 - val_loss: 0.0587\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0406 - val_loss: 0.0553\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0402 - val_loss: 0.0563\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0400 - val_loss: 0.0555\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0398 - val_loss: 0.0585\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0396 - val_loss: 0.0576\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0390 - val_loss: 0.0544\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0391 - val_loss: 0.0611\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0388 - val_loss: 0.0537\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0385 - val_loss: 0.0559\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0382 - val_loss: 0.0530\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0378 - val_loss: 0.0547\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0377 - val_loss: 0.0543\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0374 - val_loss: 0.0545\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0369 - val_loss: 0.0529\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0369 - val_loss: 0.0559\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0369 - val_loss: 0.0655\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0364 - val_loss: 0.0570\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0362 - val_loss: 0.0514\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0358 - val_loss: 0.0587\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0357 - val_loss: 0.0529\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0355 - val_loss: 0.0525\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0355 - val_loss: 0.0544\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0355 - val_loss: 0.0510\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0349 - val_loss: 0.0506\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0346 - val_loss: 0.0499\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0343 - val_loss: 0.0512\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0344 - val_loss: 0.0500\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0340 - val_loss: 0.0505\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0336 - val_loss: 0.0501\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0337 - val_loss: 0.0551\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0336 - val_loss: 0.0494\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0334 - val_loss: 0.0487\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0328 - val_loss: 0.0532\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0328 - val_loss: 0.0489\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0330 - val_loss: 0.0515\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0329 - val_loss: 0.0486\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0325 - val_loss: 0.0494\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0324 - val_loss: 0.0487\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0320 - val_loss: 0.0479\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0321 - val_loss: 0.0480\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0318 - val_loss: 0.0485\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0317 - val_loss: 0.0486\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0315 - val_loss: 0.0528\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0315 - val_loss: 0.0478\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0313 - val_loss: 0.0513\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0311 - val_loss: 0.0471\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0311 - val_loss: 0.0470\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0308 - val_loss: 0.0487\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0305 - val_loss: 0.0481\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0305 - val_loss: 0.0467\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0307 - val_loss: 0.0493\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0301 - val_loss: 0.0476\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0301 - val_loss: 0.0474\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0299 - val_loss: 0.0461\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0298 - val_loss: 0.0492\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0297 - val_loss: 0.0480\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0295 - val_loss: 0.0457\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0294 - val_loss: 0.0458\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0292 - val_loss: 0.0461\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0295 - val_loss: 0.0459\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0293 - val_loss: 0.0452\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0288 - val_loss: 0.0455\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0288 - val_loss: 0.0454\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0288 - val_loss: 0.0458\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0286 - val_loss: 0.0449\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0285 - val_loss: 0.0461\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0283 - val_loss: 0.0490\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0284 - val_loss: 0.0440\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0281 - val_loss: 0.0493\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0282 - val_loss: 0.0444\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0280 - val_loss: 0.0441\n",
      "Epoch 169/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0276 - val_loss: 0.0447\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0278 - val_loss: 0.0443\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0276 - val_loss: 0.0457\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0274 - val_loss: 0.0445\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0274 - val_loss: 0.0444\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0273 - val_loss: 0.0437\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0270 - val_loss: 0.0435\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0270 - val_loss: 0.0446\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0270 - val_loss: 0.0441\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0267 - val_loss: 0.0449\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0268 - val_loss: 0.0431\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0268 - val_loss: 0.0424\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0264 - val_loss: 0.0431\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0266 - val_loss: 0.0432\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0263 - val_loss: 0.0439\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0263 - val_loss: 0.0468\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0261 - val_loss: 0.0453\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0261 - val_loss: 0.0449\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0260 - val_loss: 0.0435\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0259 - val_loss: 0.0442\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0258 - val_loss: 0.0440\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0257 - val_loss: 0.0421\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0256 - val_loss: 0.0422\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0255 - val_loss: 0.0435\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0256 - val_loss: 0.0443\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0252 - val_loss: 0.0424\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0252 - val_loss: 0.0424\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0251 - val_loss: 0.0454\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0250 - val_loss: 0.0431\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0251 - val_loss: 0.0425\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0250 - val_loss: 0.0430\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0248 - val_loss: 0.0411\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0248 - val_loss: 0.0437\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0245 - val_loss: 0.0414\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0245 - val_loss: 0.0438\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0243 - val_loss: 0.0433\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0245 - val_loss: 0.0417\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0242 - val_loss: 0.0414\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0244 - val_loss: 0.0416\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0241 - val_loss: 0.0411\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0240 - val_loss: 0.0438\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0243 - val_loss: 0.0420\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0238 - val_loss: 0.0427\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0239 - val_loss: 0.0408\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0237 - val_loss: 0.0438\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0237 - val_loss: 0.0422\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0236 - val_loss: 0.0416\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0236 - val_loss: 0.0412\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0235 - val_loss: 0.0399\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0233 - val_loss: 0.0407\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0233 - val_loss: 0.0417\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0234 - val_loss: 0.0401\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0233 - val_loss: 0.0398\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0231 - val_loss: 0.0404\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0230 - val_loss: 0.0414\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0230 - val_loss: 0.0418\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0229 - val_loss: 0.0399\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0228 - val_loss: 0.0397\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0230 - val_loss: 0.0395\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0228 - val_loss: 0.0395\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0225 - val_loss: 0.0412\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 208us/step - loss: 0.0226 - val_loss: 0.0414\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0225 - val_loss: 0.0396\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0225 - val_loss: 0.0410\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0224 - val_loss: 0.0413\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0223 - val_loss: 0.0389\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0222 - val_loss: 0.0404\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0223 - val_loss: 0.0404\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0221 - val_loss: 0.0410\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0221 - val_loss: 0.0391\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0220 - val_loss: 0.0396\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0219 - val_loss: 0.0388\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0218 - val_loss: 0.0408\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0219 - val_loss: 0.0401\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0219 - val_loss: 0.0387\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0217 - val_loss: 0.0446\n",
      "Epoch 245/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0218 - val_loss: 0.0412\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0216 - val_loss: 0.0440\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0215 - val_loss: 0.0382\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0214 - val_loss: 0.0439\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0215 - val_loss: 0.0393\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0213 - val_loss: 0.0404\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 1.6296 - val_loss: 0.7282\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.7036 - val_loss: 0.5587\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.6017 - val_loss: 0.4981\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.5408 - val_loss: 0.4186\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.4945 - val_loss: 0.3956\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.4551 - val_loss: 0.3690\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.4202 - val_loss: 0.3471\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.3898 - val_loss: 0.3318\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.3615 - val_loss: 0.3196\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.3356 - val_loss: 0.2871\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.3116 - val_loss: 0.2657\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.2927 - val_loss: 0.2678\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.2731 - val_loss: 0.2478\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.2571 - val_loss: 0.2599\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.2400 - val_loss: 0.2255\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.2256 - val_loss: 0.2074\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.2121 - val_loss: 0.1996\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1997 - val_loss: 0.1983\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1878 - val_loss: 0.1861\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.1772 - val_loss: 0.1761\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.1664 - val_loss: 0.1752\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1577 - val_loss: 0.1604\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.1480 - val_loss: 0.1520\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1415 - val_loss: 0.1481\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1333 - val_loss: 0.1559\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.1268 - val_loss: 0.1471\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1201 - val_loss: 0.1430\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1144 - val_loss: 0.1263\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1092 - val_loss: 0.1195\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.1040 - val_loss: 0.1171\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1000 - val_loss: 0.1156\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0959 - val_loss: 0.1077\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0917 - val_loss: 0.1078\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0890 - val_loss: 0.1036\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0856 - val_loss: 0.1013\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0828 - val_loss: 0.1141\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0801 - val_loss: 0.1031\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0783 - val_loss: 0.0931\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 213us/step - loss: 0.0756 - val_loss: 0.0928\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 207us/step - loss: 0.0738 - val_loss: 0.0921\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0725 - val_loss: 0.0872\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0707 - val_loss: 0.0843\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0686 - val_loss: 0.0847\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0668 - val_loss: 0.0820\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0654 - val_loss: 0.0827\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0633 - val_loss: 0.0781\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0621 - val_loss: 0.0766\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0609 - val_loss: 0.0775\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0596 - val_loss: 0.0806\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0592 - val_loss: 0.0742\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0579 - val_loss: 0.0743\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0566 - val_loss: 0.0729\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0551 - val_loss: 0.0735\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0544 - val_loss: 0.0723\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0532 - val_loss: 0.0702\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0524 - val_loss: 0.0704\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0519 - val_loss: 0.0713\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0510 - val_loss: 0.0708\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0503 - val_loss: 0.0784\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0497 - val_loss: 0.0740\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0485 - val_loss: 0.0707\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0483 - val_loss: 0.0639\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0478 - val_loss: 0.0650\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0469 - val_loss: 0.0643\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0465 - val_loss: 0.0690\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0459 - val_loss: 0.0762\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0458 - val_loss: 0.0613\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0443 - val_loss: 0.0626\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 216us/step - loss: 0.0435 - val_loss: 0.0694\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 217us/step - loss: 0.0439 - val_loss: 0.0622\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0431 - val_loss: 0.0606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0430 - val_loss: 0.0588\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0423 - val_loss: 0.0588\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0419 - val_loss: 0.0628\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0411 - val_loss: 0.0668\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0409 - val_loss: 0.0573\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0404 - val_loss: 0.0573\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0399 - val_loss: 0.0571\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0396 - val_loss: 0.0638\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0391 - val_loss: 0.0552\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0391 - val_loss: 0.0577\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0388 - val_loss: 0.0571\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0382 - val_loss: 0.0549\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0383 - val_loss: 0.0542\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0377 - val_loss: 0.0549\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0370 - val_loss: 0.0576\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0373 - val_loss: 0.0548\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0368 - val_loss: 0.0553\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0360 - val_loss: 0.0534\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0363 - val_loss: 0.0583\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0360 - val_loss: 0.0548\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0356 - val_loss: 0.0522\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0353 - val_loss: 0.0521\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0352 - val_loss: 0.0534\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0347 - val_loss: 0.0516\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0346 - val_loss: 0.0513\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0346 - val_loss: 0.0540\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0340 - val_loss: 0.0507\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0333 - val_loss: 0.0567\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0338 - val_loss: 0.0513\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0333 - val_loss: 0.0501\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0329 - val_loss: 0.0511\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0329 - val_loss: 0.0491\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0325 - val_loss: 0.0504\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0323 - val_loss: 0.0524\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0324 - val_loss: 0.0688\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0322 - val_loss: 0.0490\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0318 - val_loss: 0.0514\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0317 - val_loss: 0.0483\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0317 - val_loss: 0.0511\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0314 - val_loss: 0.0500\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0309 - val_loss: 0.0489\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0308 - val_loss: 0.0516\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0308 - val_loss: 0.0530\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0306 - val_loss: 0.0515\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0305 - val_loss: 0.0475\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0300 - val_loss: 0.0489\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0297 - val_loss: 0.0474\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0295 - val_loss: 0.0491\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0298 - val_loss: 0.0469\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0297 - val_loss: 0.0476\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0292 - val_loss: 0.0517\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0292 - val_loss: 0.0465\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0289 - val_loss: 0.0479\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0290 - val_loss: 0.0483\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0285 - val_loss: 0.0459\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0285 - val_loss: 0.0458\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0283 - val_loss: 0.0572\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0282 - val_loss: 0.0462\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0280 - val_loss: 0.0457\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0280 - val_loss: 0.0467\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0279 - val_loss: 0.0461\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0278 - val_loss: 0.0491\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0276 - val_loss: 0.0484\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0273 - val_loss: 0.0453\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0275 - val_loss: 0.0438\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0271 - val_loss: 0.0455\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0271 - val_loss: 0.0453\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0269 - val_loss: 0.0449\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0272 - val_loss: 0.0438\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0266 - val_loss: 0.0449\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0264 - val_loss: 0.0465\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0262 - val_loss: 0.0569\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0263 - val_loss: 0.0435\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0263 - val_loss: 0.0447\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0262 - val_loss: 0.0489\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0259 - val_loss: 0.0429\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0262 - val_loss: 0.0439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0258 - val_loss: 0.0432\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0256 - val_loss: 0.0424\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0256 - val_loss: 0.0423\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0255 - val_loss: 0.0427\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0250 - val_loss: 0.0446\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0255 - val_loss: 0.0434\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0248 - val_loss: 0.0424\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0249 - val_loss: 0.0447\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0248 - val_loss: 0.0429\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0251 - val_loss: 0.0429\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0246 - val_loss: 0.0415\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0246 - val_loss: 0.0453\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0246 - val_loss: 0.0429\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0244 - val_loss: 0.0524\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0244 - val_loss: 0.0415\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0245 - val_loss: 0.0469\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0243 - val_loss: 0.0409\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0240 - val_loss: 0.0419\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0240 - val_loss: 0.0427\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0239 - val_loss: 0.0421\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0240 - val_loss: 0.0413\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0235 - val_loss: 0.0457\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0236 - val_loss: 0.0416\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0234 - val_loss: 0.0427\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0233 - val_loss: 0.0425\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0231 - val_loss: 0.0394\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0235 - val_loss: 0.0404\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0230 - val_loss: 0.0410\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0230 - val_loss: 0.0398\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0232 - val_loss: 0.0398\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0230 - val_loss: 0.0402\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0227 - val_loss: 0.0422\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0232 - val_loss: 0.0467\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0228 - val_loss: 0.0404\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0228 - val_loss: 0.0414\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0227 - val_loss: 0.0428\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0225 - val_loss: 0.0411\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0222 - val_loss: 0.0401\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0225 - val_loss: 0.0407\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0227 - val_loss: 0.0414\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0222 - val_loss: 0.0395\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0219 - val_loss: 0.0409\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0220 - val_loss: 0.0389\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0219 - val_loss: 0.0483\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0220 - val_loss: 0.0428\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0219 - val_loss: 0.0449\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0221 - val_loss: 0.0397\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0216 - val_loss: 0.0413\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0219 - val_loss: 0.0411\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0215 - val_loss: 0.0400\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0219 - val_loss: 0.0404\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0214 - val_loss: 0.0686\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0216 - val_loss: 0.0416\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0215 - val_loss: 0.0401\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0214 - val_loss: 0.0397\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0213 - val_loss: 0.0387\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0214 - val_loss: 0.0392\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0214 - val_loss: 0.0497\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0211 - val_loss: 0.0386\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0210 - val_loss: 0.0388\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0209 - val_loss: 0.0389\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0207 - val_loss: 0.0392\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0206 - val_loss: 0.0382\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0206 - val_loss: 0.0391\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0207 - val_loss: 0.0377\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0206 - val_loss: 0.0400\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0206 - val_loss: 0.0379\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0204 - val_loss: 0.0393\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0205 - val_loss: 0.0374\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0204 - val_loss: 0.0390\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0201 - val_loss: 0.0384\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0204 - val_loss: 0.0388\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0203 - val_loss: 0.0370\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0201 - val_loss: 0.0382\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0200 - val_loss: 0.0401\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0203 - val_loss: 0.0395\n",
      "Epoch 225/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0200 - val_loss: 0.0388\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0200 - val_loss: 0.0369\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0198 - val_loss: 0.0383\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0200 - val_loss: 0.0386\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0199 - val_loss: 0.0372\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0199 - val_loss: 0.0472\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0198 - val_loss: 0.0367\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0198 - val_loss: 0.0378\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0200 - val_loss: 0.0373\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0197 - val_loss: 0.0376\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0196 - val_loss: 0.0371\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0192 - val_loss: 0.0394\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0193 - val_loss: 0.0369\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0195 - val_loss: 0.0418\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0194 - val_loss: 0.0413\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0192 - val_loss: 0.0385\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0191 - val_loss: 0.0367\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0189 - val_loss: 0.0369\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0192 - val_loss: 0.0383\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0191 - val_loss: 0.0366\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0192 - val_loss: 0.0362\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0190 - val_loss: 0.0363\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0189 - val_loss: 0.0370\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0187 - val_loss: 0.0367\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0189 - val_loss: 0.0359\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0187 - val_loss: 0.0358\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 425us/step - loss: 1.5845 - val_loss: 0.6026\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.6786 - val_loss: 0.4995\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.5866 - val_loss: 0.5877\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.5322 - val_loss: 0.4833\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.4883 - val_loss: 0.4413\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.4468 - val_loss: 0.3663\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.4108 - val_loss: 0.3752\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.3804 - val_loss: 0.3156\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.3476 - val_loss: 0.2987\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.3208 - val_loss: 0.2750\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2998 - val_loss: 0.2562\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.2745 - val_loss: 0.2404\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.2564 - val_loss: 0.2316\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2388 - val_loss: 0.2248\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2212 - val_loss: 0.2184\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2066 - val_loss: 0.1988\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1923 - val_loss: 0.1909\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1778 - val_loss: 0.2488\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1677 - val_loss: 0.1627\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1574 - val_loss: 0.1555\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1462 - val_loss: 0.1600\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1376 - val_loss: 0.1444\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1284 - val_loss: 0.1516\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1223 - val_loss: 0.1376\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1144 - val_loss: 0.1258\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1089 - val_loss: 0.1257\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1025 - val_loss: 0.1150\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0979 - val_loss: 0.1091\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0938 - val_loss: 0.1043\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0910 - val_loss: 0.1032\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0865 - val_loss: 0.0996\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0827 - val_loss: 0.1001\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0800 - val_loss: 0.0942\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0783 - val_loss: 0.0932\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0751 - val_loss: 0.0950\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0735 - val_loss: 0.0892\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0707 - val_loss: 0.0844\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0685 - val_loss: 0.0834\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0671 - val_loss: 0.0825\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0652 - val_loss: 0.0797\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0637 - val_loss: 0.0793\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0626 - val_loss: 0.0941\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0611 - val_loss: 0.0796\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0603 - val_loss: 0.0738\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0586 - val_loss: 0.0751\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0572 - val_loss: 0.0697\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0558 - val_loss: 0.0702\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0551 - val_loss: 0.0762\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0544 - val_loss: 0.0673\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0525 - val_loss: 0.0679\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0525 - val_loss: 0.0800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0513 - val_loss: 0.0675\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0504 - val_loss: 0.0653\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0493 - val_loss: 0.0672\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0486 - val_loss: 0.0667\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0482 - val_loss: 0.0641\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0478 - val_loss: 0.0624\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0473 - val_loss: 0.0696\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0461 - val_loss: 0.0618\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0456 - val_loss: 0.0685\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0445 - val_loss: 0.0593\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0443 - val_loss: 0.0602\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0437 - val_loss: 0.0583\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0428 - val_loss: 0.0612\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0426 - val_loss: 0.0592\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0416 - val_loss: 0.0575\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0417 - val_loss: 0.0545\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0409 - val_loss: 0.0572\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0404 - val_loss: 0.0722\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0401 - val_loss: 0.0556\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0399 - val_loss: 0.0532\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0390 - val_loss: 0.0633\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0390 - val_loss: 0.0561\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0386 - val_loss: 0.0599\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0378 - val_loss: 0.0607\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0380 - val_loss: 0.0522\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0373 - val_loss: 0.0571\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0371 - val_loss: 0.0532\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0368 - val_loss: 0.0501\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0363 - val_loss: 0.0569\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0358 - val_loss: 0.0509\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0356 - val_loss: 0.0501\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0350 - val_loss: 0.0496\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0351 - val_loss: 0.0485\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0343 - val_loss: 0.0603\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0344 - val_loss: 0.0519\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0339 - val_loss: 0.0494\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0339 - val_loss: 0.0587\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0338 - val_loss: 0.0474\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0330 - val_loss: 0.0494\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0325 - val_loss: 0.0540\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0332 - val_loss: 0.0485\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0325 - val_loss: 0.0463\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0322 - val_loss: 0.0710\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0322 - val_loss: 0.0552\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0320 - val_loss: 0.0508\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0312 - val_loss: 0.0504\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0313 - val_loss: 0.0568\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0311 - val_loss: 0.0457\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0308 - val_loss: 0.0478\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0303 - val_loss: 0.0482\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0302 - val_loss: 0.0589\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0304 - val_loss: 0.0459\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0299 - val_loss: 0.0453\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0295 - val_loss: 0.0470\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0294 - val_loss: 0.0447\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0295 - val_loss: 0.0489\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0291 - val_loss: 0.0454\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0290 - val_loss: 0.0466\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0290 - val_loss: 0.0437\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0292 - val_loss: 0.0464\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0280 - val_loss: 0.0451\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0278 - val_loss: 0.0466\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0282 - val_loss: 0.0448\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0281 - val_loss: 0.0440\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0276 - val_loss: 0.0423\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0272 - val_loss: 0.0428\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0275 - val_loss: 0.0458\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0273 - val_loss: 0.0434\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0272 - val_loss: 0.0422\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0271 - val_loss: 0.0434\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0268 - val_loss: 0.0468\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0266 - val_loss: 0.0424\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0267 - val_loss: 0.0425\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0266 - val_loss: 0.0538\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0263 - val_loss: 0.0445\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0260 - val_loss: 0.0429\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0260 - val_loss: 0.0413\n",
      "Epoch 129/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0259 - val_loss: 0.0450\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0259 - val_loss: 0.0441\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0254 - val_loss: 0.0401\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0253 - val_loss: 0.0483\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0253 - val_loss: 0.0410\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0250 - val_loss: 0.0439\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0248 - val_loss: 0.0442\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0246 - val_loss: 0.0422\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0247 - val_loss: 0.0476\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0247 - val_loss: 0.0405\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0245 - val_loss: 0.0472\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0244 - val_loss: 0.0446\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0244 - val_loss: 0.0419\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0239 - val_loss: 0.0393\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0241 - val_loss: 0.0400\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0241 - val_loss: 0.0400\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0245 - val_loss: 0.0450\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0242 - val_loss: 0.0438\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0233 - val_loss: 0.0433\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0237 - val_loss: 0.0393\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0235 - val_loss: 0.0396\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0232 - val_loss: 0.0472\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0232 - val_loss: 0.0396\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0231 - val_loss: 0.0400\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0230 - val_loss: 0.0402\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0233 - val_loss: 0.0439\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0232 - val_loss: 0.0431\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0229 - val_loss: 0.0413\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0226 - val_loss: 0.0393\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0224 - val_loss: 0.0451\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0222 - val_loss: 0.0388\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0221 - val_loss: 0.0447\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0225 - val_loss: 0.0420\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0222 - val_loss: 0.0375\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0224 - val_loss: 0.0394\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0222 - val_loss: 0.0379\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0221 - val_loss: 0.0404\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0218 - val_loss: 0.0382\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0218 - val_loss: 0.0445\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0217 - val_loss: 0.0385\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0214 - val_loss: 0.0587\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0222 - val_loss: 0.0380\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0214 - val_loss: 0.0432\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0213 - val_loss: 0.0391\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0213 - val_loss: 0.0396\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0212 - val_loss: 0.0381\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0212 - val_loss: 0.0429\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0210 - val_loss: 0.0369\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0212 - val_loss: 0.0432\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0211 - val_loss: 0.0379\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0205 - val_loss: 0.0371\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0207 - val_loss: 0.0384\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0206 - val_loss: 0.0372\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0207 - val_loss: 0.0389\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0203 - val_loss: 0.0388\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0206 - val_loss: 0.0368\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0208 - val_loss: 0.0364\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0203 - val_loss: 0.0426\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0203 - val_loss: 0.0372\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0202 - val_loss: 0.0396\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0201 - val_loss: 0.0402\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0202 - val_loss: 0.0415\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0201 - val_loss: 0.0381\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0197 - val_loss: 0.0385\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0195 - val_loss: 0.0376\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0198 - val_loss: 0.0381\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0196 - val_loss: 0.0357\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0195 - val_loss: 0.0369\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0196 - val_loss: 0.0365\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0195 - val_loss: 0.0353\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0195 - val_loss: 0.0351\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0194 - val_loss: 0.0377\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0192 - val_loss: 0.0403\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0194 - val_loss: 0.0392\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0192 - val_loss: 0.0364\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0193 - val_loss: 0.0435\n",
      "Epoch 205/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0192 - val_loss: 0.0348\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0190 - val_loss: 0.0486\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0190 - val_loss: 0.0372\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0189 - val_loss: 0.0347\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0186 - val_loss: 0.0359\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0188 - val_loss: 0.0363\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0188 - val_loss: 0.0350\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0187 - val_loss: 0.0346\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0185 - val_loss: 0.0353\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0187 - val_loss: 0.0348\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0186 - val_loss: 0.0378\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0185 - val_loss: 0.0348\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0182 - val_loss: 0.0421\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0186 - val_loss: 0.0350\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0182 - val_loss: 0.0356\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0181 - val_loss: 0.0363\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0180 - val_loss: 0.0345\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0183 - val_loss: 0.0345\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0178 - val_loss: 0.0356\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0180 - val_loss: 0.0344\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0179 - val_loss: 0.0358\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0180 - val_loss: 0.0352\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0178 - val_loss: 0.0378\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0181 - val_loss: 0.0385\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0179 - val_loss: 0.0341\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0179 - val_loss: 0.0339\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0177 - val_loss: 0.0347\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0177 - val_loss: 0.0337\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0176 - val_loss: 0.0349\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0174 - val_loss: 0.0362\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0175 - val_loss: 0.0354\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0175 - val_loss: 0.0336\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0176 - val_loss: 0.0357\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0173 - val_loss: 0.0344\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0176 - val_loss: 0.0364\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0174 - val_loss: 0.0362\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0172 - val_loss: 0.0343\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0173 - val_loss: 0.0330\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0173 - val_loss: 0.0393\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0173 - val_loss: 0.0375\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0172 - val_loss: 0.0350\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0171 - val_loss: 0.0357\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0171 - val_loss: 0.0334\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0169 - val_loss: 0.0341\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0170 - val_loss: 0.0340\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0171 - val_loss: 0.0339\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 430us/step - loss: 1.6152 - val_loss: 0.5764\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.6465 - val_loss: 0.5685\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.5552 - val_loss: 0.4224\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.4993 - val_loss: 0.4417\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.4418 - val_loss: 0.3739\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.3930 - val_loss: 0.4550\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.3548 - val_loss: 0.2924\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3215 - val_loss: 0.2725\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2922 - val_loss: 0.2614\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2624 - val_loss: 0.2477\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2429 - val_loss: 0.2266\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2203 - val_loss: 0.2056\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2036 - val_loss: 0.1910\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1876 - val_loss: 0.1837\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1753 - val_loss: 0.1955\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1617 - val_loss: 0.1579\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1501 - val_loss: 0.1514\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1401 - val_loss: 0.1438\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1299 - val_loss: 0.1331\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1219 - val_loss: 0.1281\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.1155 - val_loss: 0.1236\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1077 - val_loss: 0.1248\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1013 - val_loss: 0.1093\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0954 - val_loss: 0.1077\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0901 - val_loss: 0.1108\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0864 - val_loss: 0.0959\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0841 - val_loss: 0.1043\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0803 - val_loss: 0.0906\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0764 - val_loss: 0.0919\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0741 - val_loss: 0.1180\n",
      "Epoch 31/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0714 - val_loss: 0.0869\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0691 - val_loss: 0.0807\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0666 - val_loss: 0.1010\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0641 - val_loss: 0.0824\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0624 - val_loss: 0.0743\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0614 - val_loss: 0.0755\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0594 - val_loss: 0.0764\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0580 - val_loss: 0.0708\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0566 - val_loss: 0.0761\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0558 - val_loss: 0.0677\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0534 - val_loss: 0.0733\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0530 - val_loss: 0.0657\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0520 - val_loss: 0.0644\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0509 - val_loss: 0.0763\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0505 - val_loss: 0.0720\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0493 - val_loss: 0.0650\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0492 - val_loss: 0.0632\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0477 - val_loss: 0.0598\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0463 - val_loss: 0.0640\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0458 - val_loss: 0.0594\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0450 - val_loss: 0.0614\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0449 - val_loss: 0.0606\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0440 - val_loss: 0.0567\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0432 - val_loss: 0.0576\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0423 - val_loss: 0.0557\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0421 - val_loss: 0.0554\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0417 - val_loss: 0.0590\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0406 - val_loss: 0.0547\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0405 - val_loss: 0.0565\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0406 - val_loss: 0.0542\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0399 - val_loss: 0.0593\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0391 - val_loss: 0.0558\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0387 - val_loss: 0.0554\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0379 - val_loss: 0.0647\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0378 - val_loss: 0.0611\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0378 - val_loss: 0.0511\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0368 - val_loss: 0.0560\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0367 - val_loss: 0.0505\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0366 - val_loss: 0.0597\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0359 - val_loss: 0.0570\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0361 - val_loss: 0.0505\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0351 - val_loss: 0.0545\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0357 - val_loss: 0.0526\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0343 - val_loss: 0.0849\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0343 - val_loss: 0.0530\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0343 - val_loss: 0.0530\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0339 - val_loss: 0.0649\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0340 - val_loss: 0.0542\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0332 - val_loss: 0.0555\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0328 - val_loss: 0.0484\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0329 - val_loss: 0.0483\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0328 - val_loss: 0.0517\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0322 - val_loss: 0.0471\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0321 - val_loss: 0.0506\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0314 - val_loss: 0.0481\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0318 - val_loss: 0.0538\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0318 - val_loss: 0.0502\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0312 - val_loss: 0.0508\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0305 - val_loss: 0.0468\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0309 - val_loss: 0.0538\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0304 - val_loss: 0.0469\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0306 - val_loss: 0.0466\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0305 - val_loss: 0.0570\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0298 - val_loss: 0.0481\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0302 - val_loss: 0.0508\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0300 - val_loss: 0.0461\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0293 - val_loss: 0.0468\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0294 - val_loss: 0.0538\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0292 - val_loss: 0.0506\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0294 - val_loss: 0.0475\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0284 - val_loss: 0.0450\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0289 - val_loss: 0.0439\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0284 - val_loss: 0.0479\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0280 - val_loss: 0.0508\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0283 - val_loss: 0.0488\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0280 - val_loss: 0.0490\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0277 - val_loss: 0.0582\n",
      "Epoch 108/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0273 - val_loss: 0.0483\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0276 - val_loss: 0.0443\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0271 - val_loss: 0.0465\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0272 - val_loss: 0.0483\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0270 - val_loss: 0.0452\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0267 - val_loss: 0.0431\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0271 - val_loss: 0.0453\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0275 - val_loss: 0.0443\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0264 - val_loss: 0.0518\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0264 - val_loss: 0.0440\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0265 - val_loss: 0.0509\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0259 - val_loss: 0.0540\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0262 - val_loss: 0.0432\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0261 - val_loss: 0.0468\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0256 - val_loss: 0.0429\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0254 - val_loss: 0.0457\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0256 - val_loss: 0.0423\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0252 - val_loss: 0.0417\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0252 - val_loss: 0.0596\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0258 - val_loss: 0.0438\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0251 - val_loss: 0.0450\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0247 - val_loss: 0.0420\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0249 - val_loss: 0.0443\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0251 - val_loss: 0.0438\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0245 - val_loss: 0.0427\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0250 - val_loss: 0.0419\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0247 - val_loss: 0.0434\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0246 - val_loss: 0.0465\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0246 - val_loss: 0.0418\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0240 - val_loss: 0.0404\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0240 - val_loss: 0.0431\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0243 - val_loss: 0.0412\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0237 - val_loss: 0.0456\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0235 - val_loss: 0.0419\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0236 - val_loss: 0.0409\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0238 - val_loss: 0.0410\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0235 - val_loss: 0.0401\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0236 - val_loss: 0.0420\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0232 - val_loss: 0.0410\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0229 - val_loss: 0.0499\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0234 - val_loss: 0.0436\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0229 - val_loss: 0.0514\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0229 - val_loss: 0.0414\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0229 - val_loss: 0.0478\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0228 - val_loss: 0.0542\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0231 - val_loss: 0.0417\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0224 - val_loss: 0.0412\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0223 - val_loss: 0.0393\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0227 - val_loss: 0.0409\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0220 - val_loss: 0.0456\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0220 - val_loss: 0.0409\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0216 - val_loss: 0.0409\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0224 - val_loss: 0.0388\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0220 - val_loss: 0.0400\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0222 - val_loss: 0.0431\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0222 - val_loss: 0.0491\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0217 - val_loss: 0.0386\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0221 - val_loss: 0.0433\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0215 - val_loss: 0.0529\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0219 - val_loss: 0.0411\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0214 - val_loss: 0.0405\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0214 - val_loss: 0.0464\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0215 - val_loss: 0.0481\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0214 - val_loss: 0.0414\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0212 - val_loss: 0.0405\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0208 - val_loss: 0.0377\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0213 - val_loss: 0.0383\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0210 - val_loss: 0.0385\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0212 - val_loss: 0.0546\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0206 - val_loss: 0.0416\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0209 - val_loss: 0.0412\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0206 - val_loss: 0.0398\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0205 - val_loss: 0.0410\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0210 - val_loss: 0.0377\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0208 - val_loss: 0.0546\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0207 - val_loss: 0.0401\n",
      "Epoch 184/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0204 - val_loss: 0.0416\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0203 - val_loss: 0.0415\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0201 - val_loss: 0.0377\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0203 - val_loss: 0.0406\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0203 - val_loss: 0.0378\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0199 - val_loss: 0.0380\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0201 - val_loss: 0.0483\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0196 - val_loss: 0.0379\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0195 - val_loss: 0.0371\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0197 - val_loss: 0.0375\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0196 - val_loss: 0.0398\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0196 - val_loss: 0.0410\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0197 - val_loss: 0.0405\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0196 - val_loss: 0.0401\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0196 - val_loss: 0.0383\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0192 - val_loss: 0.0383\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0193 - val_loss: 0.0433\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0192 - val_loss: 0.0391\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0192 - val_loss: 0.0429\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0188 - val_loss: 0.0369\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0194 - val_loss: 0.0395\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0185 - val_loss: 0.0408\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0189 - val_loss: 0.0386\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0192 - val_loss: 0.0363\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0191 - val_loss: 0.0379\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0186 - val_loss: 0.0466\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0188 - val_loss: 0.0398\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0187 - val_loss: 0.0420\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0189 - val_loss: 0.0417\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0191 - val_loss: 0.0360\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0186 - val_loss: 0.0372\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0185 - val_loss: 0.0382\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0185 - val_loss: 0.0455\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0184 - val_loss: 0.0404\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0185 - val_loss: 0.0393\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0185 - val_loss: 0.0367\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0182 - val_loss: 0.0377\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0182 - val_loss: 0.0496\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0183 - val_loss: 0.0364\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0183 - val_loss: 0.0526\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0181 - val_loss: 0.0414\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0183 - val_loss: 0.0365\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0180 - val_loss: 0.0376\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0181 - val_loss: 0.0380\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0178 - val_loss: 0.0394\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0178 - val_loss: 0.0375\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0178 - val_loss: 0.0440\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0175 - val_loss: 0.0368\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0180 - val_loss: 0.0380\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0174 - val_loss: 0.0424\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0175 - val_loss: 0.0366\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0175 - val_loss: 0.0380\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0176 - val_loss: 0.0386\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0176 - val_loss: 0.0382\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0173 - val_loss: 0.0380\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0172 - val_loss: 0.0368\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0173 - val_loss: 0.0362\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0172 - val_loss: 0.0382\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0175 - val_loss: 0.0382\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0173 - val_loss: 0.0359\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0170 - val_loss: 0.0359\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0171 - val_loss: 0.0387\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0170 - val_loss: 0.0393\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 210us/step - loss: 0.0172 - val_loss: 0.0371\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0168 - val_loss: 0.0365\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0172 - val_loss: 0.0392\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0169 - val_loss: 0.0359\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 463us/step - loss: 1.4218 - val_loss: 0.5683\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.6292 - val_loss: 0.5157\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.5355 - val_loss: 0.4730\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.4620 - val_loss: 0.3726\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.4044 - val_loss: 0.3594\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.3597 - val_loss: 0.2855\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.3165 - val_loss: 0.2648\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.2889 - val_loss: 0.2476\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.2602 - val_loss: 0.3712\n",
      "Epoch 10/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2367 - val_loss: 0.2112\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2148 - val_loss: 0.1945\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1932 - val_loss: 0.1989\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1784 - val_loss: 0.1747\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1626 - val_loss: 0.1763\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.1490 - val_loss: 0.1492\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1362 - val_loss: 0.1673\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.1261 - val_loss: 0.1297\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1181 - val_loss: 0.1252\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1093 - val_loss: 0.1166\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1026 - val_loss: 0.1118\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0973 - val_loss: 0.1095\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0928 - val_loss: 0.1354\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0864 - val_loss: 0.1017\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0837 - val_loss: 0.0936\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0787 - val_loss: 0.0916\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0764 - val_loss: 0.1103\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0733 - val_loss: 0.1029\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0710 - val_loss: 0.0888\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0681 - val_loss: 0.0813\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0660 - val_loss: 0.0796\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0641 - val_loss: 0.0787\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0624 - val_loss: 0.0947\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0616 - val_loss: 0.0739\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0589 - val_loss: 0.0728\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0569 - val_loss: 0.0691\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0560 - val_loss: 0.0745\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0548 - val_loss: 0.0674\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0537 - val_loss: 0.0667\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0523 - val_loss: 0.0660\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0512 - val_loss: 0.0874\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0498 - val_loss: 0.0699\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0487 - val_loss: 0.0771\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0482 - val_loss: 0.0621\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0478 - val_loss: 0.0613\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0468 - val_loss: 0.0634\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0461 - val_loss: 0.0597\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0453 - val_loss: 0.0613\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0445 - val_loss: 0.0585\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0435 - val_loss: 0.0592\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0425 - val_loss: 0.0552\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0421 - val_loss: 0.0545\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 236us/step - loss: 0.0417 - val_loss: 0.0602\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.0408 - val_loss: 0.0648\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 222us/step - loss: 0.0406 - val_loss: 0.0520\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0398 - val_loss: 0.0521\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0396 - val_loss: 0.0598\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0394 - val_loss: 0.0524\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0381 - val_loss: 0.0624\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0379 - val_loss: 0.0530\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0379 - val_loss: 0.0577\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0378 - val_loss: 0.0538\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0371 - val_loss: 0.0495\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0365 - val_loss: 0.0548\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0357 - val_loss: 0.0600\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0356 - val_loss: 0.0490\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0348 - val_loss: 0.0497\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0347 - val_loss: 0.0555\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0350 - val_loss: 0.0574\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0348 - val_loss: 0.0542\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0338 - val_loss: 0.0473\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0337 - val_loss: 0.0475\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0335 - val_loss: 0.0476\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0334 - val_loss: 0.0475\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0330 - val_loss: 0.0582\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0326 - val_loss: 0.0463\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0316 - val_loss: 0.0539\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0322 - val_loss: 0.0460\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0319 - val_loss: 0.0544\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0315 - val_loss: 0.0569\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0306 - val_loss: 0.0462\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0306 - val_loss: 0.0444\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0314 - val_loss: 0.0506\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0306 - val_loss: 0.0643\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0307 - val_loss: 0.0436\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0307 - val_loss: 0.0436\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0299 - val_loss: 0.0507\n",
      "Epoch 87/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0290 - val_loss: 0.0495\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0295 - val_loss: 0.0452\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0289 - val_loss: 0.0470\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0293 - val_loss: 0.0517\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0290 - val_loss: 0.0462\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0282 - val_loss: 0.0473\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0286 - val_loss: 0.0465\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0285 - val_loss: 0.0487\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0284 - val_loss: 0.0525\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0278 - val_loss: 0.0604\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0281 - val_loss: 0.0650\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0279 - val_loss: 0.0492\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0279 - val_loss: 0.0444\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0273 - val_loss: 0.0506\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0272 - val_loss: 0.0423\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0266 - val_loss: 0.0415\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0265 - val_loss: 0.0454\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0263 - val_loss: 0.0407\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0264 - val_loss: 0.0433\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0261 - val_loss: 0.0470\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0271 - val_loss: 0.0507\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0265 - val_loss: 0.0475\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0260 - val_loss: 0.0476\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0257 - val_loss: 0.0515\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0260 - val_loss: 0.0420\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0258 - val_loss: 0.0409\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0253 - val_loss: 0.0425\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0256 - val_loss: 0.0439\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0256 - val_loss: 0.0453\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0251 - val_loss: 0.0403\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0249 - val_loss: 0.0415\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0248 - val_loss: 0.0447\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0251 - val_loss: 0.0411\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0245 - val_loss: 0.0422\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0245 - val_loss: 0.0410\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0247 - val_loss: 0.0412\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0240 - val_loss: 0.0472\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0240 - val_loss: 0.0395\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0241 - val_loss: 0.0400\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0233 - val_loss: 0.0428\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0239 - val_loss: 0.0404\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0237 - val_loss: 0.0457\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0233 - val_loss: 0.0404\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0238 - val_loss: 0.0435\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0230 - val_loss: 0.0405\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0227 - val_loss: 0.0394\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0233 - val_loss: 0.0394\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0228 - val_loss: 0.0410\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0226 - val_loss: 0.0391\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0227 - val_loss: 0.0383\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0229 - val_loss: 0.0397\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0225 - val_loss: 0.0481\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0226 - val_loss: 0.0381\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0224 - val_loss: 0.0385\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0229 - val_loss: 0.0393\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0224 - val_loss: 0.0437\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0223 - val_loss: 0.0400\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0221 - val_loss: 0.0400\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0223 - val_loss: 0.0424\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0216 - val_loss: 0.0407\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0221 - val_loss: 0.0466\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0216 - val_loss: 0.0383\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0220 - val_loss: 0.0382\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0216 - val_loss: 0.0501\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0213 - val_loss: 0.0390\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0215 - val_loss: 0.0429\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0213 - val_loss: 0.0384\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.0214 - val_loss: 0.0397\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0209 - val_loss: 0.0454\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0207 - val_loss: 0.0410\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0211 - val_loss: 0.0410\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0208 - val_loss: 0.0404\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0205 - val_loss: 0.0391\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0208 - val_loss: 0.0366\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0209 - val_loss: 0.0376\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0208 - val_loss: 0.0373\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0201 - val_loss: 0.0365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0202 - val_loss: 0.0377\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0202 - val_loss: 0.0384\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0208 - val_loss: 0.0381\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0204 - val_loss: 0.0443\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0202 - val_loss: 0.0361\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0199 - val_loss: 0.0439\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0200 - val_loss: 0.0356\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0196 - val_loss: 0.0363\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0198 - val_loss: 0.0364\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0195 - val_loss: 0.0410\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0192 - val_loss: 0.0396\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0201 - val_loss: 0.0421\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0195 - val_loss: 0.0411\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0197 - val_loss: 0.0397\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0195 - val_loss: 0.0377\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0194 - val_loss: 0.0404\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0194 - val_loss: 0.0366\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0195 - val_loss: 0.0517\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0191 - val_loss: 0.0620\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0194 - val_loss: 0.0383\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0193 - val_loss: 0.0368\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0207 - val_loss: 0.0401\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0190 - val_loss: 0.0363\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0192 - val_loss: 0.0352\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0191 - val_loss: 0.0361\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0189 - val_loss: 0.0416\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0190 - val_loss: 0.0416\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0191 - val_loss: 0.0363\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0187 - val_loss: 0.0363\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0183 - val_loss: 0.0392\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0186 - val_loss: 0.0389\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0186 - val_loss: 0.0355\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0185 - val_loss: 0.0512\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0183 - val_loss: 0.0354\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0183 - val_loss: 0.0375\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0181 - val_loss: 0.0422\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0184 - val_loss: 0.0399\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0178 - val_loss: 0.0347\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0181 - val_loss: 0.0365\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0178 - val_loss: 0.0369\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0181 - val_loss: 0.0427\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0179 - val_loss: 0.0341\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0174 - val_loss: 0.0367\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0178 - val_loss: 0.0339\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0174 - val_loss: 0.0338\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0175 - val_loss: 0.0342\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0177 - val_loss: 0.0679\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0180 - val_loss: 0.0332\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0175 - val_loss: 0.0374\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0174 - val_loss: 0.0369\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0174 - val_loss: 0.0360\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0175 - val_loss: 0.0335\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0173 - val_loss: 0.0344\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0173 - val_loss: 0.0376\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0174 - val_loss: 0.0359\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0171 - val_loss: 0.0361\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0174 - val_loss: 0.0347\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0172 - val_loss: 0.0394\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0172 - val_loss: 0.0355\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0172 - val_loss: 0.0364\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0170 - val_loss: 0.0357\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0168 - val_loss: 0.0333\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0172 - val_loss: 0.0339\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0168 - val_loss: 0.0334\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0167 - val_loss: 0.0342\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0169 - val_loss: 0.0340\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0167 - val_loss: 0.0355\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0167 - val_loss: 0.0341\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0167 - val_loss: 0.0357\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0170 - val_loss: 0.0349\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0168 - val_loss: 0.0346\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0164 - val_loss: 0.0366\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0164 - val_loss: 0.0344\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0163 - val_loss: 0.0364\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0165 - val_loss: 0.0339\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0163 - val_loss: 0.0466\n",
      "Epoch 240/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0163 - val_loss: 0.0356\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0164 - val_loss: 0.0402\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0162 - val_loss: 0.0326\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0161 - val_loss: 0.0338\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0162 - val_loss: 0.0354\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0160 - val_loss: 0.0352\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0160 - val_loss: 0.0341\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0162 - val_loss: 0.0369\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0157 - val_loss: 0.0344\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0156 - val_loss: 0.0402\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0159 - val_loss: 0.0368\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 433us/step - loss: 1.4789 - val_loss: 0.5753\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.6201 - val_loss: 0.4967\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.5239 - val_loss: 0.5068\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.4551 - val_loss: 0.4891\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.3929 - val_loss: 0.3144\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3508 - val_loss: 0.3548\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.3082 - val_loss: 0.2876\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2764 - val_loss: 0.2779\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.2424 - val_loss: 0.2229\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.2188 - val_loss: 0.2295\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.1961 - val_loss: 0.1831\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1793 - val_loss: 0.1657\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1590 - val_loss: 0.1631\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.1467 - val_loss: 0.1553\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1331 - val_loss: 0.1512\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1220 - val_loss: 0.1409\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1149 - val_loss: 0.1585\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1066 - val_loss: 0.1171\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0985 - val_loss: 0.1051\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0945 - val_loss: 0.1005\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0877 - val_loss: 0.1144\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0848 - val_loss: 0.1005\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0792 - val_loss: 0.1107\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0760 - val_loss: 0.0924\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0728 - val_loss: 0.0841\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0705 - val_loss: 0.0880\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0670 - val_loss: 0.1017\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0640 - val_loss: 0.0767\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0625 - val_loss: 0.0726\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0612 - val_loss: 0.0797\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0593 - val_loss: 0.0701\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0570 - val_loss: 0.0722\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0562 - val_loss: 0.0806\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0561 - val_loss: 0.0800\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0534 - val_loss: 0.0650\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0526 - val_loss: 0.0622\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0518 - val_loss: 0.0694\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0492 - val_loss: 0.0644\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0486 - val_loss: 0.0628\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0485 - val_loss: 0.0582\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0472 - val_loss: 0.0641\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0454 - val_loss: 0.0590\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0462 - val_loss: 0.0614\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0450 - val_loss: 0.0567\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0447 - val_loss: 0.0662\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0432 - val_loss: 0.0797\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0417 - val_loss: 0.0556\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0419 - val_loss: 0.0645\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0413 - val_loss: 0.0791\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0407 - val_loss: 0.0532\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0399 - val_loss: 0.0864\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0398 - val_loss: 0.0515\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0395 - val_loss: 0.0526\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0383 - val_loss: 0.0648\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0386 - val_loss: 0.0551\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0377 - val_loss: 0.0724\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0375 - val_loss: 0.0592\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0372 - val_loss: 0.0620\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0367 - val_loss: 0.0503\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0364 - val_loss: 0.0489\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0357 - val_loss: 0.0493\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0354 - val_loss: 0.0528\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0352 - val_loss: 0.0690\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0339 - val_loss: 0.0843\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0345 - val_loss: 0.0809\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0341 - val_loss: 0.0464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0344 - val_loss: 0.0531\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0335 - val_loss: 0.0476\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0329 - val_loss: 0.0467\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0323 - val_loss: 0.0464\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0325 - val_loss: 0.0543\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0329 - val_loss: 0.0464\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0325 - val_loss: 0.0513\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0321 - val_loss: 0.0472\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0321 - val_loss: 0.0545\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0306 - val_loss: 0.0490\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0308 - val_loss: 0.0461\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0308 - val_loss: 0.0428\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0310 - val_loss: 0.0490\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0305 - val_loss: 0.0437\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0298 - val_loss: 0.0493\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0296 - val_loss: 0.0691\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0298 - val_loss: 0.0423\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0290 - val_loss: 0.0478\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0292 - val_loss: 0.0451\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0295 - val_loss: 0.0415\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0284 - val_loss: 0.0437\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0285 - val_loss: 0.0478\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0291 - val_loss: 0.0423\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0286 - val_loss: 0.0526\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0290 - val_loss: 0.0427\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0277 - val_loss: 0.0421\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0280 - val_loss: 0.0448\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0278 - val_loss: 0.0484\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0271 - val_loss: 0.0424\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0273 - val_loss: 0.0613\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0275 - val_loss: 0.0438\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0264 - val_loss: 0.0428\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0263 - val_loss: 0.0426\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0270 - val_loss: 0.0454\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0258 - val_loss: 0.0553\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0267 - val_loss: 0.0562\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0262 - val_loss: 0.0486\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0259 - val_loss: 0.0430\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0259 - val_loss: 0.0478\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0257 - val_loss: 0.0410\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0251 - val_loss: 0.0408\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0255 - val_loss: 0.0445\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0248 - val_loss: 0.0420\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0261 - val_loss: 0.0426\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0246 - val_loss: 0.0437\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0244 - val_loss: 0.0484\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0250 - val_loss: 0.0412\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0248 - val_loss: 0.0404\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0252 - val_loss: 0.0400\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0239 - val_loss: 0.0457\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0234 - val_loss: 0.0414\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0244 - val_loss: 0.0395\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0235 - val_loss: 0.0416\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0234 - val_loss: 0.0401\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0238 - val_loss: 0.0462\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0232 - val_loss: 0.0400\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0236 - val_loss: 0.0385\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0240 - val_loss: 0.0440\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0229 - val_loss: 0.0532\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0226 - val_loss: 0.0429\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0236 - val_loss: 0.0404\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0228 - val_loss: 0.0422\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0225 - val_loss: 0.0488\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0218 - val_loss: 0.0391\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0227 - val_loss: 0.0382\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0219 - val_loss: 0.0392\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0228 - val_loss: 0.0427\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0226 - val_loss: 0.0385\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0220 - val_loss: 0.0403\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0225 - val_loss: 0.0397\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0221 - val_loss: 0.0368\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0220 - val_loss: 0.0413\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0218 - val_loss: 0.0452\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0218 - val_loss: 0.0389\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0208 - val_loss: 0.0640\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0215 - val_loss: 0.0386\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0212 - val_loss: 0.0439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0216 - val_loss: 0.0389\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0213 - val_loss: 0.0540\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0208 - val_loss: 0.0446\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0218 - val_loss: 0.0373\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0211 - val_loss: 0.0388\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0205 - val_loss: 0.0498\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0206 - val_loss: 0.0363\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0204 - val_loss: 0.0393\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0207 - val_loss: 0.0364\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0205 - val_loss: 0.0365\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0200 - val_loss: 0.0396\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0204 - val_loss: 0.0484\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0201 - val_loss: 0.0366\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0209 - val_loss: 0.0405\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0195 - val_loss: 0.0366\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0200 - val_loss: 0.0444\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0201 - val_loss: 0.0402\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0199 - val_loss: 0.0362\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0194 - val_loss: 0.0473\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0194 - val_loss: 0.0384\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0195 - val_loss: 0.0360\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0191 - val_loss: 0.0371\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0195 - val_loss: 0.0356\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0201 - val_loss: 0.0344\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0196 - val_loss: 0.0353\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0191 - val_loss: 0.0366\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0191 - val_loss: 0.0351\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0189 - val_loss: 0.0383\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0193 - val_loss: 0.0425\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0189 - val_loss: 0.0389\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0189 - val_loss: 0.0359\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0185 - val_loss: 0.0450\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0189 - val_loss: 0.0428\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0195 - val_loss: 0.0362\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0189 - val_loss: 0.0356\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0181 - val_loss: 0.0394\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0186 - val_loss: 0.0486\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0188 - val_loss: 0.0360\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0181 - val_loss: 0.0404\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0188 - val_loss: 0.0369\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0189 - val_loss: 0.0365\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0182 - val_loss: 0.0446\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0186 - val_loss: 0.0351\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0177 - val_loss: 0.0398\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0181 - val_loss: 0.0354\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0186 - val_loss: 0.0427\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0182 - val_loss: 0.0483\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0177 - val_loss: 0.0395\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0176 - val_loss: 0.0343\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0177 - val_loss: 0.0346\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0177 - val_loss: 0.0392\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0180 - val_loss: 0.0415\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0177 - val_loss: 0.0352\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0174 - val_loss: 0.0408\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0174 - val_loss: 0.0347\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0175 - val_loss: 0.0353\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0170 - val_loss: 0.0342\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0171 - val_loss: 0.0329\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0168 - val_loss: 0.0381\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0168 - val_loss: 0.0351\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0167 - val_loss: 0.0365\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0169 - val_loss: 0.0335\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0168 - val_loss: 0.0378\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0171 - val_loss: 0.0330\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0174 - val_loss: 0.0366\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0167 - val_loss: 0.0347\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0166 - val_loss: 0.0364\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0169 - val_loss: 0.0377\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0167 - val_loss: 0.0454\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0171 - val_loss: 0.0413\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0173 - val_loss: 0.0395\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0170 - val_loss: 0.0541\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0165 - val_loss: 0.0340\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0167 - val_loss: 0.0335\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0168 - val_loss: 0.0348\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0162 - val_loss: 0.0352\n",
      "Epoch 220/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0156 - val_loss: 0.0349\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0163 - val_loss: 0.0359\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0162 - val_loss: 0.0358\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0160 - val_loss: 0.0343\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0160 - val_loss: 0.0345\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0159 - val_loss: 0.0363\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0160 - val_loss: 0.0461\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0170 - val_loss: 0.0387\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0164 - val_loss: 0.0344\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0160 - val_loss: 0.0342\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0161 - val_loss: 0.0335\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0159 - val_loss: 0.0336\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0155 - val_loss: 0.0471\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0153 - val_loss: 0.0358\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0165 - val_loss: 0.0363\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0154 - val_loss: 0.0375\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0159 - val_loss: 0.0328\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0152 - val_loss: 0.0354\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0158 - val_loss: 0.0347\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0153 - val_loss: 0.0383\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0157 - val_loss: 0.0360\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0151 - val_loss: 0.0320\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0162 - val_loss: 0.0349\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0159 - val_loss: 0.0361\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0149 - val_loss: 0.0444\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0152 - val_loss: 0.0378\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0149 - val_loss: 0.0337\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0151 - val_loss: 0.0337\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0150 - val_loss: 0.0329\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0153 - val_loss: 0.0342\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0154 - val_loss: 0.0367\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 467us/step - loss: 1.5652 - val_loss: 0.7950\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.6054 - val_loss: 0.4862\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.4994 - val_loss: 0.3847\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.4201 - val_loss: 0.3234\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.3610 - val_loss: 0.3457\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.3153 - val_loss: 0.3066\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2797 - val_loss: 0.2489\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.2497 - val_loss: 0.2056\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.2186 - val_loss: 0.1873\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.1927 - val_loss: 0.2013\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.1700 - val_loss: 0.1756\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.1550 - val_loss: 0.1712\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.1411 - val_loss: 0.1578\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1294 - val_loss: 0.1344\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1169 - val_loss: 0.1491\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1095 - val_loss: 0.1139\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.1023 - val_loss: 0.1082\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0953 - val_loss: 0.1395\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0901 - val_loss: 0.1459\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0836 - val_loss: 0.1055\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0797 - val_loss: 0.0933\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0765 - val_loss: 0.0934\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0748 - val_loss: 0.0909\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0706 - val_loss: 0.1158\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0688 - val_loss: 0.0800\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0659 - val_loss: 0.0754\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0635 - val_loss: 0.0761\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0623 - val_loss: 0.0750\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0597 - val_loss: 0.0861\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 199us/step - loss: 0.0586 - val_loss: 0.0812\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0567 - val_loss: 0.0765\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0545 - val_loss: 0.0642\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0534 - val_loss: 0.0741\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0543 - val_loss: 0.0716\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0524 - val_loss: 0.0693\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0515 - val_loss: 0.0688\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0510 - val_loss: 0.0865\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0502 - val_loss: 0.0629\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0481 - val_loss: 0.0677\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0478 - val_loss: 0.0806\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0468 - val_loss: 0.0578\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0467 - val_loss: 0.0593\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0458 - val_loss: 0.0564\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0442 - val_loss: 0.0586\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0435 - val_loss: 0.0552\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0431 - val_loss: 0.0581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0416 - val_loss: 0.0600\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0419 - val_loss: 0.0586\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0426 - val_loss: 0.0558\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0414 - val_loss: 0.0546\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0421 - val_loss: 0.0530\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0408 - val_loss: 0.0640\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0405 - val_loss: 0.0584\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0386 - val_loss: 0.0668\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0382 - val_loss: 0.0605\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0394 - val_loss: 0.0682\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0373 - val_loss: 0.0493\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0372 - val_loss: 0.0497\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0381 - val_loss: 0.0588\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0369 - val_loss: 0.0575\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0346 - val_loss: 0.0670\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0356 - val_loss: 0.0725\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0365 - val_loss: 0.1348\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0358 - val_loss: 0.0504\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0347 - val_loss: 0.0571\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0342 - val_loss: 0.0504\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0342 - val_loss: 0.0676\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0330 - val_loss: 0.0625\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0330 - val_loss: 0.0487\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0330 - val_loss: 0.0685\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0325 - val_loss: 0.0486\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0327 - val_loss: 0.0474\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0331 - val_loss: 0.0516\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0317 - val_loss: 0.0449\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0319 - val_loss: 0.0468\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0312 - val_loss: 0.0513\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0321 - val_loss: 0.0464\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0310 - val_loss: 0.0476\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0303 - val_loss: 0.0533\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0301 - val_loss: 0.0539\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0302 - val_loss: 0.0463\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0306 - val_loss: 0.0518\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0299 - val_loss: 0.0458\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0295 - val_loss: 0.0439\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0296 - val_loss: 0.0488\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0290 - val_loss: 0.0492\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0290 - val_loss: 0.0555\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0289 - val_loss: 0.0463\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0284 - val_loss: 0.0511\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0283 - val_loss: 0.0433\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0282 - val_loss: 0.0488\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0275 - val_loss: 0.0526\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0292 - val_loss: 0.0473\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0299 - val_loss: 0.0413\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0275 - val_loss: 0.0486\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0276 - val_loss: 0.0423\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0269 - val_loss: 0.0442\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0267 - val_loss: 0.0452\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0271 - val_loss: 0.0433\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0269 - val_loss: 0.0505\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0264 - val_loss: 0.0448\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0266 - val_loss: 0.0432\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0271 - val_loss: 0.0453\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0258 - val_loss: 0.0410\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0259 - val_loss: 0.0467\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0255 - val_loss: 0.0605\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0263 - val_loss: 0.0407\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0249 - val_loss: 0.0438\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0263 - val_loss: 0.0614\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0251 - val_loss: 0.0433\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0255 - val_loss: 0.0411\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0243 - val_loss: 0.0548\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0237 - val_loss: 0.0477\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0248 - val_loss: 0.0415\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0239 - val_loss: 0.0415\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0241 - val_loss: 0.0445\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0243 - val_loss: 0.0635\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0244 - val_loss: 0.0440\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0249 - val_loss: 0.0415\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0241 - val_loss: 0.0617\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0237 - val_loss: 0.0543\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0238 - val_loss: 0.0431\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0235 - val_loss: 0.0417\n",
      "Epoch 124/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0239 - val_loss: 0.0554\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0240 - val_loss: 0.0410\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0231 - val_loss: 0.0419\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0228 - val_loss: 0.0400\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0223 - val_loss: 0.0390\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0220 - val_loss: 0.0425\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0222 - val_loss: 0.0404\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0231 - val_loss: 0.0407\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0219 - val_loss: 0.0537\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0221 - val_loss: 0.0443\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0220 - val_loss: 0.0411\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0228 - val_loss: 0.0384\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0219 - val_loss: 0.0392\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0220 - val_loss: 0.0368\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0216 - val_loss: 0.0395\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0216 - val_loss: 0.0550\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0217 - val_loss: 0.0416\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0221 - val_loss: 0.0409\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0212 - val_loss: 0.0437\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0203 - val_loss: 0.0488\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0209 - val_loss: 0.0550\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0208 - val_loss: 0.0750\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0210 - val_loss: 0.0432\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0210 - val_loss: 0.0367\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0210 - val_loss: 0.0544\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0214 - val_loss: 0.0567\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0206 - val_loss: 0.0446\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0200 - val_loss: 0.0539\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0205 - val_loss: 0.0444\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0205 - val_loss: 0.0558\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0204 - val_loss: 0.0362\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0202 - val_loss: 0.0363\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0208 - val_loss: 0.0361\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0201 - val_loss: 0.0411\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0199 - val_loss: 0.0362\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0194 - val_loss: 0.0405\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0201 - val_loss: 0.0358\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0195 - val_loss: 0.0437\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0196 - val_loss: 0.0493\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0203 - val_loss: 0.0433\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0194 - val_loss: 0.0475\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0195 - val_loss: 0.0409\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0195 - val_loss: 0.0390\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0190 - val_loss: 0.0381\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0198 - val_loss: 0.0368\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0192 - val_loss: 0.0351\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0194 - val_loss: 0.0373\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0183 - val_loss: 0.0519\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0190 - val_loss: 0.0364\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0194 - val_loss: 0.0352\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0182 - val_loss: 0.0381\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0189 - val_loss: 0.0351\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0177 - val_loss: 0.0353\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 206us/step - loss: 0.0182 - val_loss: 0.0392\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 201us/step - loss: 0.0178 - val_loss: 0.0375\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0178 - val_loss: 0.0341\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0180 - val_loss: 0.0414\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0182 - val_loss: 0.0384\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0184 - val_loss: 0.0414\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0183 - val_loss: 0.0406\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0178 - val_loss: 0.0396\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0179 - val_loss: 0.0355\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0179 - val_loss: 0.0516\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0179 - val_loss: 0.0356\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0173 - val_loss: 0.0405\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0176 - val_loss: 0.0356\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0181 - val_loss: 0.0449\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0177 - val_loss: 0.0468\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0176 - val_loss: 0.0360\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0176 - val_loss: 0.0383\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0169 - val_loss: 0.0345\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0173 - val_loss: 0.0337\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0172 - val_loss: 0.0368\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0169 - val_loss: 0.0410\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0172 - val_loss: 0.0351\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0175 - val_loss: 0.0353\n",
      "Epoch 200/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0174 - val_loss: 0.0345\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0181 - val_loss: 0.0377\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0175 - val_loss: 0.0329\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0166 - val_loss: 0.0354\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0172 - val_loss: 0.0410\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0169 - val_loss: 0.0371\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0174 - val_loss: 0.0339\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0170 - val_loss: 0.0401\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0170 - val_loss: 0.0430\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0168 - val_loss: 0.0427\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0161 - val_loss: 0.0355\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0164 - val_loss: 0.0326\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0163 - val_loss: 0.0348\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0163 - val_loss: 0.0393\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0165 - val_loss: 0.0411\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0164 - val_loss: 0.0397\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0160 - val_loss: 0.0359\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0159 - val_loss: 0.0337\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0158 - val_loss: 0.0342\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0166 - val_loss: 0.0376\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0160 - val_loss: 0.0354\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0161 - val_loss: 0.0424\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0166 - val_loss: 0.0478\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0164 - val_loss: 0.0340\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0164 - val_loss: 0.0323\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0158 - val_loss: 0.0361\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0164 - val_loss: 0.0452\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0159 - val_loss: 0.0368\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0160 - val_loss: 0.0353\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0156 - val_loss: 0.0321\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0158 - val_loss: 0.0466\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0150 - val_loss: 0.0337\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0157 - val_loss: 0.0393\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0155 - val_loss: 0.0395\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0151 - val_loss: 0.0363\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0154 - val_loss: 0.0336\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0153 - val_loss: 0.0389\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0155 - val_loss: 0.0331\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0149 - val_loss: 0.0324\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0159 - val_loss: 0.0379\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0151 - val_loss: 0.0336\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0152 - val_loss: 0.0325\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0154 - val_loss: 0.0430\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0152 - val_loss: 0.0332\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0151 - val_loss: 0.0432\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0158 - val_loss: 0.0352\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0155 - val_loss: 0.0368\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0155 - val_loss: 0.0342\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0148 - val_loss: 0.0338\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0154 - val_loss: 0.0500\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0148 - val_loss: 0.0372\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 1.5351 - val_loss: 0.5528\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.6033 - val_loss: 0.4159\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.4908 - val_loss: 0.3967\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.4185 - val_loss: 0.3206\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.3627 - val_loss: 0.2810\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.3175 - val_loss: 0.2496\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.2763 - val_loss: 0.2449\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.2469 - val_loss: 0.2380\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.2243 - val_loss: 0.1959\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.2013 - val_loss: 0.1987\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.1804 - val_loss: 0.1793\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.1625 - val_loss: 0.1585\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.1503 - val_loss: 0.1585\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1366 - val_loss: 0.1966\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.1266 - val_loss: 0.1385\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.1173 - val_loss: 0.1265\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.1095 - val_loss: 0.1365\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 211us/step - loss: 0.1011 - val_loss: 0.1080\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0960 - val_loss: 0.1796\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0885 - val_loss: 0.1020\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0849 - val_loss: 0.1125\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0828 - val_loss: 0.1096\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0778 - val_loss: 0.1815\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0742 - val_loss: 0.0857\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0708 - val_loss: 0.1051\n",
      "Epoch 26/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0688 - val_loss: 0.0952\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0676 - val_loss: 0.0767\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0626 - val_loss: 0.0729\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0625 - val_loss: 0.0833\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0597 - val_loss: 0.0716\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0581 - val_loss: 0.0942\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0578 - val_loss: 0.0691\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0542 - val_loss: 0.1030\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0546 - val_loss: 0.0743\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0528 - val_loss: 0.0672\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0519 - val_loss: 0.0704\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0500 - val_loss: 0.0724\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0508 - val_loss: 0.0663\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0478 - val_loss: 0.0656\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0480 - val_loss: 0.0600\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0454 - val_loss: 0.0834\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0457 - val_loss: 0.0619\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0444 - val_loss: 0.0640\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0452 - val_loss: 0.0950\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0436 - val_loss: 0.0991\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0435 - val_loss: 0.0576\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0438 - val_loss: 0.0809\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0422 - val_loss: 0.0548\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0405 - val_loss: 0.0534\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0399 - val_loss: 0.0529\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0382 - val_loss: 0.0639\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0396 - val_loss: 0.0849\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0380 - val_loss: 0.0591\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0366 - val_loss: 0.0585\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0381 - val_loss: 0.0530\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0365 - val_loss: 0.0507\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0368 - val_loss: 0.0601\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0369 - val_loss: 0.0554\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0354 - val_loss: 0.0526\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0345 - val_loss: 0.0595\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0344 - val_loss: 0.0516\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0336 - val_loss: 0.0492\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0342 - val_loss: 0.0520\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0327 - val_loss: 0.0485\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0330 - val_loss: 0.0494\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0330 - val_loss: 0.0491\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0328 - val_loss: 0.0549\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0326 - val_loss: 0.1113\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0307 - val_loss: 0.0654\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0323 - val_loss: 0.0553\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0321 - val_loss: 0.0466\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0305 - val_loss: 0.0577\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0304 - val_loss: 0.0492\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0298 - val_loss: 0.0514\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0308 - val_loss: 0.0490\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0293 - val_loss: 0.0449\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0291 - val_loss: 0.0508\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0296 - val_loss: 0.0592\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0290 - val_loss: 0.0460\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0284 - val_loss: 0.0477\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0294 - val_loss: 0.0534\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0297 - val_loss: 0.0688\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0282 - val_loss: 0.0471\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0274 - val_loss: 0.0476\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0282 - val_loss: 0.0455\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0261 - val_loss: 0.0440\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0274 - val_loss: 0.0532\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0272 - val_loss: 0.0497\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0272 - val_loss: 0.0467\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0259 - val_loss: 0.0561\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0259 - val_loss: 0.0434\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0260 - val_loss: 0.0455\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0262 - val_loss: 0.0572\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0253 - val_loss: 0.0417\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0253 - val_loss: 0.0419\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0257 - val_loss: 0.0501\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0255 - val_loss: 0.0426\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0249 - val_loss: 0.0420\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0246 - val_loss: 0.0462\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0243 - val_loss: 0.0416\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0243 - val_loss: 0.0549\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0243 - val_loss: 0.0416\n",
      "Epoch 103/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0246 - val_loss: 0.0425\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0235 - val_loss: 0.0661\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0239 - val_loss: 0.0770\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0240 - val_loss: 0.0460\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0235 - val_loss: 0.0432\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0236 - val_loss: 0.0443\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0232 - val_loss: 0.0418\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0232 - val_loss: 0.0414\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0241 - val_loss: 0.0449\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0232 - val_loss: 0.0490\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0225 - val_loss: 0.0419\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0223 - val_loss: 0.0412\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0227 - val_loss: 0.0388\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0224 - val_loss: 0.0533\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0218 - val_loss: 0.0430\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0215 - val_loss: 0.0421\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0220 - val_loss: 0.0414\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0216 - val_loss: 0.0450\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0219 - val_loss: 0.0422\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0215 - val_loss: 0.0432\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0210 - val_loss: 0.0474\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0225 - val_loss: 0.0509\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0216 - val_loss: 0.0419\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0211 - val_loss: 0.0404\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0210 - val_loss: 0.0418\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0205 - val_loss: 0.0460\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0216 - val_loss: 0.0425\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0207 - val_loss: 0.0412\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0208 - val_loss: 0.0402\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0212 - val_loss: 0.0389\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0206 - val_loss: 0.0426\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0210 - val_loss: 0.0459\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0202 - val_loss: 0.0469\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0202 - val_loss: 0.0419\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0201 - val_loss: 0.0420\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0198 - val_loss: 0.0434\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0196 - val_loss: 0.0423\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0196 - val_loss: 0.0526\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0201 - val_loss: 0.0801\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0193 - val_loss: 0.0381\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0194 - val_loss: 0.0381\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0195 - val_loss: 0.0424\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0190 - val_loss: 0.0387\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0201 - val_loss: 0.0427\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0192 - val_loss: 0.0507\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0191 - val_loss: 0.0384\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0192 - val_loss: 0.0528\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0191 - val_loss: 0.0393\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0190 - val_loss: 0.0430\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0187 - val_loss: 0.0390\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0189 - val_loss: 0.0389\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0185 - val_loss: 0.0457\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0183 - val_loss: 0.0415\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0190 - val_loss: 0.0407\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0189 - val_loss: 0.0437\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0188 - val_loss: 0.0409\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0187 - val_loss: 0.0470\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0186 - val_loss: 0.0397\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0180 - val_loss: 0.0375\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0182 - val_loss: 0.0380\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0181 - val_loss: 0.0399\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0187 - val_loss: 0.0383\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0177 - val_loss: 0.0393\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0183 - val_loss: 0.0380\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0178 - val_loss: 0.0432\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0178 - val_loss: 0.0484\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0182 - val_loss: 0.0373\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0183 - val_loss: 0.0397\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0181 - val_loss: 0.0379\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0179 - val_loss: 0.0369\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0172 - val_loss: 0.0433\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0172 - val_loss: 0.0430\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0175 - val_loss: 0.0456\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0172 - val_loss: 0.0518\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0173 - val_loss: 0.0364\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0172 - val_loss: 0.0415\n",
      "Epoch 179/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0178 - val_loss: 0.0428\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0172 - val_loss: 0.0382\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0177 - val_loss: 0.0372\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0175 - val_loss: 0.0399\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0167 - val_loss: 0.0369\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0166 - val_loss: 0.0401\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0163 - val_loss: 0.0411\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0165 - val_loss: 0.0504\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0168 - val_loss: 0.0404\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0164 - val_loss: 0.0420\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0163 - val_loss: 0.0400\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0169 - val_loss: 0.0385\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0161 - val_loss: 0.0424\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0165 - val_loss: 0.0432\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0168 - val_loss: 0.0411\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0166 - val_loss: 0.0412\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0159 - val_loss: 0.0383\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0162 - val_loss: 0.0356\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0156 - val_loss: 0.0449\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0165 - val_loss: 0.0399\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0169 - val_loss: 0.0355\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0158 - val_loss: 0.0354\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0158 - val_loss: 0.0456\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0162 - val_loss: 0.0477\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0159 - val_loss: 0.0362\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0159 - val_loss: 0.0400\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0154 - val_loss: 0.0378\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0157 - val_loss: 0.0371\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0156 - val_loss: 0.0369\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0156 - val_loss: 0.0389\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0154 - val_loss: 0.0424\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0154 - val_loss: 0.0391\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0155 - val_loss: 0.0357\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0155 - val_loss: 0.1428\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0175 - val_loss: 0.0354\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0155 - val_loss: 0.0599\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0159 - val_loss: 0.0415\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0151 - val_loss: 0.0442\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0149 - val_loss: 0.0386\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0155 - val_loss: 0.0353\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0150 - val_loss: 0.0359\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0156 - val_loss: 0.0344\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0151 - val_loss: 0.0379\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0147 - val_loss: 0.0352\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0150 - val_loss: 0.0343\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0150 - val_loss: 0.0398\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0155 - val_loss: 0.0379\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0146 - val_loss: 0.0357\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0150 - val_loss: 0.0367\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0148 - val_loss: 0.0370\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0153 - val_loss: 0.0359\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0150 - val_loss: 0.0349\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0147 - val_loss: 0.0373\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0148 - val_loss: 0.0361\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0149 - val_loss: 0.0424\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0151 - val_loss: 0.0386\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0144 - val_loss: 0.0381\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0147 - val_loss: 0.0411\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0142 - val_loss: 0.0357\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0140 - val_loss: 0.0351\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0144 - val_loss: 0.0353\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 198us/step - loss: 0.0141 - val_loss: 0.0371\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0145 - val_loss: 0.0341\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0147 - val_loss: 0.0397\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0144 - val_loss: 0.0364\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0146 - val_loss: 0.0347\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0142 - val_loss: 0.0392\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0138 - val_loss: 0.0343\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0141 - val_loss: 0.0377\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0144 - val_loss: 0.0363\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0143 - val_loss: 0.0417\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0141 - val_loss: 0.0471\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 432us/step - loss: 1.7554 - val_loss: 0.5452\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.6080 - val_loss: 0.5257\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.4814 - val_loss: 0.4262\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.4122 - val_loss: 0.3078\n",
      "Epoch 5/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.3566 - val_loss: 0.2638\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.2955 - val_loss: 0.2366\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2608 - val_loss: 0.2346\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.2210 - val_loss: 0.2521\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1995 - val_loss: 0.1888\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1781 - val_loss: 0.2669\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.1583 - val_loss: 0.1829\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1436 - val_loss: 0.1542\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1248 - val_loss: 0.1228\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1140 - val_loss: 0.1527\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.1057 - val_loss: 0.5373\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.1015 - val_loss: 0.1188\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0921 - val_loss: 0.1016\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0880 - val_loss: 0.0917\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0836 - val_loss: 0.0942\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0770 - val_loss: 0.0966\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0737 - val_loss: 0.0939\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0730 - val_loss: 0.0783\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0680 - val_loss: 0.0938\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0685 - val_loss: 0.1255\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0659 - val_loss: 0.0776\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0625 - val_loss: 0.0688\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0586 - val_loss: 0.0739\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0635 - val_loss: 0.1931\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0584 - val_loss: 0.0745\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0542 - val_loss: 0.0747\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0540 - val_loss: 0.0775\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0522 - val_loss: 0.0636\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0534 - val_loss: 0.0637\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0533 - val_loss: 0.0641\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0526 - val_loss: 0.0602\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0473 - val_loss: 0.0555\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0469 - val_loss: 0.0581\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0486 - val_loss: 0.1865\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0481 - val_loss: 0.0617\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0453 - val_loss: 0.0617\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0432 - val_loss: 0.0585\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0454 - val_loss: 0.0603\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0432 - val_loss: 0.0586\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0419 - val_loss: 0.0596\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 194us/step - loss: 0.0417 - val_loss: 0.0689\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0424 - val_loss: 0.0514\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0425 - val_loss: 0.0510\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0388 - val_loss: 0.0507\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0388 - val_loss: 0.0531\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0392 - val_loss: 0.0501\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0372 - val_loss: 0.0503\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0371 - val_loss: 0.0895\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0368 - val_loss: 0.0575\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0365 - val_loss: 0.0638\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0358 - val_loss: 0.1484\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0355 - val_loss: 0.0563\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0361 - val_loss: 0.0481\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0343 - val_loss: 0.0511\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0341 - val_loss: 0.0565\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0334 - val_loss: 0.0506\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0338 - val_loss: 0.0965\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0340 - val_loss: 0.0622\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0333 - val_loss: 0.0550\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 200us/step - loss: 0.0326 - val_loss: 0.0461\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0332 - val_loss: 0.0455\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0321 - val_loss: 0.0523\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0320 - val_loss: 0.0570\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0320 - val_loss: 0.0701\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0319 - val_loss: 0.0503\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0299 - val_loss: 0.0469\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0307 - val_loss: 0.0487\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0305 - val_loss: 0.0491\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0298 - val_loss: 0.0480\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0309 - val_loss: 0.0443\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0309 - val_loss: 0.0539\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0295 - val_loss: 0.0465\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0306 - val_loss: 0.0426\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0283 - val_loss: 0.0502\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0292 - val_loss: 0.0453\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0285 - val_loss: 0.0484\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0283 - val_loss: 0.0438\n",
      "Epoch 82/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0285 - val_loss: 0.0488\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0282 - val_loss: 0.0439\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0279 - val_loss: 0.0429\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0278 - val_loss: 0.0514\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0275 - val_loss: 0.0414\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0263 - val_loss: 0.0424\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0262 - val_loss: 0.0452\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 1s 148us/step - loss: 0.0279 - val_loss: 0.0449\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0265 - val_loss: 0.0492\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0271 - val_loss: 0.0433\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0254 - val_loss: 0.0455\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0263 - val_loss: 0.0409\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0257 - val_loss: 0.0406\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0258 - val_loss: 0.0405\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0252 - val_loss: 0.0527\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0266 - val_loss: 0.0473\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0254 - val_loss: 0.0460\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0245 - val_loss: 0.0542\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0247 - val_loss: 0.0396\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0250 - val_loss: 0.0587\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0259 - val_loss: 0.0411\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0244 - val_loss: 0.0396\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0234 - val_loss: 0.0525\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0237 - val_loss: 0.0442\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0230 - val_loss: 0.0429\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0228 - val_loss: 0.0434\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0246 - val_loss: 0.0400\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0235 - val_loss: 0.0405\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0228 - val_loss: 0.0388\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0239 - val_loss: 0.0378\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0232 - val_loss: 0.0446\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0227 - val_loss: 0.0378\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0237 - val_loss: 0.0378\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0227 - val_loss: 0.0388\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0229 - val_loss: 0.0391\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0221 - val_loss: 0.0474\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0228 - val_loss: 0.0370\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0228 - val_loss: 0.1181\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0227 - val_loss: 0.0449\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0218 - val_loss: 0.0403\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0226 - val_loss: 0.0421\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0216 - val_loss: 0.0435\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0212 - val_loss: 0.0374\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0239 - val_loss: 0.0372\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0219 - val_loss: 0.0385\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.0214 - val_loss: 0.0460\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0212 - val_loss: 0.0443\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0210 - val_loss: 0.0387\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0212 - val_loss: 0.0373\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0214 - val_loss: 0.0438\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0201 - val_loss: 0.0370\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0209 - val_loss: 0.0378\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0206 - val_loss: 0.0421\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0201 - val_loss: 0.0403\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0199 - val_loss: 0.0505\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0205 - val_loss: 0.0351\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0205 - val_loss: 0.0468\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0205 - val_loss: 0.0369\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 195us/step - loss: 0.0200 - val_loss: 0.0396\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0199 - val_loss: 0.0358\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0196 - val_loss: 0.0358\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0203 - val_loss: 0.0454\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0199 - val_loss: 0.0349\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0196 - val_loss: 0.0362\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0200 - val_loss: 0.0479\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0202 - val_loss: 0.0554\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0192 - val_loss: 0.0378\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0199 - val_loss: 0.0377\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0184 - val_loss: 0.0404\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0197 - val_loss: 0.0346\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0197 - val_loss: 0.0348\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0190 - val_loss: 0.0359\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0189 - val_loss: 0.0429\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0192 - val_loss: 0.0353\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0189 - val_loss: 0.0374\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0190 - val_loss: 0.0417\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0184 - val_loss: 0.0351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0186 - val_loss: 0.0406\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0186 - val_loss: 0.0402\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0183 - val_loss: 0.0362\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0179 - val_loss: 0.0475\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0178 - val_loss: 0.0356\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0192 - val_loss: 0.0457\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0181 - val_loss: 0.0427\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0187 - val_loss: 0.0425\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0181 - val_loss: 0.0364\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0179 - val_loss: 0.0353\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0188 - val_loss: 0.0366\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0177 - val_loss: 0.0369\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0172 - val_loss: 0.0475\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0176 - val_loss: 0.0350\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0181 - val_loss: 0.0381\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0177 - val_loss: 0.0365\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0180 - val_loss: 0.0369\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0180 - val_loss: 0.0356\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0174 - val_loss: 0.0367\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0172 - val_loss: 0.0365\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0172 - val_loss: 0.0363\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0173 - val_loss: 0.0375\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0171 - val_loss: 0.0419\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0172 - val_loss: 0.0395\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0172 - val_loss: 0.0437\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0171 - val_loss: 0.0333\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0175 - val_loss: 0.0344\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0167 - val_loss: 0.0401\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0170 - val_loss: 0.0350\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0166 - val_loss: 0.0378\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0163 - val_loss: 0.0375\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0168 - val_loss: 0.0346\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0164 - val_loss: 0.0418\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 1s 147us/step - loss: 0.0174 - val_loss: 0.0447\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0166 - val_loss: 0.0339\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0164 - val_loss: 0.0353\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0165 - val_loss: 0.0357\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0166 - val_loss: 0.0404\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0168 - val_loss: 0.0379\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0169 - val_loss: 0.0372\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0163 - val_loss: 0.0358\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0163 - val_loss: 0.0362\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0163 - val_loss: 0.0393\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0158 - val_loss: 0.0354\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0158 - val_loss: 0.0408\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0163 - val_loss: 0.0341\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0158 - val_loss: 0.0335\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0159 - val_loss: 0.0375\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0155 - val_loss: 0.0334\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0158 - val_loss: 0.0333\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 204us/step - loss: 0.0162 - val_loss: 0.0423\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0156 - val_loss: 0.0377\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0156 - val_loss: 0.0343\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0156 - val_loss: 0.0375\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0153 - val_loss: 0.0335\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0152 - val_loss: 0.0353\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0155 - val_loss: 0.0354\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0150 - val_loss: 0.0337\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0156 - val_loss: 0.0336\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0152 - val_loss: 0.0360\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0160 - val_loss: 0.0332\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0155 - val_loss: 0.0353\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0148 - val_loss: 0.0337\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0152 - val_loss: 0.0374\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0151 - val_loss: 0.0323\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0158 - val_loss: 0.0374\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0151 - val_loss: 0.0398\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0150 - val_loss: 0.0361\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0157 - val_loss: 0.0361\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0155 - val_loss: 0.0329\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0149 - val_loss: 0.0459\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0148 - val_loss: 0.0454\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0143 - val_loss: 0.0351\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0157 - val_loss: 0.0342\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0147 - val_loss: 0.0348\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0146 - val_loss: 0.0378\n",
      "Epoch 235/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0150 - val_loss: 0.0521\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0150 - val_loss: 0.0389\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0146 - val_loss: 0.0353\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0150 - val_loss: 0.0337\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0154 - val_loss: 0.0335\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 1s 149us/step - loss: 0.0146 - val_loss: 0.0326\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0148 - val_loss: 0.0387\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0151 - val_loss: 0.0350\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0148 - val_loss: 0.0336\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0144 - val_loss: 0.0333\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0144 - val_loss: 0.0339\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0147 - val_loss: 0.0366\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0142 - val_loss: 0.0329\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0142 - val_loss: 0.0329\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0142 - val_loss: 0.0379\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0141 - val_loss: 0.0369\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 431us/step - loss: 1.8992 - val_loss: 0.9482\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.5900 - val_loss: 0.5007\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.4518 - val_loss: 0.3429\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.3739 - val_loss: 0.3296\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.3116 - val_loss: 0.2883\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.2744 - val_loss: 0.2183\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.2393 - val_loss: 0.1978\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.2110 - val_loss: 0.2868\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.1920 - val_loss: 0.2176\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1692 - val_loss: 0.2544\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.1541 - val_loss: 0.1611\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.1353 - val_loss: 0.2317\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1296 - val_loss: 0.1275\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.1168 - val_loss: 0.1349\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.1095 - val_loss: 0.1082\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.1045 - val_loss: 0.1082\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0956 - val_loss: 0.2046\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0883 - val_loss: 0.0953\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0810 - val_loss: 0.1156\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0795 - val_loss: 0.1027\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0826 - val_loss: 0.0964\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0753 - val_loss: 0.0990\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0685 - val_loss: 0.1745\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0677 - val_loss: 0.0850\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0665 - val_loss: 0.0748\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0631 - val_loss: 0.1030\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0632 - val_loss: 0.1066\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0640 - val_loss: 0.0684\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0577 - val_loss: 0.0752\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0571 - val_loss: 0.0819\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.0564 - val_loss: 0.0841\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0555 - val_loss: 0.0674\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0512 - val_loss: 0.0641\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0521 - val_loss: 0.0649\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0500 - val_loss: 0.0652\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0495 - val_loss: 0.0800\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0458 - val_loss: 0.0635\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0458 - val_loss: 0.0626\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0456 - val_loss: 0.0621\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0464 - val_loss: 0.0600\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0435 - val_loss: 0.0964\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0461 - val_loss: 0.0562\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0440 - val_loss: 0.0641\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0441 - val_loss: 0.0615\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0420 - val_loss: 0.1014\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0421 - val_loss: 0.0880\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0430 - val_loss: 0.0671\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0393 - val_loss: 0.0515\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0394 - val_loss: 0.0762\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0402 - val_loss: 0.0555\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0399 - val_loss: 0.0583\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0393 - val_loss: 0.0580\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0368 - val_loss: 0.0516\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0378 - val_loss: 0.0610\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 203us/step - loss: 0.0358 - val_loss: 0.0490\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0370 - val_loss: 0.0669\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0344 - val_loss: 0.0506\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0352 - val_loss: 0.0541\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0367 - val_loss: 0.0529\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0350 - val_loss: 0.1233\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0339 - val_loss: 0.0502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0345 - val_loss: 0.0519\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0362 - val_loss: 0.0537\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0341 - val_loss: 0.0567\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0349 - val_loss: 0.0702\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0326 - val_loss: 0.0538\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0347 - val_loss: 0.0448\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0327 - val_loss: 0.0562\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0325 - val_loss: 0.0433\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0359 - val_loss: 0.0480\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0314 - val_loss: 0.0627\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0310 - val_loss: 0.0546\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0304 - val_loss: 0.0516\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0322 - val_loss: 0.0471\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0302 - val_loss: 0.0555\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0318 - val_loss: 0.0495\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0292 - val_loss: 0.0432\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0298 - val_loss: 0.0479\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0292 - val_loss: 0.0450\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0289 - val_loss: 0.0436\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0290 - val_loss: 0.0513\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0284 - val_loss: 0.0405\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0321 - val_loss: 0.0425\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0277 - val_loss: 0.0422\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0290 - val_loss: 0.0497\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0277 - val_loss: 0.0605\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0289 - val_loss: 0.0513\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0290 - val_loss: 0.0479\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0272 - val_loss: 0.0490\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0263 - val_loss: 0.0432\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0261 - val_loss: 0.0544\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0267 - val_loss: 0.0419\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0278 - val_loss: 0.0448\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0277 - val_loss: 0.0472\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0273 - val_loss: 0.0490\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0253 - val_loss: 0.0605\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0259 - val_loss: 0.0402\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0257 - val_loss: 0.0411\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0288 - val_loss: 0.0401\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0264 - val_loss: 0.0397\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0251 - val_loss: 0.0431\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0251 - val_loss: 0.0445\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0241 - val_loss: 0.0460\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0253 - val_loss: 0.0436\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0245 - val_loss: 0.0455\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0235 - val_loss: 0.0437\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0241 - val_loss: 0.0431\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0253 - val_loss: 0.0656\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0234 - val_loss: 0.0405\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0245 - val_loss: 0.0424\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0239 - val_loss: 0.0430\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0236 - val_loss: 0.0561\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0234 - val_loss: 0.0458\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0225 - val_loss: 0.0412\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0228 - val_loss: 0.0443\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0238 - val_loss: 0.0388\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0235 - val_loss: 0.0414\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0231 - val_loss: 0.0390\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0225 - val_loss: 0.0415\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0249 - val_loss: 0.0463\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0230 - val_loss: 0.0439\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0225 - val_loss: 0.0490\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0219 - val_loss: 0.0497\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0229 - val_loss: 0.0407\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0226 - val_loss: 0.0392\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0216 - val_loss: 0.0414\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0239 - val_loss: 0.0577\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0221 - val_loss: 0.0456\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0214 - val_loss: 0.0421\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0230 - val_loss: 0.0402\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 212us/step - loss: 0.0219 - val_loss: 0.0399\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0212 - val_loss: 0.0369\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0211 - val_loss: 0.0476\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0216 - val_loss: 0.0368\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0209 - val_loss: 0.0653\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0216 - val_loss: 0.0534\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0216 - val_loss: 0.0379\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0210 - val_loss: 0.0411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0213 - val_loss: 0.0479\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0206 - val_loss: 0.0417\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0214 - val_loss: 0.0488\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0212 - val_loss: 0.0486\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0203 - val_loss: 0.0606\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0204 - val_loss: 0.0386\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0198 - val_loss: 0.0483\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0200 - val_loss: 0.0361\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0196 - val_loss: 0.0412\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0193 - val_loss: 0.0376\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0197 - val_loss: 0.0381\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0199 - val_loss: 0.0423\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0205 - val_loss: 0.0459\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0202 - val_loss: 0.0366\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0196 - val_loss: 0.0468\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0196 - val_loss: 0.0414\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0194 - val_loss: 0.0494\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0199 - val_loss: 0.0399\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0186 - val_loss: 0.0361\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0199 - val_loss: 0.0369\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0193 - val_loss: 0.0366\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0187 - val_loss: 0.0359\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0182 - val_loss: 0.0387\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0193 - val_loss: 0.0347\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0183 - val_loss: 0.0438\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0189 - val_loss: 0.0352\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.0192 - val_loss: 0.0350\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0192 - val_loss: 0.0383\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0179 - val_loss: 0.0349\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0194 - val_loss: 0.0375\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0194 - val_loss: 0.0495\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0187 - val_loss: 0.0473\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0182 - val_loss: 0.0371\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0186 - val_loss: 0.0537\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0180 - val_loss: 0.0380\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0177 - val_loss: 0.0439\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0178 - val_loss: 0.0369\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0183 - val_loss: 0.0376\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0175 - val_loss: 0.0485\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0179 - val_loss: 0.0385\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0172 - val_loss: 0.0452\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0180 - val_loss: 0.0373\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0175 - val_loss: 0.0367\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0183 - val_loss: 0.0381\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0179 - val_loss: 0.0402\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0176 - val_loss: 0.0399\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0170 - val_loss: 0.0487\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0169 - val_loss: 0.0356\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0180 - val_loss: 0.0349\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0179 - val_loss: 0.0402\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0167 - val_loss: 0.0480\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0170 - val_loss: 0.0358\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0161 - val_loss: 0.0686\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0165 - val_loss: 0.0355\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0177 - val_loss: 0.0407\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0171 - val_loss: 0.0369\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0169 - val_loss: 0.0510\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0171 - val_loss: 0.0347\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0170 - val_loss: 0.0429\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0160 - val_loss: 0.0351\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0163 - val_loss: 0.0427\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0162 - val_loss: 0.0431\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0181 - val_loss: 0.0471\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0171 - val_loss: 0.0373\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0164 - val_loss: 0.0347\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0161 - val_loss: 0.0367\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0159 - val_loss: 0.0341\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0159 - val_loss: 0.0339\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0172 - val_loss: 0.0363\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0171 - val_loss: 0.0407\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0166 - val_loss: 0.0360\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0158 - val_loss: 0.0333\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0159 - val_loss: 0.0391\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0156 - val_loss: 0.0332\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0160 - val_loss: 0.0398\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0157 - val_loss: 0.0352\n",
      "Epoch 215/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0163 - val_loss: 0.0338\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0164 - val_loss: 0.0499\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0155 - val_loss: 0.0354\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0154 - val_loss: 0.0347\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0153 - val_loss: 0.0368\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0156 - val_loss: 0.0357\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 188us/step - loss: 0.0159 - val_loss: 0.0394\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 192us/step - loss: 0.0159 - val_loss: 0.0355\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0151 - val_loss: 0.0372\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0149 - val_loss: 0.0378\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0155 - val_loss: 0.0330\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0151 - val_loss: 0.0436\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0158 - val_loss: 0.0413\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0164 - val_loss: 0.0358\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0155 - val_loss: 0.0336\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0150 - val_loss: 0.0386\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0147 - val_loss: 0.0365\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0146 - val_loss: 0.0407\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0147 - val_loss: 0.0353\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0146 - val_loss: 0.0389\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0148 - val_loss: 0.0354\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0151 - val_loss: 0.0354\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0157 - val_loss: 0.0340\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0145 - val_loss: 0.0373\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0149 - val_loss: 0.0418\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0148 - val_loss: 0.0353\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0154 - val_loss: 0.0436\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0145 - val_loss: 0.0327\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0142 - val_loss: 0.0331\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0144 - val_loss: 0.0361\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0148 - val_loss: 0.0346\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0143 - val_loss: 0.0341\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0153 - val_loss: 0.0337\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0141 - val_loss: 0.0355\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0143 - val_loss: 0.0369\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0141 - val_loss: 0.0328\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 2.3362 - val_loss: 0.6980\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.6038 - val_loss: 0.4197\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.4490 - val_loss: 0.3167\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.3571 - val_loss: 0.2923\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.3049 - val_loss: 0.2310\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2569 - val_loss: 0.2168\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.2186 - val_loss: 0.1870\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.2024 - val_loss: 0.2005\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1834 - val_loss: 0.1608\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.1605 - val_loss: 0.1811\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.1414 - val_loss: 0.2183\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.1302 - val_loss: 0.1478\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.1187 - val_loss: 0.3180\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1090 - val_loss: 0.1362\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1049 - val_loss: 0.1050\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0973 - val_loss: 0.1608\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0928 - val_loss: 0.0961\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0879 - val_loss: 0.0857\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0835 - val_loss: 0.0959\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0757 - val_loss: 0.0799\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0746 - val_loss: 0.0859\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0723 - val_loss: 0.0754\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0707 - val_loss: 0.0771\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0708 - val_loss: 0.0860\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0691 - val_loss: 0.0884\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0629 - val_loss: 0.0768\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0600 - val_loss: 0.0833\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0658 - val_loss: 0.0796\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0583 - val_loss: 0.0754\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0602 - val_loss: 0.0630\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0554 - val_loss: 0.0637\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0558 - val_loss: 0.1104\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0502 - val_loss: 0.0634\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0490 - val_loss: 0.0608\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0485 - val_loss: 0.0813\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0462 - val_loss: 0.0670\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0485 - val_loss: 0.0616\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0467 - val_loss: 0.0759\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0463 - val_loss: 0.0731\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0453 - val_loss: 0.0627\n",
      "Epoch 41/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0454 - val_loss: 0.0807\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 186us/step - loss: 0.0464 - val_loss: 0.0679\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0423 - val_loss: 0.0751\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0429 - val_loss: 0.0630\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 209us/step - loss: 0.0412 - val_loss: 0.0681\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 214us/step - loss: 0.0416 - val_loss: 0.0589\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 202us/step - loss: 0.0403 - val_loss: 0.0659\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0412 - val_loss: 0.0752\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0422 - val_loss: 0.0755\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 193us/step - loss: 0.0381 - val_loss: 0.0817\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0395 - val_loss: 0.0537\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0364 - val_loss: 0.0532\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0364 - val_loss: 0.0576\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0361 - val_loss: 0.0589\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0362 - val_loss: 0.0798\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0356 - val_loss: 0.0813\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0357 - val_loss: 0.0668\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0335 - val_loss: 0.0671\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0352 - val_loss: 0.0551\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0329 - val_loss: 0.0561\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0354 - val_loss: 0.0542\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0341 - val_loss: 0.0511\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0343 - val_loss: 0.0522\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0318 - val_loss: 0.0530\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0321 - val_loss: 0.0672\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0303 - val_loss: 0.0525\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0306 - val_loss: 0.0607\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0295 - val_loss: 0.0545\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0300 - val_loss: 0.0639\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0323 - val_loss: 0.0521\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0294 - val_loss: 0.0550\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0304 - val_loss: 0.0517\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0300 - val_loss: 0.0653\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0310 - val_loss: 0.0529\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0288 - val_loss: 0.0602\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0303 - val_loss: 0.0793\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0291 - val_loss: 0.0675\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0279 - val_loss: 0.0555\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0303 - val_loss: 0.0583\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0284 - val_loss: 0.0516\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0273 - val_loss: 0.0523\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0280 - val_loss: 0.0614\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0269 - val_loss: 0.0514\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0272 - val_loss: 0.0558\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0284 - val_loss: 0.0634\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0268 - val_loss: 0.0509\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0269 - val_loss: 0.0506\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0276 - val_loss: 0.0497\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0257 - val_loss: 0.0511\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0261 - val_loss: 0.0590\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0266 - val_loss: 0.0610\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0270 - val_loss: 0.0661\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0260 - val_loss: 0.0499\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0246 - val_loss: 0.0558\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0255 - val_loss: 0.0573\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0248 - val_loss: 0.0541\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0253 - val_loss: 0.0482\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0265 - val_loss: 0.0537\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0254 - val_loss: 0.0652\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0246 - val_loss: 0.0547\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0248 - val_loss: 0.0902\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0245 - val_loss: 0.0501\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0262 - val_loss: 0.0542\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0236 - val_loss: 0.0620\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0243 - val_loss: 0.0490\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0234 - val_loss: 0.0573\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0236 - val_loss: 0.0476\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0242 - val_loss: 0.0504\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0221 - val_loss: 0.0475\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0227 - val_loss: 0.0509\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0267 - val_loss: 0.0542\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0226 - val_loss: 0.0484\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0228 - val_loss: 0.0473\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0237 - val_loss: 0.0468\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0228 - val_loss: 0.0529\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 1s 150us/step - loss: 0.0221 - val_loss: 0.0697\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0213 - val_loss: 0.0490\n",
      "Epoch 118/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0208 - val_loss: 0.0498\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0214 - val_loss: 0.0540\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0230 - val_loss: 0.0526\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0208 - val_loss: 0.0564\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0211 - val_loss: 0.0470\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0217 - val_loss: 0.0540\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0204 - val_loss: 0.0476\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0216 - val_loss: 0.0574\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0211 - val_loss: 0.0588\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0216 - val_loss: 0.0475\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0201 - val_loss: 0.0512\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0213 - val_loss: 0.0641\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0206 - val_loss: 0.0528\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0201 - val_loss: 0.0477\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0203 - val_loss: 0.0475\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0199 - val_loss: 0.0470\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0213 - val_loss: 0.0558\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0195 - val_loss: 0.0490\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0212 - val_loss: 0.0535\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0205 - val_loss: 0.0467\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.0200 - val_loss: 0.0514\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0190 - val_loss: 0.0490\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0187 - val_loss: 0.0536\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0207 - val_loss: 0.0536\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0201 - val_loss: 0.0519\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0198 - val_loss: 0.0496\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0192 - val_loss: 0.0523\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0186 - val_loss: 0.0597\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0187 - val_loss: 0.0486\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0188 - val_loss: 0.0501\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0179 - val_loss: 0.0538\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0183 - val_loss: 0.0468\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0188 - val_loss: 0.0582\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0191 - val_loss: 0.0480\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0196 - val_loss: 0.0490\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0182 - val_loss: 0.0532\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0183 - val_loss: 0.0497\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0188 - val_loss: 0.0529\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0187 - val_loss: 0.0454\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0184 - val_loss: 0.0490\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0178 - val_loss: 0.0528\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0171 - val_loss: 0.0488\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0186 - val_loss: 0.0525\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0174 - val_loss: 0.0507\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0171 - val_loss: 0.0465\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0179 - val_loss: 0.0510\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0176 - val_loss: 0.0539\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0173 - val_loss: 0.0478\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0173 - val_loss: 0.0456\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0169 - val_loss: 0.0477\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0181 - val_loss: 0.0584\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0176 - val_loss: 0.0475\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0164 - val_loss: 0.0493\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0170 - val_loss: 0.0458\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0166 - val_loss: 0.0460\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0176 - val_loss: 0.0525\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0169 - val_loss: 0.0454\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0181 - val_loss: 0.1549\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0191 - val_loss: 0.0477\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0169 - val_loss: 0.0458\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0169 - val_loss: 0.0444\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0169 - val_loss: 0.0672\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0169 - val_loss: 0.0464\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0172 - val_loss: 0.0436\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0175 - val_loss: 0.0464\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0164 - val_loss: 0.0589\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0166 - val_loss: 0.0555\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0172 - val_loss: 0.0465\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0161 - val_loss: 0.0439\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0164 - val_loss: 0.0453\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0160 - val_loss: 0.0464\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0170 - val_loss: 0.0523\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0167 - val_loss: 0.0452\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0166 - val_loss: 0.0510\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0158 - val_loss: 0.0515\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0160 - val_loss: 0.0445\n",
      "Epoch 194/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0160 - val_loss: 0.0490\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0155 - val_loss: 0.0475\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0154 - val_loss: 0.0732\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0162 - val_loss: 0.0714\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0155 - val_loss: 0.0587\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0166 - val_loss: 0.0463\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0165 - val_loss: 0.0618\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0165 - val_loss: 0.0491\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0160 - val_loss: 0.0749\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0158 - val_loss: 0.0444\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0170 - val_loss: 0.0495\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0166 - val_loss: 0.0457\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0154 - val_loss: 0.0476\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0151 - val_loss: 0.0461\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0156 - val_loss: 0.0539\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0154 - val_loss: 0.0463\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0150 - val_loss: 0.0464\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0157 - val_loss: 0.0448\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0150 - val_loss: 0.0687\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0163 - val_loss: 0.0443\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0146 - val_loss: 0.0496\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0145 - val_loss: 0.0475\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0142 - val_loss: 0.0514\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0149 - val_loss: 0.0457\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0152 - val_loss: 0.0634\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0152 - val_loss: 0.0513\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0152 - val_loss: 0.0437\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0156 - val_loss: 0.0518\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0149 - val_loss: 0.0478\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0153 - val_loss: 0.0484\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.0150 - val_loss: 0.0473\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0143 - val_loss: 0.0452\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0152 - val_loss: 0.0527\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0143 - val_loss: 0.0417\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0149 - val_loss: 0.0465\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0147 - val_loss: 0.0479\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0149 - val_loss: 0.0490\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0145 - val_loss: 0.0454\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0142 - val_loss: 0.0433\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0157 - val_loss: 0.0427\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0143 - val_loss: 0.0431\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0150 - val_loss: 0.0432\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0140 - val_loss: 0.0453\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0151 - val_loss: 0.0552\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0138 - val_loss: 0.0585\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0147 - val_loss: 0.0484\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 1s 152us/step - loss: 0.0139 - val_loss: 0.0447\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0139 - val_loss: 0.0460\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0148 - val_loss: 0.0426\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0139 - val_loss: 0.0593\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0139 - val_loss: 0.0441\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0132 - val_loss: 0.0594\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0137 - val_loss: 0.0451\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0139 - val_loss: 0.0429\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0140 - val_loss: 0.0432\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0134 - val_loss: 0.0409\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0134 - val_loss: 0.0506\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 456us/step - loss: 3.5145 - val_loss: 0.5497\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.5626 - val_loss: 0.3554\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 1s 151us/step - loss: 0.4283 - val_loss: 0.3052\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.3254 - val_loss: 0.2889\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.2646 - val_loss: 0.2775\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 196us/step - loss: 0.2238 - val_loss: 0.2091\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 205us/step - loss: 0.1904 - val_loss: 0.2082\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 191us/step - loss: 0.1660 - val_loss: 0.1852\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 197us/step - loss: 0.1495 - val_loss: 0.1529\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.1303 - val_loss: 0.1226\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 189us/step - loss: 0.1174 - val_loss: 0.1212\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.1073 - val_loss: 0.1090\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0997 - val_loss: 0.0989\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0974 - val_loss: 0.0951\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0889 - val_loss: 0.1148\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0855 - val_loss: 0.2478\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0807 - val_loss: 0.0837\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0792 - val_loss: 0.0852\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0733 - val_loss: 0.1108\n",
      "Epoch 20/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0750 - val_loss: 0.0748\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0692 - val_loss: 0.1149\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0680 - val_loss: 0.1024\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0645 - val_loss: 0.1004\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0629 - val_loss: 0.0919\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0607 - val_loss: 0.0676\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0593 - val_loss: 0.0920\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0562 - val_loss: 0.0648\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0580 - val_loss: 0.1345\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0512 - val_loss: 0.0919\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0527 - val_loss: 0.1201\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0497 - val_loss: 0.0728\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0492 - val_loss: 0.0671\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0509 - val_loss: 0.0559\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0502 - val_loss: 0.0548\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0471 - val_loss: 0.0717\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0477 - val_loss: 0.0537\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0466 - val_loss: 0.0694\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0424 - val_loss: 0.0669\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0442 - val_loss: 0.0680\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0439 - val_loss: 0.1646\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0427 - val_loss: 0.0528\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0424 - val_loss: 0.0520\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0435 - val_loss: 0.0505\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0396 - val_loss: 0.0561\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0381 - val_loss: 0.0502\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0383 - val_loss: 0.0767\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0414 - val_loss: 0.0847\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0364 - val_loss: 0.0849\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0370 - val_loss: 0.0471\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0355 - val_loss: 0.0783\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0365 - val_loss: 0.0492\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0358 - val_loss: 0.0888\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0345 - val_loss: 0.0556\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0355 - val_loss: 0.0606\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0342 - val_loss: 0.0578\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0323 - val_loss: 0.0494\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0321 - val_loss: 0.0520\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0330 - val_loss: 0.0598\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0342 - val_loss: 0.0557\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0327 - val_loss: 0.0491\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0304 - val_loss: 0.0608\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0320 - val_loss: 0.0439\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0313 - val_loss: 0.0531\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0337 - val_loss: 0.0548\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0318 - val_loss: 0.0438\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0302 - val_loss: 0.0447\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0305 - val_loss: 0.0492\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0311 - val_loss: 0.0640\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0290 - val_loss: 0.0686\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0284 - val_loss: 0.0526\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0291 - val_loss: 0.0507\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0286 - val_loss: 0.0456\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0296 - val_loss: 0.0450\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0301 - val_loss: 0.0530\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0290 - val_loss: 0.0459\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0279 - val_loss: 0.0561\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0280 - val_loss: 0.0528\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0270 - val_loss: 0.0463\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0267 - val_loss: 0.0510\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0277 - val_loss: 0.0450\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0270 - val_loss: 0.0880\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0268 - val_loss: 0.0448\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0265 - val_loss: 0.0503\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0254 - val_loss: 0.0400\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0263 - val_loss: 0.0403\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0264 - val_loss: 0.0572\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0249 - val_loss: 0.0580\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 178us/step - loss: 0.0252 - val_loss: 0.0514\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0250 - val_loss: 0.0449\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0263 - val_loss: 0.0784\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0246 - val_loss: 0.0518\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0243 - val_loss: 0.0424\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0243 - val_loss: 0.0488\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0257 - val_loss: 0.0445\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0246 - val_loss: 0.0426\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0233 - val_loss: 0.0440\n",
      "Epoch 97/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0247 - val_loss: 0.0711\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0231 - val_loss: 0.0444\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0231 - val_loss: 0.0455\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0233 - val_loss: 0.0444\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0246 - val_loss: 0.0572\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0232 - val_loss: 0.0466\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0233 - val_loss: 0.0433\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0230 - val_loss: 0.0440\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0227 - val_loss: 0.0503\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0222 - val_loss: 0.0422\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0228 - val_loss: 0.0466\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0240 - val_loss: 0.0490\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0213 - val_loss: 0.0407\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0234 - val_loss: 0.0820\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0220 - val_loss: 0.0507\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0207 - val_loss: 0.0458\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0205 - val_loss: 0.0483\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0216 - val_loss: 0.0421\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0209 - val_loss: 0.0447\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0209 - val_loss: 0.0658\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0217 - val_loss: 0.0488\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0206 - val_loss: 0.0383\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0209 - val_loss: 0.0416\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0203 - val_loss: 0.0448\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0206 - val_loss: 0.0454\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0211 - val_loss: 0.0488\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0216 - val_loss: 0.0622\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0215 - val_loss: 0.0444\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0196 - val_loss: 0.0399\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0197 - val_loss: 0.0456\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0208 - val_loss: 0.0460\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0208 - val_loss: 0.0475\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0195 - val_loss: 0.0452\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0197 - val_loss: 0.0425\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0199 - val_loss: 0.0458\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0197 - val_loss: 0.0460\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0190 - val_loss: 0.0415\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0192 - val_loss: 0.0402\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0185 - val_loss: 0.0399\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0186 - val_loss: 0.0451\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0180 - val_loss: 0.0422\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0186 - val_loss: 0.0402\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0185 - val_loss: 0.0571\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0206 - val_loss: 0.0451\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0194 - val_loss: 0.0442\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0179 - val_loss: 0.0376\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0188 - val_loss: 0.0397\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0192 - val_loss: 0.0477\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0182 - val_loss: 0.0429\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0172 - val_loss: 0.0422\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0188 - val_loss: 0.0602\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0183 - val_loss: 0.0464\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0182 - val_loss: 0.0403\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0187 - val_loss: 0.0393\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0171 - val_loss: 0.0385\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0177 - val_loss: 0.0387\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0172 - val_loss: 0.0387\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0174 - val_loss: 0.0424\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0177 - val_loss: 0.0403\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0172 - val_loss: 0.0374\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0173 - val_loss: 0.0376\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0168 - val_loss: 0.0375\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0172 - val_loss: 0.0737\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0192 - val_loss: 0.0371\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0163 - val_loss: 0.0424\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0163 - val_loss: 0.0432\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0174 - val_loss: 0.0376\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0163 - val_loss: 0.0752\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0171 - val_loss: 0.0527\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0166 - val_loss: 0.0407\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0167 - val_loss: 0.0713\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0163 - val_loss: 0.0551\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0170 - val_loss: 0.0390\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0165 - val_loss: 0.0375\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0158 - val_loss: 0.0426\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0159 - val_loss: 0.0380\n",
      "Epoch 173/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0164 - val_loss: 0.0459\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0164 - val_loss: 0.0381\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0167 - val_loss: 0.0662\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0165 - val_loss: 0.0377\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0157 - val_loss: 0.0410\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0149 - val_loss: 0.0450\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0162 - val_loss: 0.0457\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0164 - val_loss: 0.0384\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0163 - val_loss: 0.0460\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0171 - val_loss: 0.0414\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0155 - val_loss: 0.0368\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0160 - val_loss: 0.0432\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0152 - val_loss: 0.0383\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0160 - val_loss: 0.0485\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0155 - val_loss: 0.0507\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0158 - val_loss: 0.0398\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0155 - val_loss: 0.0399\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0155 - val_loss: 0.0414\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0153 - val_loss: 0.0376\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0164 - val_loss: 0.0396\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0153 - val_loss: 0.0384\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0159 - val_loss: 0.0502\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0154 - val_loss: 0.0369\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0153 - val_loss: 0.0390\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0153 - val_loss: 0.0379\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0146 - val_loss: 0.0380\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0149 - val_loss: 0.0361\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0147 - val_loss: 0.0646\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0149 - val_loss: 0.0367\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0141 - val_loss: 0.0421\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0157 - val_loss: 0.0415\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0151 - val_loss: 0.0401\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0154 - val_loss: 0.0386\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0146 - val_loss: 0.0361\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0146 - val_loss: 0.0488\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0151 - val_loss: 0.0364\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0157 - val_loss: 0.0375\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0144 - val_loss: 0.0392\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0155 - val_loss: 0.0574\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0152 - val_loss: 0.0385\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0139 - val_loss: 0.0393\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0145 - val_loss: 0.0465\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0151 - val_loss: 0.0384\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0142 - val_loss: 0.0382\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0147 - val_loss: 0.0359\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0150 - val_loss: 0.0377\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0140 - val_loss: 0.0405\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0138 - val_loss: 0.0411\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0139 - val_loss: 0.0420\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0147 - val_loss: 0.0432\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0146 - val_loss: 0.0439\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0143 - val_loss: 0.0625\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0142 - val_loss: 0.0390\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0146 - val_loss: 0.0366\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0135 - val_loss: 0.0393\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0142 - val_loss: 0.0381\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0141 - val_loss: 0.0371\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0143 - val_loss: 0.0433\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0142 - val_loss: 0.0362\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0139 - val_loss: 0.0384\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0136 - val_loss: 0.0421\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0139 - val_loss: 0.0368\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0136 - val_loss: 0.0370\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0149 - val_loss: 0.0400\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0136 - val_loss: 0.0534\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0139 - val_loss: 0.0402\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0136 - val_loss: 0.0385\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0137 - val_loss: 0.0524\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0133 - val_loss: 0.0376\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0139 - val_loss: 0.0367\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0136 - val_loss: 0.0365\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0133 - val_loss: 0.0377\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0132 - val_loss: 0.0382\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0132 - val_loss: 0.0358\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0134 - val_loss: 0.0422\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0135 - val_loss: 0.0387\n",
      "Epoch 249/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0129 - val_loss: 0.0377\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0124 - val_loss: 0.0391\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 438us/step - loss: 6.6540 - val_loss: 0.4138\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.3676 - val_loss: 0.3262\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2853 - val_loss: 0.3460\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.2289 - val_loss: 0.2282\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1951 - val_loss: 0.1946\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1732 - val_loss: 0.1736\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1605 - val_loss: 0.2158\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1457 - val_loss: 0.1382\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1330 - val_loss: 0.1365\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1268 - val_loss: 0.1272\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1179 - val_loss: 0.1441\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1148 - val_loss: 0.1975\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1073 - val_loss: 0.1294\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0975 - val_loss: 0.2659\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0994 - val_loss: 0.1178\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0887 - val_loss: 0.0939\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0895 - val_loss: 0.0919\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0821 - val_loss: 0.0857\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0792 - val_loss: 0.1340\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0768 - val_loss: 0.0827\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0688 - val_loss: 0.0889\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0700 - val_loss: 0.0813\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0684 - val_loss: 0.0900\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0690 - val_loss: 0.1047\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0665 - val_loss: 0.1032\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0623 - val_loss: 0.0670\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0595 - val_loss: 0.0990\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0564 - val_loss: 0.1555\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0582 - val_loss: 0.0783\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0528 - val_loss: 0.0782\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0509 - val_loss: 0.0814\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0510 - val_loss: 0.0595\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0491 - val_loss: 0.0854\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0474 - val_loss: 0.0604\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0483 - val_loss: 0.0594\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0472 - val_loss: 0.0659\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0502 - val_loss: 0.0615\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0450 - val_loss: 0.0637\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0424 - val_loss: 0.0617\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0436 - val_loss: 0.1063\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0441 - val_loss: 0.0572\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0416 - val_loss: 0.0563\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0469 - val_loss: 0.0556\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0435 - val_loss: 0.0531\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0445 - val_loss: 0.0686\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0390 - val_loss: 0.0642\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0385 - val_loss: 0.0667\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0402 - val_loss: 0.0527\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0399 - val_loss: 0.0497\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0365 - val_loss: 0.0719\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0406 - val_loss: 0.0529\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0368 - val_loss: 0.0687\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0352 - val_loss: 0.0604\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0380 - val_loss: 0.0563\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0348 - val_loss: 0.0768\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0336 - val_loss: 0.0466\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0335 - val_loss: 0.0475\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0332 - val_loss: 0.0510\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0325 - val_loss: 0.0555\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0338 - val_loss: 0.0467\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0341 - val_loss: 0.0473\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0345 - val_loss: 0.0723\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0303 - val_loss: 0.0728\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0305 - val_loss: 0.0472\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0333 - val_loss: 0.0871\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0304 - val_loss: 0.0755\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0301 - val_loss: 0.0485\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0314 - val_loss: 0.0473\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0302 - val_loss: 0.0485\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0287 - val_loss: 0.0551\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0285 - val_loss: 0.0455\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0307 - val_loss: 0.0432\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0287 - val_loss: 0.0491\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0285 - val_loss: 0.0440\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0301 - val_loss: 0.0643\n",
      "Epoch 76/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0288 - val_loss: 0.0441\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0289 - val_loss: 0.0443\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0268 - val_loss: 0.0463\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0303 - val_loss: 0.0635\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0271 - val_loss: 0.0430\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0271 - val_loss: 0.0470\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0270 - val_loss: 0.0437\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0264 - val_loss: 0.0483\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0257 - val_loss: 0.0433\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0261 - val_loss: 0.0526\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0270 - val_loss: 0.0464\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0326 - val_loss: 0.0576\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0273 - val_loss: 0.0530\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0303 - val_loss: 0.0449\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0252 - val_loss: 0.0459\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0254 - val_loss: 0.0507\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0246 - val_loss: 0.0442\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0236 - val_loss: 0.0491\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0244 - val_loss: 0.0423\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0233 - val_loss: 0.0421\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0259 - val_loss: 0.0883\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0262 - val_loss: 0.0440\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0236 - val_loss: 0.0767\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0245 - val_loss: 0.0410\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0229 - val_loss: 0.0526\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0226 - val_loss: 0.0400\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0265 - val_loss: 0.0402\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0228 - val_loss: 0.0422\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0227 - val_loss: 0.0515\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0242 - val_loss: 0.0412\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0229 - val_loss: 0.0491\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0226 - val_loss: 0.0390\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0227 - val_loss: 0.0570\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0222 - val_loss: 0.0639\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0224 - val_loss: 0.0415\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0222 - val_loss: 0.0503\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0217 - val_loss: 0.0373\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0231 - val_loss: 0.0461\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0226 - val_loss: 0.0397\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0223 - val_loss: 0.0387\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0220 - val_loss: 0.0388\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0209 - val_loss: 0.0382\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0218 - val_loss: 0.0389\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0211 - val_loss: 0.0383\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0205 - val_loss: 0.0399\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0208 - val_loss: 0.0419\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0224 - val_loss: 0.0841\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0211 - val_loss: 0.0370\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0210 - val_loss: 0.0406\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0205 - val_loss: 0.0371\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0203 - val_loss: 0.0463\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0201 - val_loss: 0.0389\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0215 - val_loss: 0.0416\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0221 - val_loss: 0.0399\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0207 - val_loss: 0.0492\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0196 - val_loss: 0.0458\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0199 - val_loss: 0.0432\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0198 - val_loss: 0.1163\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0215 - val_loss: 0.0378\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0200 - val_loss: 0.0369\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0203 - val_loss: 0.0368\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0206 - val_loss: 0.0419\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0195 - val_loss: 0.0472\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0198 - val_loss: 0.0367\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0187 - val_loss: 0.0363\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0200 - val_loss: 0.0390\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0196 - val_loss: 0.0372\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0202 - val_loss: 0.1091\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0187 - val_loss: 0.0411\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0199 - val_loss: 0.0366\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0187 - val_loss: 0.0370\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0195 - val_loss: 0.0852\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0187 - val_loss: 0.0371\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0195 - val_loss: 0.0472\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0207 - val_loss: 0.0409\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0185 - val_loss: 0.0456\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0191 - val_loss: 0.0447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0190 - val_loss: 0.0357\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0190 - val_loss: 0.0441\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0178 - val_loss: 0.0413\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0185 - val_loss: 0.0349\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0178 - val_loss: 0.0488\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0183 - val_loss: 0.0379\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0187 - val_loss: 0.0397\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0178 - val_loss: 0.0540\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0185 - val_loss: 0.0381\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0187 - val_loss: 0.0379\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0171 - val_loss: 0.0373\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0173 - val_loss: 0.0413\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0174 - val_loss: 0.0415\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0180 - val_loss: 0.0354\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 1s 153us/step - loss: 0.0175 - val_loss: 0.0362\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0205 - val_loss: 0.0408\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0180 - val_loss: 0.0471\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0173 - val_loss: 0.0575\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0170 - val_loss: 0.0366\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0182 - val_loss: 0.0516\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0173 - val_loss: 0.0361\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0166 - val_loss: 0.0403\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0184 - val_loss: 0.0363\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0167 - val_loss: 0.0533\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0169 - val_loss: 0.0408\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0166 - val_loss: 0.0373\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0177 - val_loss: 0.0366\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0183 - val_loss: 0.0406\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0170 - val_loss: 0.0345\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0166 - val_loss: 0.0352\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0177 - val_loss: 0.0393\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0172 - val_loss: 0.0458\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0164 - val_loss: 0.1309\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0175 - val_loss: 0.0378\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0162 - val_loss: 0.1758\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0262 - val_loss: 0.0362\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0165 - val_loss: 0.0347\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0169 - val_loss: 0.1024\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0165 - val_loss: 0.0361\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0164 - val_loss: 0.0470\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0171 - val_loss: 0.0388\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0174 - val_loss: 0.0372\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0157 - val_loss: 0.0505\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0172 - val_loss: 0.0415\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0156 - val_loss: 0.0382\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0156 - val_loss: 0.0370\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0161 - val_loss: 0.0340\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0162 - val_loss: 0.0393\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0162 - val_loss: 0.0412\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0158 - val_loss: 0.0340\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0148 - val_loss: 0.0342\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0156 - val_loss: 0.0380\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0158 - val_loss: 0.0335\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0162 - val_loss: 0.0361\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0152 - val_loss: 0.0340\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0152 - val_loss: 0.0335\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0157 - val_loss: 0.0347\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0148 - val_loss: 0.0374\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0158 - val_loss: 0.0372\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0156 - val_loss: 0.0367\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0147 - val_loss: 0.0361\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0156 - val_loss: 0.0346\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0159 - val_loss: 0.0389\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0149 - val_loss: 0.0365\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0152 - val_loss: 0.0366\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0156 - val_loss: 0.0504\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0149 - val_loss: 0.0508\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0152 - val_loss: 0.0329\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0145 - val_loss: 0.0371\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0150 - val_loss: 0.0338\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0152 - val_loss: 0.0657\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0153 - val_loss: 0.0361\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0150 - val_loss: 0.0351\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0144 - val_loss: 0.0359\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0146 - val_loss: 0.0330\n",
      "Epoch 229/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0144 - val_loss: 0.0358\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0142 - val_loss: 0.0343\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0146 - val_loss: 0.0345\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0142 - val_loss: 0.0354\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0144 - val_loss: 0.0336\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0146 - val_loss: 0.0357\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0140 - val_loss: 0.0333\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0149 - val_loss: 0.0372\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0161 - val_loss: 0.0371\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0141 - val_loss: 0.0329\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0145 - val_loss: 0.0385\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0137 - val_loss: 0.0331\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0138 - val_loss: 0.0333\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0141 - val_loss: 0.0362\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0141 - val_loss: 0.0333\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0154 - val_loss: 0.0372\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0155 - val_loss: 0.0361\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0135 - val_loss: 0.0340\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0175 - val_loss: 0.0430\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0144 - val_loss: 0.0344\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0133 - val_loss: 0.0357\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0140 - val_loss: 0.0377\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 441us/step - loss: 4.1273 - val_loss: 0.6644\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.4902 - val_loss: 0.3519\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3427 - val_loss: 0.2656\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2846 - val_loss: 0.2098\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2424 - val_loss: 0.2477\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1975 - val_loss: 0.1535\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1765 - val_loss: 0.1348\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1521 - val_loss: 0.2727\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1407 - val_loss: 0.2288\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1224 - val_loss: 0.1306\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1099 - val_loss: 0.1659\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0989 - val_loss: 0.0882\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0940 - val_loss: 0.0883\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0917 - val_loss: 0.0798\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0858 - val_loss: 0.1531\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0811 - val_loss: 0.0840\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0852 - val_loss: 0.5842\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0812 - val_loss: 0.2703\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0763 - val_loss: 0.0675\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0784 - val_loss: 0.0675\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0662 - val_loss: 0.0680\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0655 - val_loss: 0.0651\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0587 - val_loss: 0.0859\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0651 - val_loss: 0.0627\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0634 - val_loss: 0.1301\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0576 - val_loss: 0.0814\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0583 - val_loss: 0.0580\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0570 - val_loss: 0.0604\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0596 - val_loss: 0.1009\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0504 - val_loss: 0.0567\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0560 - val_loss: 0.0542\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0503 - val_loss: 0.0691\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0486 - val_loss: 0.0531\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0475 - val_loss: 0.0668\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0486 - val_loss: 0.1305\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0476 - val_loss: 0.1047\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0413 - val_loss: 0.0882\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0465 - val_loss: 0.0752\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0434 - val_loss: 0.0459\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0478 - val_loss: 0.0455\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0447 - val_loss: 0.0853\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0436 - val_loss: 0.0577\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0454 - val_loss: 0.0542\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0419 - val_loss: 0.0526\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0404 - val_loss: 0.1512\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0404 - val_loss: 0.0493\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0426 - val_loss: 0.0478\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0389 - val_loss: 0.0526\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0412 - val_loss: 0.0433\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0388 - val_loss: 0.0875\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0391 - val_loss: 0.0662\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0420 - val_loss: 0.0656\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0370 - val_loss: 0.0647\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0399 - val_loss: 0.0512\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0335 - val_loss: 0.0475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0373 - val_loss: 0.0691\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0372 - val_loss: 0.0586\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0353 - val_loss: 0.0406\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0338 - val_loss: 0.0532\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0343 - val_loss: 0.0404\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0374 - val_loss: 0.0451\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0335 - val_loss: 0.0514\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0336 - val_loss: 0.0460\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0347 - val_loss: 0.0402\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0328 - val_loss: 0.0418\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0374 - val_loss: 0.0398\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0339 - val_loss: 0.0407\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0307 - val_loss: 0.0509\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0334 - val_loss: 0.0453\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0333 - val_loss: 0.0398\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0350 - val_loss: 0.0460\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0325 - val_loss: 0.0465\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0300 - val_loss: 0.0391\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0303 - val_loss: 0.0425\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0307 - val_loss: 0.0412\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0308 - val_loss: 0.0379\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0298 - val_loss: 0.0361\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0289 - val_loss: 0.0512\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0277 - val_loss: 0.0483\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0326 - val_loss: 0.0471\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0309 - val_loss: 0.0491\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0281 - val_loss: 0.0413\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0283 - val_loss: 0.0568\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0276 - val_loss: 0.0381\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0284 - val_loss: 0.0548\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0300 - val_loss: 0.0376\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0282 - val_loss: 0.0430\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0255 - val_loss: 0.0407\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0265 - val_loss: 0.0403\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0283 - val_loss: 0.0648\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0265 - val_loss: 0.0618\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0240 - val_loss: 0.0378\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0259 - val_loss: 0.0479\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0247 - val_loss: 0.0418\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0257 - val_loss: 0.0358\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0253 - val_loss: 0.0432\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0248 - val_loss: 0.0340\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0257 - val_loss: 0.0426\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0233 - val_loss: 0.0367\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0227 - val_loss: 0.0385\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0254 - val_loss: 0.0373\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0240 - val_loss: 0.0866\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0243 - val_loss: 0.0361\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0233 - val_loss: 0.0468\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0236 - val_loss: 0.0602\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0229 - val_loss: 0.0443\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0225 - val_loss: 0.0339\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0236 - val_loss: 0.0383\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0219 - val_loss: 0.0540\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0232 - val_loss: 0.0492\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0232 - val_loss: 0.0806\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0249 - val_loss: 0.0385\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0230 - val_loss: 0.0399\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0218 - val_loss: 0.0425\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0215 - val_loss: 0.0390\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0239 - val_loss: 0.0375\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0235 - val_loss: 0.0451\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0221 - val_loss: 0.0517\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0228 - val_loss: 0.0356\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0215 - val_loss: 0.0355\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0213 - val_loss: 0.0384\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0211 - val_loss: 0.0474\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0202 - val_loss: 0.0706\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0207 - val_loss: 0.0387\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0228 - val_loss: 0.0628\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0213 - val_loss: 0.0454\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0198 - val_loss: 0.0511\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0212 - val_loss: 0.0437\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0203 - val_loss: 0.0445\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0221 - val_loss: 0.0367\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0207 - val_loss: 0.0439\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0212 - val_loss: 0.0361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0210 - val_loss: 0.0466\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0192 - val_loss: 0.0369\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0196 - val_loss: 0.0365\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0189 - val_loss: 0.0377\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0196 - val_loss: 0.0366\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0209 - val_loss: 0.0364\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0195 - val_loss: 0.0357\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0199 - val_loss: 0.0348\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0199 - val_loss: 0.0467\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0190 - val_loss: 0.0341\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0198 - val_loss: 0.0398\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0193 - val_loss: 0.0344\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0196 - val_loss: 0.0392\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0197 - val_loss: 0.0340\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0193 - val_loss: 0.0341\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0191 - val_loss: 0.0428\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0184 - val_loss: 0.0379\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0183 - val_loss: 0.0341\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0181 - val_loss: 0.0431\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0180 - val_loss: 0.0367\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0181 - val_loss: 0.0404\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0194 - val_loss: 0.0441\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0203 - val_loss: 0.0632\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0181 - val_loss: 0.0427\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0192 - val_loss: 0.0354\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0183 - val_loss: 0.0332\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0187 - val_loss: 0.0378\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0178 - val_loss: 0.0345\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0184 - val_loss: 0.0363\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0179 - val_loss: 0.0367\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0186 - val_loss: 0.0342\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0189 - val_loss: 0.0391\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0175 - val_loss: 0.0405\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0182 - val_loss: 0.0560\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0199 - val_loss: 0.0343\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0183 - val_loss: 0.0347\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0172 - val_loss: 0.0397\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0175 - val_loss: 0.0340\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0171 - val_loss: 0.0398\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0170 - val_loss: 0.0419\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0180 - val_loss: 0.0430\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0174 - val_loss: 0.0440\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0173 - val_loss: 0.0326\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0157 - val_loss: 0.0396\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0184 - val_loss: 0.0364\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0161 - val_loss: 0.0354\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0181 - val_loss: 0.0418\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0166 - val_loss: 0.0355\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0183 - val_loss: 0.0366\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0175 - val_loss: 0.0477\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0162 - val_loss: 0.0698\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0171 - val_loss: 0.0357\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0160 - val_loss: 0.0386\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0168 - val_loss: 0.0374\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0167 - val_loss: 0.0327\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0175 - val_loss: 0.0344\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0174 - val_loss: 0.0409\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0175 - val_loss: 0.0371\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0163 - val_loss: 0.0396\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0157 - val_loss: 0.0331\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0167 - val_loss: 0.0353\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0160 - val_loss: 0.0333\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0164 - val_loss: 0.0651\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0164 - val_loss: 0.0368\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0157 - val_loss: 0.0419\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0160 - val_loss: 0.0338\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0159 - val_loss: 0.0351\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0153 - val_loss: 0.0332\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0153 - val_loss: 0.0327\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0152 - val_loss: 0.0477\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0165 - val_loss: 0.0615\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0154 - val_loss: 0.0450\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0160 - val_loss: 0.0330\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0170 - val_loss: 0.0412\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0164 - val_loss: 0.0338\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0152 - val_loss: 0.0372\n",
      "Epoch 209/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0152 - val_loss: 0.0327\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0148 - val_loss: 0.0550\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0166 - val_loss: 0.0326\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0160 - val_loss: 0.0408\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0152 - val_loss: 0.0337\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0155 - val_loss: 0.0332\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0154 - val_loss: 0.0333\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0145 - val_loss: 0.0391\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0148 - val_loss: 0.0408\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0148 - val_loss: 0.0489\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0156 - val_loss: 0.0335\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0140 - val_loss: 0.0317\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0150 - val_loss: 0.0337\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0158 - val_loss: 0.0382\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0145 - val_loss: 0.0360\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0156 - val_loss: 0.0357\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0145 - val_loss: 0.0366\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0145 - val_loss: 0.0323\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0149 - val_loss: 0.0454\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0142 - val_loss: 0.0798\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0151 - val_loss: 0.0397\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0155 - val_loss: 0.0370\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0149 - val_loss: 0.0370\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0148 - val_loss: 0.0345\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0151 - val_loss: 0.0326\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0151 - val_loss: 0.0376\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0146 - val_loss: 0.0314\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0140 - val_loss: 0.0346\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0151 - val_loss: 0.0331\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0144 - val_loss: 0.0322\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0145 - val_loss: 0.0353\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0140 - val_loss: 0.0605\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0144 - val_loss: 0.0370\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0138 - val_loss: 0.0402\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0153 - val_loss: 0.0770\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0138 - val_loss: 0.0360\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0143 - val_loss: 0.0451\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0140 - val_loss: 0.0333\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0147 - val_loss: 0.0372\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0138 - val_loss: 0.0394\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0142 - val_loss: 0.0363\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0140 - val_loss: 0.0323\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 440us/step - loss: 12.8494 - val_loss: 0.4269\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.3852 - val_loss: 0.4120\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.3533 - val_loss: 0.2067\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.2303 - val_loss: 0.1749\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.2153 - val_loss: 0.3642\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1672 - val_loss: 0.1497\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1613 - val_loss: 0.1507\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1446 - val_loss: 0.1233\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1295 - val_loss: 0.0962\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1135 - val_loss: 0.0917\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1421 - val_loss: 0.1186\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1256 - val_loss: 0.2617\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1141 - val_loss: 0.0956\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1017 - val_loss: 0.0796\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1292 - val_loss: 0.2149\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0936 - val_loss: 0.1134\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0891 - val_loss: 0.1210\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1009 - val_loss: 0.0889\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0822 - val_loss: 0.1295\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0746 - val_loss: 0.2015\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0770 - val_loss: 0.1433\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0819 - val_loss: 0.0944\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0649 - val_loss: 0.0990\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0696 - val_loss: 0.0988\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0851 - val_loss: 0.0688\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0741 - val_loss: 0.2707\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0720 - val_loss: 0.1158\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0606 - val_loss: 0.0693\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0694 - val_loss: 0.0717\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0580 - val_loss: 0.0779\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0541 - val_loss: 0.0754\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0602 - val_loss: 0.0587\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0690 - val_loss: 0.0594\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0608 - val_loss: 0.0556\n",
      "Epoch 35/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0611 - val_loss: 0.0671\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0551 - val_loss: 0.1302\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0495 - val_loss: 0.0836\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0597 - val_loss: 0.1035\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0952 - val_loss: 0.1385\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0549 - val_loss: 0.0693\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0544 - val_loss: 0.0582\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0669 - val_loss: 0.0595\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0478 - val_loss: 0.0891\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0490 - val_loss: 0.1642\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0460 - val_loss: 0.0549\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0515 - val_loss: 0.0592\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0576 - val_loss: 0.0640\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0478 - val_loss: 0.0567\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0473 - val_loss: 0.0526\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0449 - val_loss: 0.0576\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0425 - val_loss: 0.0530\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0463 - val_loss: 0.0598\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0441 - val_loss: 0.0495\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0457 - val_loss: 0.0871\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0466 - val_loss: 0.0889\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0501 - val_loss: 0.0586\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0419 - val_loss: 0.0546\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0431 - val_loss: 0.1865\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0440 - val_loss: 0.0903\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0398 - val_loss: 0.0555\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0540 - val_loss: 0.0637\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0425 - val_loss: 0.0577\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0399 - val_loss: 0.0942\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0392 - val_loss: 0.0927\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0605 - val_loss: 0.0468\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0415 - val_loss: 0.0574\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0400 - val_loss: 0.0674\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0388 - val_loss: 0.0450\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0374 - val_loss: 0.1088\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0390 - val_loss: 0.0579\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0387 - val_loss: 0.0848\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0362 - val_loss: 0.0521\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0403 - val_loss: 0.0480\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0374 - val_loss: 0.0473\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0382 - val_loss: 0.0466\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0349 - val_loss: 0.0448\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0345 - val_loss: 0.0535\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0375 - val_loss: 0.0467\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0384 - val_loss: 0.0545\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0373 - val_loss: 0.0757\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0347 - val_loss: 0.0766\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0328 - val_loss: 0.0523\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0330 - val_loss: 0.1215\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0349 - val_loss: 0.0455\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0375 - val_loss: 0.0649\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0359 - val_loss: 0.0451\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0330 - val_loss: 0.0476\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0339 - val_loss: 0.0534\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0357 - val_loss: 0.0836\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0338 - val_loss: 0.1107\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0325 - val_loss: 0.0417\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0361 - val_loss: 0.0506\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0318 - val_loss: 0.0501\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0327 - val_loss: 0.0459\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0326 - val_loss: 0.0498\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0367 - val_loss: 0.0473\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0309 - val_loss: 0.0541\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0323 - val_loss: 0.0757\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0301 - val_loss: 0.0888\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0304 - val_loss: 0.0464\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0302 - val_loss: 0.0567\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0311 - val_loss: 0.1112\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0312 - val_loss: 0.0448\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0333 - val_loss: 0.0575\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0303 - val_loss: 0.0745\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0352 - val_loss: 0.0834\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0301 - val_loss: 0.0459\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0308 - val_loss: 0.0493\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0456 - val_loss: 0.0551\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0292 - val_loss: 0.0448\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0302 - val_loss: 0.0416\n",
      "Epoch 112/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0304 - val_loss: 0.0458\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0333 - val_loss: 0.0484\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0330 - val_loss: 0.0422\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0310 - val_loss: 0.0449\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0317 - val_loss: 0.0846\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0299 - val_loss: 0.0502\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0307 - val_loss: 0.0484\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0278 - val_loss: 0.0465\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0292 - val_loss: 0.0432\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0262 - val_loss: 0.0550\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0262 - val_loss: 0.0433\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0325 - val_loss: 0.0817\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0296 - val_loss: 0.0450\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0302 - val_loss: 0.0450\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0263 - val_loss: 0.0416\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0274 - val_loss: 0.0477\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0308 - val_loss: 0.0678\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0279 - val_loss: 0.0444\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0273 - val_loss: 0.0426\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0285 - val_loss: 0.0741\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0313 - val_loss: 0.0416\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0276 - val_loss: 0.0501\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0416 - val_loss: 0.0452\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0311 - val_loss: 0.0433\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0270 - val_loss: 0.0434\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0242 - val_loss: 0.0437\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0266 - val_loss: 0.0420\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0259 - val_loss: 0.0408\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0235 - val_loss: 0.0942\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0294 - val_loss: 0.0971\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0255 - val_loss: 0.0779\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0238 - val_loss: 0.0523\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0248 - val_loss: 0.0456\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0264 - val_loss: 0.0490\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0250 - val_loss: 0.0443\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0280 - val_loss: 0.0503\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0259 - val_loss: 0.0424\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0250 - val_loss: 0.0465\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0267 - val_loss: 0.0452\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0255 - val_loss: 0.0413\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0249 - val_loss: 0.0635\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0241 - val_loss: 0.0463\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0253 - val_loss: 0.0413\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0240 - val_loss: 0.0476\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0244 - val_loss: 0.0718\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0239 - val_loss: 0.0521\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0249 - val_loss: 0.0530\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0245 - val_loss: 0.0446\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0228 - val_loss: 0.0564\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0233 - val_loss: 0.0463\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0270 - val_loss: 0.0474\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0263 - val_loss: 0.0430\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0226 - val_loss: 0.0431\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0232 - val_loss: 0.0411\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0222 - val_loss: 0.0413\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0237 - val_loss: 0.0424\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0225 - val_loss: 0.0412\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0253 - val_loss: 0.0832\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0225 - val_loss: 0.0874\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0275 - val_loss: 0.0680\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0253 - val_loss: 0.0410\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0229 - val_loss: 0.0403\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0231 - val_loss: 0.0503\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0237 - val_loss: 0.0392\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0235 - val_loss: 0.0756\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0226 - val_loss: 0.0400\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0224 - val_loss: 0.0410\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0220 - val_loss: 0.0450\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0232 - val_loss: 0.0637\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0227 - val_loss: 0.0410\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0215 - val_loss: 0.0519\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0222 - val_loss: 0.0441\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0233 - val_loss: 0.0485\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0237 - val_loss: 0.0530\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0231 - val_loss: 0.0816\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0208 - val_loss: 0.0444\n",
      "Epoch 188/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0220 - val_loss: 0.0483\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0239 - val_loss: 0.0402\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0217 - val_loss: 0.0412\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0238 - val_loss: 0.0436\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0210 - val_loss: 0.0438\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0198 - val_loss: 0.0419\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0224 - val_loss: 0.0651\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0200 - val_loss: 0.0422\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0213 - val_loss: 0.0639\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0304 - val_loss: 0.0434\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0199 - val_loss: 0.0391\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0251 - val_loss: 0.0730\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0210 - val_loss: 0.0501\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0235 - val_loss: 0.0420\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0225 - val_loss: 0.0954\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0247 - val_loss: 0.0437\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0210 - val_loss: 0.0392\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0209 - val_loss: 0.0394\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0225 - val_loss: 0.0420\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0224 - val_loss: 0.0426\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0217 - val_loss: 0.0470\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0208 - val_loss: 0.0434\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0224 - val_loss: 0.0391\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0195 - val_loss: 0.0436\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0209 - val_loss: 0.0421\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0213 - val_loss: 0.0407\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0206 - val_loss: 0.0499\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0222 - val_loss: 0.0397\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0213 - val_loss: 0.0502\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0205 - val_loss: 0.0612\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0205 - val_loss: 0.0382\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0199 - val_loss: 0.0421\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0228 - val_loss: 0.0418\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0224 - val_loss: 0.0396\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0211 - val_loss: 0.0513\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0202 - val_loss: 0.0650\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0198 - val_loss: 0.0383\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0198 - val_loss: 0.0441\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0200 - val_loss: 0.0512\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0208 - val_loss: 0.0434\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0205 - val_loss: 0.0417\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0197 - val_loss: 0.1159\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0192 - val_loss: 0.0432\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0197 - val_loss: 0.0394\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0219 - val_loss: 0.0414\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0223 - val_loss: 0.1748\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0202 - val_loss: 0.0466\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0206 - val_loss: 0.0411\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0202 - val_loss: 0.0465\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0195 - val_loss: 0.0473\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0216 - val_loss: 0.0425\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0192 - val_loss: 0.0400\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0203 - val_loss: 0.0456\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0214 - val_loss: 0.0427\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0196 - val_loss: 0.0395\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0192 - val_loss: 0.0422\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0245 - val_loss: 0.0432\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0185 - val_loss: 0.0393\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0200 - val_loss: 0.0543\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0201 - val_loss: 0.0418\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0198 - val_loss: 0.0749\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0203 - val_loss: 0.0412\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0190 - val_loss: 0.0643\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 444us/step - loss: 85.4729 - val_loss: 0.4615\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3797 - val_loss: 0.3623\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2769 - val_loss: 0.4374\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.2247 - val_loss: 0.3399\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1983 - val_loss: 0.2454\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1780 - val_loss: 0.2058\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1655 - val_loss: 0.1977\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1550 - val_loss: 0.2726\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1422 - val_loss: 0.1433\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1343 - val_loss: 0.1352\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1275 - val_loss: 0.1417\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1177 - val_loss: 0.2378\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1088 - val_loss: 0.2304\n",
      "Epoch 14/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1054 - val_loss: 0.1608\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1020 - val_loss: 0.1232\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0966 - val_loss: 0.1321\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1046 - val_loss: 0.1388\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0963 - val_loss: 0.1177\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0863 - val_loss: 0.1616\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0844 - val_loss: 0.1196\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0814 - val_loss: 0.1203\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0774 - val_loss: 0.1036\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0733 - val_loss: 0.1081\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0762 - val_loss: 0.1686\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0704 - val_loss: 0.0957\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0714 - val_loss: 0.1066\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0686 - val_loss: 0.1300\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0666 - val_loss: 0.1059\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0660 - val_loss: 0.1296\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0618 - val_loss: 0.0907\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0620 - val_loss: 0.0942\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0576 - val_loss: 0.1005\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0572 - val_loss: 0.1615\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0588 - val_loss: 0.0842\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0583 - val_loss: 0.1235\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0563 - val_loss: 0.1301\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0548 - val_loss: 0.0851\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0561 - val_loss: 0.0818\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0504 - val_loss: 0.0863\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0510 - val_loss: 0.0911\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0528 - val_loss: 0.1026\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0480 - val_loss: 0.0851\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0505 - val_loss: 0.0810\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0485 - val_loss: 0.0800\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0483 - val_loss: 0.1309\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0486 - val_loss: 0.0910\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0445 - val_loss: 0.0890\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0447 - val_loss: 0.0771\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0471 - val_loss: 0.0882\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0472 - val_loss: 0.0826\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0446 - val_loss: 0.0724\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0433 - val_loss: 0.0817\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0421 - val_loss: 0.0832\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0445 - val_loss: 0.1039\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0416 - val_loss: 0.0755\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0412 - val_loss: 0.0747\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0400 - val_loss: 0.0732\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0390 - val_loss: 0.1074\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0388 - val_loss: 0.1149\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0374 - val_loss: 0.0853\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0392 - val_loss: 0.0723\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0369 - val_loss: 0.0762\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0378 - val_loss: 0.0700\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0370 - val_loss: 0.0787\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0349 - val_loss: 0.0731\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0354 - val_loss: 0.0662\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0339 - val_loss: 0.1255\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0341 - val_loss: 0.0959\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0350 - val_loss: 0.0686\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0319 - val_loss: 0.1233\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0343 - val_loss: 0.0695\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0339 - val_loss: 0.0656\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0318 - val_loss: 0.0899\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0361 - val_loss: 0.0657\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0338 - val_loss: 0.0741\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0331 - val_loss: 0.0793\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0309 - val_loss: 0.0772\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0317 - val_loss: 0.0776\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0306 - val_loss: 0.0679\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0320 - val_loss: 0.0695\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0357 - val_loss: 0.0609\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0314 - val_loss: 0.0628\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0313 - val_loss: 0.0674\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0291 - val_loss: 0.0617\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0329 - val_loss: 0.0762\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0323 - val_loss: 0.0669\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0310 - val_loss: 0.0705\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0309 - val_loss: 0.0691\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0298 - val_loss: 0.0640\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0289 - val_loss: 0.0666\n",
      "Epoch 91/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0285 - val_loss: 0.0723\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0289 - val_loss: 0.0932\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0284 - val_loss: 0.0914\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0274 - val_loss: 0.0754\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0298 - val_loss: 0.0602\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0280 - val_loss: 0.0726\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0272 - val_loss: 0.0615\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0285 - val_loss: 0.1209\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0299 - val_loss: 0.0672\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0291 - val_loss: 0.0611\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0268 - val_loss: 0.0573\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0305 - val_loss: 0.0621\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0269 - val_loss: 0.0583\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0275 - val_loss: 0.0740\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0268 - val_loss: 0.0649\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0275 - val_loss: 0.0688\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0261 - val_loss: 0.0585\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0261 - val_loss: 0.0594\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0262 - val_loss: 0.1038\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0256 - val_loss: 0.0650\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0251 - val_loss: 0.0612\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0274 - val_loss: 0.0598\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0250 - val_loss: 0.0564\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0263 - val_loss: 0.1101\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0258 - val_loss: 0.0603\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0285 - val_loss: 0.0667\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0275 - val_loss: 0.0695\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0275 - val_loss: 0.0679\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0286 - val_loss: 0.0646\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0293 - val_loss: 0.0822\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0246 - val_loss: 0.0572\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0245 - val_loss: 0.0641\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0244 - val_loss: 0.0668\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0262 - val_loss: 0.0589\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0247 - val_loss: 0.0610\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0243 - val_loss: 0.0547\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0242 - val_loss: 0.0606\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0241 - val_loss: 0.0737\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0242 - val_loss: 0.0576\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0238 - val_loss: 0.0606\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0243 - val_loss: 0.0598\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0250 - val_loss: 0.0572\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0225 - val_loss: 0.1335\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0255 - val_loss: 0.0587\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0230 - val_loss: 0.0790\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0229 - val_loss: 0.0546\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0250 - val_loss: 0.0556\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0252 - val_loss: 0.0763\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0231 - val_loss: 0.0577\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0236 - val_loss: 0.0619\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0219 - val_loss: 0.0644\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0233 - val_loss: 0.0666\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0235 - val_loss: 0.0625\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0224 - val_loss: 0.0790\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0234 - val_loss: 0.1121\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0231 - val_loss: 0.0596\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0232 - val_loss: 0.0560\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0223 - val_loss: 0.0552\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0230 - val_loss: 0.0728\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0228 - val_loss: 0.1020\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0222 - val_loss: 0.0599\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0209 - val_loss: 0.0618\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0218 - val_loss: 0.0591\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0211 - val_loss: 0.0810\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0230 - val_loss: 0.0741\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0245 - val_loss: 0.0561\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0214 - val_loss: 0.0743\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0226 - val_loss: 0.0566\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0210 - val_loss: 0.0626\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0237 - val_loss: 0.0535\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0238 - val_loss: 0.0602\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0211 - val_loss: 0.0547\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0221 - val_loss: 0.0673\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0241 - val_loss: 0.0918\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0213 - val_loss: 0.0536\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0213 - val_loss: 0.0793\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0216 - val_loss: 0.0549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0208 - val_loss: 0.0658\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0206 - val_loss: 0.0526\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0208 - val_loss: 0.2544\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0222 - val_loss: 0.0533\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0202 - val_loss: 0.0537\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0208 - val_loss: 0.0527\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0212 - val_loss: 0.0602\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0265 - val_loss: 0.0654\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0209 - val_loss: 0.0553\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0211 - val_loss: 0.0561\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0219 - val_loss: 0.0530\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0224 - val_loss: 0.0547\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0204 - val_loss: 0.0539\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0197 - val_loss: 0.0617\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0202 - val_loss: 0.0568\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0189 - val_loss: 0.0572\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0211 - val_loss: 0.0519\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0216 - val_loss: 0.0561\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0200 - val_loss: 0.0587\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0206 - val_loss: 0.0735\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0213 - val_loss: 0.0575\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0206 - val_loss: 0.0546\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0192 - val_loss: 0.0552\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0197 - val_loss: 0.0563\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0208 - val_loss: 0.0629\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0188 - val_loss: 0.0569\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0232 - val_loss: 0.0661\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0211 - val_loss: 0.1129\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0196 - val_loss: 0.0893\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0210 - val_loss: 0.0549\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0201 - val_loss: 0.0601\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0206 - val_loss: 0.0610\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0196 - val_loss: 0.0545\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0194 - val_loss: 0.0523\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0204 - val_loss: 0.1149\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0205 - val_loss: 0.0588\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0206 - val_loss: 0.0572\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0204 - val_loss: 0.0539\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0190 - val_loss: 0.0730\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0192 - val_loss: 0.0638\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0206 - val_loss: 0.0535\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0201 - val_loss: 0.0559\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0192 - val_loss: 0.0569\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0198 - val_loss: 0.0588\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0190 - val_loss: 0.0572\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0183 - val_loss: 0.1126\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0199 - val_loss: 0.0525\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0190 - val_loss: 0.0584\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0187 - val_loss: 0.0648\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0202 - val_loss: 0.0597\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0195 - val_loss: 0.0717\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0208 - val_loss: 0.0541\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0195 - val_loss: 0.0570\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0179 - val_loss: 0.0716\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0191 - val_loss: 0.0678\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0178 - val_loss: 0.0576\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0168 - val_loss: 0.0557\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0182 - val_loss: 0.0519\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0189 - val_loss: 0.0545\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0183 - val_loss: 0.0572\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0165 - val_loss: 0.0658\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0178 - val_loss: 0.0588\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0189 - val_loss: 0.0573\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0170 - val_loss: 0.0535\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0188 - val_loss: 0.0592\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0172 - val_loss: 0.0601\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0193 - val_loss: 0.0531\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0194 - val_loss: 0.1467\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0204 - val_loss: 0.0549\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0184 - val_loss: 0.0681\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0171 - val_loss: 0.1039\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0173 - val_loss: 0.0533\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0185 - val_loss: 0.0574\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0180 - val_loss: 0.0731\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0195 - val_loss: 0.0576\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0175 - val_loss: 0.0544\n",
      "Epoch 244/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0174 - val_loss: 0.0544\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0181 - val_loss: 0.0545\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0190 - val_loss: 0.0614\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0194 - val_loss: 0.0541\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0175 - val_loss: 0.0606\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0160 - val_loss: 0.0622\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0185 - val_loss: 0.0539\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 443us/step - loss: 9.2895 - val_loss: 0.6099\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3375 - val_loss: 0.2223\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2451 - val_loss: 0.2409\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2160 - val_loss: 0.2111\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1838 - val_loss: 0.6922\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1532 - val_loss: 0.1416\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1319 - val_loss: 0.2366\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1289 - val_loss: 0.1417\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.1160 - val_loss: 0.1429\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.1092 - val_loss: 0.1076\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1087 - val_loss: 0.4771\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1042 - val_loss: 0.1313\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0888 - val_loss: 0.0961\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0853 - val_loss: 0.1217\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0868 - val_loss: 0.1018\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0850 - val_loss: 0.0832\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0864 - val_loss: 0.1094\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0758 - val_loss: 0.0737\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0786 - val_loss: 0.0731\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0708 - val_loss: 0.0822\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0688 - val_loss: 0.3822\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0647 - val_loss: 0.1174\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0622 - val_loss: 0.0731\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0583 - val_loss: 0.0645\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0646 - val_loss: 0.0806\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0533 - val_loss: 0.0584\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0557 - val_loss: 0.0662\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0590 - val_loss: 0.0642\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0646 - val_loss: 0.0827\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0539 - val_loss: 0.0606\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0512 - val_loss: 0.1145\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0539 - val_loss: 0.0606\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0569 - val_loss: 0.6366\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0556 - val_loss: 0.0745\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0499 - val_loss: 0.0544\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0503 - val_loss: 0.0590\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0431 - val_loss: 0.0639\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0456 - val_loss: 0.0546\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0450 - val_loss: 0.0619\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0429 - val_loss: 0.0800\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0520 - val_loss: 0.0976\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0444 - val_loss: 0.0631\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0502 - val_loss: 0.0881\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0506 - val_loss: 0.0709\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0417 - val_loss: 0.0516\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0397 - val_loss: 0.0467\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0410 - val_loss: 0.0826\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0400 - val_loss: 0.0550\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0377 - val_loss: 0.0524\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0406 - val_loss: 0.0958\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0389 - val_loss: 0.0530\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0338 - val_loss: 0.0679\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0405 - val_loss: 0.0593\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0342 - val_loss: 0.0487\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0377 - val_loss: 0.0755\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0398 - val_loss: 0.0479\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0425 - val_loss: 0.2075\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0341 - val_loss: 0.0752\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0355 - val_loss: 0.0453\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0339 - val_loss: 0.0460\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0356 - val_loss: 0.0475\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0359 - val_loss: 0.0438\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0326 - val_loss: 0.0729\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0417 - val_loss: 0.0500\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0328 - val_loss: 0.0431\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0292 - val_loss: 0.0438\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0453 - val_loss: 0.0470\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0309 - val_loss: 0.0686\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0348 - val_loss: 0.2103\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0341 - val_loss: 0.0444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0356 - val_loss: 0.0492\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0353 - val_loss: 0.0513\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0346 - val_loss: 0.0855\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0345 - val_loss: 0.0448\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0287 - val_loss: 0.0576\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0366 - val_loss: 0.0535\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0299 - val_loss: 0.0468\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0272 - val_loss: 0.0411\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0309 - val_loss: 0.0433\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0317 - val_loss: 0.0423\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0284 - val_loss: 0.0572\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0266 - val_loss: 0.0400\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0274 - val_loss: 0.0626\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0320 - val_loss: 0.0639\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0279 - val_loss: 0.0460\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0311 - val_loss: 0.0512\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0280 - val_loss: 0.0480\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0298 - val_loss: 0.0400\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0292 - val_loss: 0.0628\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0285 - val_loss: 0.0603\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0283 - val_loss: 0.0525\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0261 - val_loss: 0.1047\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0299 - val_loss: 0.0396\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0267 - val_loss: 0.0460\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0252 - val_loss: 0.0452\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0255 - val_loss: 0.0398\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0293 - val_loss: 0.0492\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0271 - val_loss: 0.0982\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0248 - val_loss: 0.0715\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0250 - val_loss: 0.0447\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0245 - val_loss: 0.0415\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0244 - val_loss: 0.0554\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0250 - val_loss: 0.0380\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0255 - val_loss: 0.0389\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0260 - val_loss: 0.0657\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0244 - val_loss: 0.0483\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0251 - val_loss: 0.0450\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0258 - val_loss: 0.0475\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0257 - val_loss: 0.0465\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0263 - val_loss: 0.0411\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0243 - val_loss: 0.0657\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0248 - val_loss: 0.0382\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0255 - val_loss: 0.1628\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0307 - val_loss: 0.0398\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0276 - val_loss: 0.0405\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0271 - val_loss: 0.0410\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0221 - val_loss: 0.0366\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0249 - val_loss: 0.0482\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0278 - val_loss: 0.0446\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0218 - val_loss: 0.0573\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0256 - val_loss: 0.0388\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0266 - val_loss: 0.0571\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0289 - val_loss: 0.1327\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0223 - val_loss: 0.0394\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0223 - val_loss: 0.0394\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0201 - val_loss: 0.0419\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0338 - val_loss: 0.0372\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0235 - val_loss: 0.0613\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0218 - val_loss: 0.0396\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0225 - val_loss: 0.0376\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0220 - val_loss: 0.0495\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0209 - val_loss: 0.0420\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0224 - val_loss: 0.0374\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0328 - val_loss: 0.0644\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0220 - val_loss: 0.0836\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0225 - val_loss: 0.0396\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0234 - val_loss: 0.0433\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0309 - val_loss: 0.0373\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0230 - val_loss: 0.0407\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0196 - val_loss: 0.0380\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0219 - val_loss: 0.0386\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0223 - val_loss: 0.3200\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0217 - val_loss: 0.0383\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0421 - val_loss: 0.0545\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0246 - val_loss: 0.0393\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0244 - val_loss: 0.0495\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0224 - val_loss: 0.0398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0188 - val_loss: 0.0420\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0195 - val_loss: 0.1322\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0209 - val_loss: 0.0443\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0217 - val_loss: 0.0675\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0282 - val_loss: 0.0477\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0229 - val_loss: 0.0391\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0220 - val_loss: 0.0437\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0207 - val_loss: 0.0417\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0190 - val_loss: 0.0386\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0203 - val_loss: 0.0366\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0190 - val_loss: 0.0374\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0247 - val_loss: 0.0414\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0186 - val_loss: 0.0391\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0204 - val_loss: 0.0363\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0193 - val_loss: 0.0358\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0185 - val_loss: 0.0477\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0191 - val_loss: 0.0527\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0200 - val_loss: 0.0376\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0214 - val_loss: 0.0364\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0186 - val_loss: 0.0604\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0188 - val_loss: 0.0373\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0199 - val_loss: 0.0464\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0175 - val_loss: 0.0397\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0195 - val_loss: 0.0637\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0208 - val_loss: 0.0497\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0198 - val_loss: 0.0449\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0207 - val_loss: 0.0523\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0179 - val_loss: 0.0402\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0199 - val_loss: 0.0800\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0190 - val_loss: 0.0357\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0181 - val_loss: 0.0414\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0195 - val_loss: 0.0371\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0185 - val_loss: 0.0450\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0193 - val_loss: 0.0508\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0178 - val_loss: 0.0376\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0188 - val_loss: 0.0353\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0180 - val_loss: 0.0369\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0196 - val_loss: 0.0377\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0193 - val_loss: 0.0359\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0174 - val_loss: 0.0359\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0185 - val_loss: 0.0388\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0179 - val_loss: 0.0543\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0249 - val_loss: 0.0376\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0170 - val_loss: 0.0374\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0181 - val_loss: 0.0389\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0160 - val_loss: 0.0361\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0168 - val_loss: 0.0430\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0172 - val_loss: 0.0417\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0165 - val_loss: 0.0650\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0185 - val_loss: 0.0948\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0205 - val_loss: 0.0395\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0194 - val_loss: 0.0383\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0172 - val_loss: 0.0520\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0157 - val_loss: 0.0363\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0175 - val_loss: 0.4781\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0213 - val_loss: 0.0478\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0193 - val_loss: 0.0505\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0171 - val_loss: 0.0471\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0222 - val_loss: 0.0382\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0169 - val_loss: 0.0384\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0175 - val_loss: 0.0403\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0169 - val_loss: 0.0382\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0179 - val_loss: 0.0386\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0194 - val_loss: 0.0397\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0183 - val_loss: 0.0386\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0171 - val_loss: 0.0544\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0176 - val_loss: 0.1784\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0202 - val_loss: 0.0420\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0180 - val_loss: 0.0438\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0191 - val_loss: 0.0374\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0236 - val_loss: 0.0393\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0177 - val_loss: 0.0435\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0183 - val_loss: 0.0579\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0161 - val_loss: 0.0367\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0204 - val_loss: 0.0369\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0154 - val_loss: 0.0500\n",
      "Epoch 224/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0183 - val_loss: 0.0436\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0177 - val_loss: 0.0391\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0169 - val_loss: 0.0364\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0187 - val_loss: 0.0826\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0154 - val_loss: 0.0363\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0168 - val_loss: 0.1626\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0180 - val_loss: 0.0387\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0170 - val_loss: 0.0375\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0175 - val_loss: 0.0372\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0171 - val_loss: 0.0359\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0160 - val_loss: 0.0462\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0152 - val_loss: 0.0616\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0162 - val_loss: 0.0470\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0151 - val_loss: 0.0360\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0210 - val_loss: 0.0356\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0146 - val_loss: 0.0438\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0156 - val_loss: 0.0570\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0179 - val_loss: 0.0413\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0157 - val_loss: 0.0356\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0145 - val_loss: 0.0350\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0152 - val_loss: 0.0370\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0165 - val_loss: 0.0363\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0154 - val_loss: 0.0378\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0162 - val_loss: 0.0356\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0163 - val_loss: 0.0377\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0165 - val_loss: 0.0378\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1440 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "lr=0.02;\n",
    "n_lr = 20;\n",
    "learn_rate = np.linspace(1/n_lr,1,n_lr)*lr; #20 puntos de (0,1]\n",
    "j=0;\n",
    "plt.figure();\n",
    "for i in learn_rate:\n",
    "    j+=1;\n",
    "    plt.subplot(5,n_lr/5,j);\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=SGD(lr=i),loss='mean_squared_error')\n",
    "    history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "    plt.title('lr='+str(i));\n",
    "    plt.plot(history.history['loss'],label='train');\n",
    "    plt.plot(history.history['val_loss'],label='validation');\n",
    "    plt.ylim([0,0.5]);\n",
    "\n",
    "plt.show();\n",
    "plt.tight_layout();\n",
    "plt.rcParams[\"figure.figsize\"]=[15,20]; #para que los plot encagen bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 36.7078 - val_loss: 16.2211\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 9.1111 - val_loss: 9.0139\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 5.0657 - val_loss: 5.8240\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 3.2309 - val_loss: 4.4974\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 2.3519 - val_loss: 3.6714\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 1.8074 - val_loss: 3.2825\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 1.5626 - val_loss: 3.0704\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 1.3570 - val_loss: 2.8601\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 1.2361 - val_loss: 2.6533\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 1.1258 - val_loss: 2.5185\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 1.0311 - val_loss: 2.5370\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.9688 - val_loss: 2.3813\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.9132 - val_loss: 2.3404\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.8644 - val_loss: 2.2358\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.8153 - val_loss: 2.2180\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.7785 - val_loss: 2.1819\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.7417 - val_loss: 2.1240\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.7134 - val_loss: 2.0889\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.6823 - val_loss: 2.0816\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.6567 - val_loss: 2.0475\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.6317 - val_loss: 2.0329\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.6095 - val_loss: 1.9531\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.5884 - val_loss: 1.9458\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.5715 - val_loss: 1.9797\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.5526 - val_loss: 1.9013\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.5371 - val_loss: 1.9548\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.5216 - val_loss: 1.8708\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.5049 - val_loss: 1.8337\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.4904 - val_loss: 1.8422\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.4803 - val_loss: 1.8283\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.4686 - val_loss: 1.7926\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.4561 - val_loss: 1.7828\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.4449 - val_loss: 1.7384\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.4345 - val_loss: 1.7503\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.4254 - val_loss: 1.7512\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.4163 - val_loss: 1.6979\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.4066 - val_loss: 1.7066\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3974 - val_loss: 1.6743\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3919 - val_loss: 1.6343\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3828 - val_loss: 1.6384\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.3751 - val_loss: 1.6183\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.3664 - val_loss: 1.5940\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.3589 - val_loss: 1.6270\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3566 - val_loss: 1.5792\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3482 - val_loss: 1.5782\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3411 - val_loss: 1.5604\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.3356 - val_loss: 1.5449\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.3295 - val_loss: 1.5531\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3228 - val_loss: 1.5722\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.3190 - val_loss: 1.5188\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3131 - val_loss: 1.5461\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.3076 - val_loss: 1.4884\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.3031 - val_loss: 1.4668\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2993 - val_loss: 1.4805\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2947 - val_loss: 1.5126\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2910 - val_loss: 1.4843\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2864 - val_loss: 1.4368\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2812 - val_loss: 1.4266\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2792 - val_loss: 1.4326\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.2747 - val_loss: 1.4124\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.2694 - val_loss: 1.4089\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.2669 - val_loss: 1.4048\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2631 - val_loss: 1.3963\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2585 - val_loss: 1.4117\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2561 - val_loss: 1.3859\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2528 - val_loss: 1.3715\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.2499 - val_loss: 1.3725\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2461 - val_loss: 1.3807\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2425 - val_loss: 1.3508\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2399 - val_loss: 1.3335\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.2362 - val_loss: 1.3307\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.2345 - val_loss: 1.3350\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.2320 - val_loss: 1.3373\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2282 - val_loss: 1.3017\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2265 - val_loss: 1.3272\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2231 - val_loss: 1.3044\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2200 - val_loss: 1.2925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2182 - val_loss: 1.2785\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.2156 - val_loss: 1.2744\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2144 - val_loss: 1.2860\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2105 - val_loss: 1.2906\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.2087 - val_loss: 1.2958\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.2067 - val_loss: 1.2465\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2043 - val_loss: 1.2599\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2010 - val_loss: 1.2588\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.1983 - val_loss: 1.2353\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1974 - val_loss: 1.2163\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1954 - val_loss: 1.2196\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1933 - val_loss: 1.1979\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1917 - val_loss: 1.2121\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1905 - val_loss: 1.2256\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1876 - val_loss: 1.2198\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1858 - val_loss: 1.2180\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1843 - val_loss: 1.2021\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1823 - val_loss: 1.2021\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1808 - val_loss: 1.1867\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1803 - val_loss: 1.2004\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1775 - val_loss: 1.1945\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1760 - val_loss: 1.1940\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1741 - val_loss: 1.1548\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1724 - val_loss: 1.1677\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1712 - val_loss: 1.1992\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1694 - val_loss: 1.1594\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1686 - val_loss: 1.1595\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1674 - val_loss: 1.1530\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1652 - val_loss: 1.1698\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1636 - val_loss: 1.1113\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1630 - val_loss: 1.2311\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1629 - val_loss: 1.1384\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1601 - val_loss: 1.1329\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1587 - val_loss: 1.1355\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1573 - val_loss: 1.1369\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1566 - val_loss: 1.1128\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1549 - val_loss: 1.1172\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1532 - val_loss: 1.1001\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1527 - val_loss: 1.1203\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1512 - val_loss: 1.0999\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1509 - val_loss: 1.0961\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1492 - val_loss: 1.1101\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1479 - val_loss: 1.0709\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1467 - val_loss: 1.0762\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1460 - val_loss: 1.0960\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1452 - val_loss: 1.0914\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1441 - val_loss: 1.1056\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1425 - val_loss: 1.0610\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1414 - val_loss: 1.0734\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1409 - val_loss: 1.0577\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1396 - val_loss: 1.0771\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1393 - val_loss: 1.0541\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1371 - val_loss: 1.0471\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1374 - val_loss: 1.0493\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1355 - val_loss: 1.0239\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1351 - val_loss: 1.0341\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1348 - val_loss: 1.0458\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1332 - val_loss: 1.0530\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1329 - val_loss: 1.0234\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1316 - val_loss: 1.0243\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1307 - val_loss: 1.0412\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1302 - val_loss: 1.0169\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1298 - val_loss: 1.0397\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1279 - val_loss: 1.0060\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1268 - val_loss: 1.0204\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1268 - val_loss: 1.0423\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1264 - val_loss: 0.9992\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1249 - val_loss: 1.0194\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1247 - val_loss: 1.0068\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1238 - val_loss: 0.9944\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1231 - val_loss: 1.0033\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1223 - val_loss: 1.0039\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1212 - val_loss: 0.9947\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1209 - val_loss: 0.9964\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1200 - val_loss: 0.9794\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1210 - val_loss: 0.9940\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1190 - val_loss: 0.9828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1184 - val_loss: 0.9732\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1174 - val_loss: 0.9753\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1167 - val_loss: 0.9772\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1153 - val_loss: 0.9577\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.1154 - val_loss: 0.9613\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1150 - val_loss: 0.9529\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1146 - val_loss: 0.9642\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1137 - val_loss: 0.9429\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1132 - val_loss: 0.9583\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1116 - val_loss: 0.9657\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1116 - val_loss: 0.9449\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1115 - val_loss: 0.9538\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1105 - val_loss: 0.9388\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1099 - val_loss: 0.9430\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1094 - val_loss: 0.9321\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1089 - val_loss: 0.9367\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1086 - val_loss: 0.9281\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1077 - val_loss: 0.9419\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1068 - val_loss: 0.9289\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1073 - val_loss: 0.9370\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1063 - val_loss: 0.9490\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1056 - val_loss: 0.9226\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1050 - val_loss: 0.9206\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1043 - val_loss: 0.9066\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1037 - val_loss: 0.9381\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1032 - val_loss: 0.9385\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1034 - val_loss: 0.9198\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1025 - val_loss: 0.9321\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1015 - val_loss: 0.9014\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1016 - val_loss: 0.9278\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1010 - val_loss: 0.8988\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1011 - val_loss: 0.8904\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1003 - val_loss: 0.8853\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0996 - val_loss: 0.9056\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0993 - val_loss: 0.8970\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0984 - val_loss: 0.9095\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0988 - val_loss: 0.9003\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0980 - val_loss: 0.8934\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0981 - val_loss: 0.8958\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0968 - val_loss: 0.8930\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0966 - val_loss: 0.8932\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0964 - val_loss: 0.9066\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0960 - val_loss: 0.8847\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0953 - val_loss: 0.8959\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0954 - val_loss: 0.8734\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0946 - val_loss: 0.8785\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0940 - val_loss: 0.8716\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0941 - val_loss: 0.8726\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0934 - val_loss: 0.8786\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0933 - val_loss: 0.8688\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0927 - val_loss: 0.8737\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0923 - val_loss: 0.8686\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0919 - val_loss: 0.8646\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0918 - val_loss: 0.8735\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0914 - val_loss: 0.8724\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0911 - val_loss: 0.8534\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0902 - val_loss: 0.8567\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0899 - val_loss: 0.8699\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0894 - val_loss: 0.8457\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0892 - val_loss: 0.8487\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0893 - val_loss: 0.8491\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0889 - val_loss: 0.8539\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0883 - val_loss: 0.8502\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0877 - val_loss: 0.8455\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0883 - val_loss: 0.8334\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0874 - val_loss: 0.8370\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0871 - val_loss: 0.8420\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0868 - val_loss: 0.8321\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0860 - val_loss: 0.8432\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0863 - val_loss: 0.8324\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0854 - val_loss: 0.8228\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0853 - val_loss: 0.8227\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0849 - val_loss: 0.8202\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0841 - val_loss: 0.8341\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0843 - val_loss: 0.8187\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0844 - val_loss: 0.8198\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0844 - val_loss: 0.8190\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0837 - val_loss: 0.8137\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0831 - val_loss: 0.8262\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0824 - val_loss: 0.8164\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0821 - val_loss: 0.8069\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0821 - val_loss: 0.8303\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0823 - val_loss: 0.8150\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0817 - val_loss: 0.8120\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0815 - val_loss: 0.8136\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0809 - val_loss: 0.8000\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0808 - val_loss: 0.8109\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0800 - val_loss: 0.7968\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 1s 154us/step - loss: 0.0803 - val_loss: 0.7979\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0799 - val_loss: 0.8050\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0807 - val_loss: 0.7915\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0796 - val_loss: 0.7865\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0789 - val_loss: 0.8004\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0789 - val_loss: 0.7902\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0783 - val_loss: 0.7883\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0776 - val_loss: 0.7940\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 447us/step - loss: 24.8974 - val_loss: 10.0197\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 4.8008 - val_loss: 4.7533\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 2.2787 - val_loss: 3.3411\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 1.5342 - val_loss: 2.8408\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 1.2050 - val_loss: 2.4356\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 1.0112 - val_loss: 2.2370\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.8828 - val_loss: 2.1490\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.7968 - val_loss: 2.0976\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.7162 - val_loss: 1.9405\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.6638 - val_loss: 2.1269\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.6260 - val_loss: 1.9342\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.5820 - val_loss: 1.8861\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.5503 - val_loss: 1.7345\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.5234 - val_loss: 1.7371\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.4928 - val_loss: 1.6805\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.4714 - val_loss: 1.6256\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.4475 - val_loss: 1.5802\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.4304 - val_loss: 1.5706\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.4115 - val_loss: 1.5184\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.4004 - val_loss: 1.5115\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3810 - val_loss: 1.5594\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3729 - val_loss: 1.4788\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3593 - val_loss: 1.4414\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.3477 - val_loss: 1.4103\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3358 - val_loss: 1.3658\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.3273 - val_loss: 1.3892\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3212 - val_loss: 1.3564\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3093 - val_loss: 1.3474\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3029 - val_loss: 1.3213\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2904 - val_loss: 1.3150\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2858 - val_loss: 1.2685\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2771 - val_loss: 1.2471\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2735 - val_loss: 1.2890\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2678 - val_loss: 1.2857\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2596 - val_loss: 1.3638\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.2512 - val_loss: 1.2852\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2488 - val_loss: 1.2355\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2440 - val_loss: 1.3118\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2406 - val_loss: 1.2174\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.2310 - val_loss: 1.2196\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2295 - val_loss: 1.1479\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2214 - val_loss: 1.1590\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2172 - val_loss: 1.1499\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2126 - val_loss: 1.1236\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2174 - val_loss: 1.1347\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2076 - val_loss: 1.1219\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2024 - val_loss: 1.1555\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1986 - val_loss: 1.0902\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1955 - val_loss: 1.1183\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1925 - val_loss: 1.0468\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1895 - val_loss: 1.0705\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1875 - val_loss: 1.0576\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1817 - val_loss: 1.0492\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1825 - val_loss: 1.0621\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1778 - val_loss: 1.0353\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1766 - val_loss: 1.0577\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1725 - val_loss: 1.0297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1702 - val_loss: 1.0139\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1695 - val_loss: 0.9876\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1654 - val_loss: 0.9924\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1623 - val_loss: 1.0340\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1620 - val_loss: 1.0044\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1592 - val_loss: 1.0182\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1583 - val_loss: 0.9872\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1566 - val_loss: 0.9408\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1544 - val_loss: 0.9625\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1496 - val_loss: 1.0146\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1504 - val_loss: 0.9686\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1475 - val_loss: 0.9554\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1467 - val_loss: 0.9660\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1444 - val_loss: 0.9353\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1437 - val_loss: 0.9173\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1410 - val_loss: 1.0003\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1393 - val_loss: 0.9204\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1391 - val_loss: 0.9449\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1369 - val_loss: 0.9288\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1362 - val_loss: 0.9457\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1354 - val_loss: 0.8997\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1321 - val_loss: 0.9110\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1302 - val_loss: 0.9074\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1304 - val_loss: 0.9109\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1279 - val_loss: 0.8703\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1262 - val_loss: 0.9065\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1270 - val_loss: 0.8987\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1229 - val_loss: 0.8734\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1236 - val_loss: 0.8909\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1216 - val_loss: 0.8679\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1200 - val_loss: 0.9050\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1193 - val_loss: 0.8979\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1185 - val_loss: 0.8761\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1165 - val_loss: 0.8487\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1153 - val_loss: 0.9303\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1152 - val_loss: 0.8467\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1163 - val_loss: 0.9811\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1145 - val_loss: 0.8442\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1117 - val_loss: 0.8705\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1121 - val_loss: 0.8834\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1098 - val_loss: 0.8440\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1083 - val_loss: 0.9012\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1084 - val_loss: 0.8634\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1073 - val_loss: 0.8319\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1071 - val_loss: 0.8713\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1048 - val_loss: 0.8242\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1048 - val_loss: 0.8156\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1043 - val_loss: 0.8322\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1021 - val_loss: 0.8569\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1036 - val_loss: 0.8459\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1026 - val_loss: 0.8315\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1007 - val_loss: 0.8233\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1003 - val_loss: 0.8341\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0989 - val_loss: 0.8255\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0966 - val_loss: 0.8392\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0966 - val_loss: 0.7948\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0970 - val_loss: 0.8095\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0972 - val_loss: 0.8224\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0973 - val_loss: 0.8382\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0941 - val_loss: 0.7939\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0941 - val_loss: 1.4432\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1008 - val_loss: 0.8379\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0937 - val_loss: 0.8094\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0910 - val_loss: 0.7799\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0923 - val_loss: 0.8161\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0913 - val_loss: 0.7837\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0899 - val_loss: 0.7614\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0899 - val_loss: 0.7942\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0890 - val_loss: 0.7731\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0890 - val_loss: 0.7794\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0875 - val_loss: 0.7641\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0869 - val_loss: 0.7899\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0866 - val_loss: 0.7747\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0855 - val_loss: 0.7541\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0851 - val_loss: 0.7492\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0851 - val_loss: 0.7952\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0832 - val_loss: 0.7861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0850 - val_loss: 0.7895\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0849 - val_loss: 0.7610\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0829 - val_loss: 0.7587\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0825 - val_loss: 0.8140\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0824 - val_loss: 0.8697\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0807 - val_loss: 0.7670\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0800 - val_loss: 0.7437\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0811 - val_loss: 0.7558\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0793 - val_loss: 0.7653\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0782 - val_loss: 0.7474\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0792 - val_loss: 0.7586\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0784 - val_loss: 0.7532\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0771 - val_loss: 0.7436\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0777 - val_loss: 0.7395\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0776 - val_loss: 0.7227\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0757 - val_loss: 0.7109\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0763 - val_loss: 0.7141\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0753 - val_loss: 0.7423\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0770 - val_loss: 0.7231\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0757 - val_loss: 0.7491\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0743 - val_loss: 0.7235\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0732 - val_loss: 0.7140\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0740 - val_loss: 0.7135\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0731 - val_loss: 0.7159\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0717 - val_loss: 0.7193\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0720 - val_loss: 0.7176\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0716 - val_loss: 0.7174\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0711 - val_loss: 0.7100\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0712 - val_loss: 0.6954\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0829 - val_loss: 0.6867\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0720 - val_loss: 0.6971\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0719 - val_loss: 0.6995\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0701 - val_loss: 0.6834\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0682 - val_loss: 0.6977\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0685 - val_loss: 0.7162\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0683 - val_loss: 0.6775\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0686 - val_loss: 0.7089\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0719 - val_loss: 0.7067\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0696 - val_loss: 0.7169\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0671 - val_loss: 0.7107\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0669 - val_loss: 0.6878\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0655 - val_loss: 0.7041\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0676 - val_loss: 0.6846\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0659 - val_loss: 0.6943\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0654 - val_loss: 0.7000\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0643 - val_loss: 0.6802\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0649 - val_loss: 0.7096\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0643 - val_loss: 0.6840\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0646 - val_loss: 0.6881\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0675 - val_loss: 0.6839\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0636 - val_loss: 0.6796\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0626 - val_loss: 0.6812\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0644 - val_loss: 0.6663\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0629 - val_loss: 0.6806\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0688 - val_loss: 0.6728\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0623 - val_loss: 0.6840\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0634 - val_loss: 0.6840\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0610 - val_loss: 0.6621\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0669 - val_loss: 0.6814\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0641 - val_loss: 0.6577\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0644 - val_loss: 0.6963\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0605 - val_loss: 0.6546\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0606 - val_loss: 0.6635\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0593 - val_loss: 0.6472\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0586 - val_loss: 0.6585\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0602 - val_loss: 0.6654\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0672 - val_loss: 0.6732\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0588 - val_loss: 1.0432\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0614 - val_loss: 0.6713\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0579 - val_loss: 0.6429\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0574 - val_loss: 0.6597\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0577 - val_loss: 0.6616\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0582 - val_loss: 0.6581\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0572 - val_loss: 0.6770\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0581 - val_loss: 0.6592\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0579 - val_loss: 0.6456\n",
      "Epoch 211/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0564 - val_loss: 0.6390\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0575 - val_loss: 0.6307\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0555 - val_loss: 0.6674\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0551 - val_loss: 0.6458\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0552 - val_loss: 0.6480\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0557 - val_loss: 0.6489\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0546 - val_loss: 0.6447\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0544 - val_loss: 0.6530\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0559 - val_loss: 0.6381\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0547 - val_loss: 0.6406\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0562 - val_loss: 0.6273\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0546 - val_loss: 0.6448\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0538 - val_loss: 0.6577\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0538 - val_loss: 0.6277\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0540 - val_loss: 0.6349\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0544 - val_loss: 0.6477\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0553 - val_loss: 0.6363\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0529 - val_loss: 0.6326\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0519 - val_loss: 0.6174\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0530 - val_loss: 0.6288\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0528 - val_loss: 0.6431\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0530 - val_loss: 0.6327\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0550 - val_loss: 0.6351\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0613 - val_loss: 0.6320\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0566 - val_loss: 0.6466\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0528 - val_loss: 0.6625\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0543 - val_loss: 0.6408\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0517 - val_loss: 0.6442\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0511 - val_loss: 0.6214\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0506 - val_loss: 0.6160\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0504 - val_loss: 0.6083\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0510 - val_loss: 0.6233\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0499 - val_loss: 0.6329\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0494 - val_loss: 0.6289\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0504 - val_loss: 0.6290\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0497 - val_loss: 0.6324\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0489 - val_loss: 0.6261\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0493 - val_loss: 0.6221\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0492 - val_loss: 0.6140\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0489 - val_loss: 0.6142\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 447us/step - loss: 18.0184 - val_loss: 5.9111\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 2.8339 - val_loss: 2.9204\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 1.4813 - val_loss: 2.4380\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 1.0845 - val_loss: 2.2143\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.9172 - val_loss: 2.0135\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.7858 - val_loss: 1.9150\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.7047 - val_loss: 1.8671\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.6438 - val_loss: 1.7570\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.5834 - val_loss: 1.7924\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.5387 - val_loss: 1.6389\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.4938 - val_loss: 1.8387\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.4701 - val_loss: 1.5875\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.4371 - val_loss: 1.5336\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.4195 - val_loss: 1.4067\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.3904 - val_loss: 1.4635\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.3736 - val_loss: 1.4212\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.3573 - val_loss: 1.3698\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.3514 - val_loss: 1.3369\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.3246 - val_loss: 1.3413\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.3206 - val_loss: 1.3480\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.3005 - val_loss: 1.2758\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2873 - val_loss: 1.2418\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.2774 - val_loss: 1.2629\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2711 - val_loss: 1.2029\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.2631 - val_loss: 1.3000\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2567 - val_loss: 1.1916\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2401 - val_loss: 1.1850\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2350 - val_loss: 1.1327\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2294 - val_loss: 1.1733\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2208 - val_loss: 1.1504\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2141 - val_loss: 1.1694\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2119 - val_loss: 1.1380\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.2079 - val_loss: 1.1181\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.2464 - val_loss: 1.0920\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2652 - val_loss: 1.0511\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2015 - val_loss: 1.0773\n",
      "Epoch 37/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1885 - val_loss: 1.0213\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1888 - val_loss: 1.0389\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1793 - val_loss: 1.0134\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1758 - val_loss: 0.9764\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1709 - val_loss: 0.9877\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1691 - val_loss: 1.0082\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2019 - val_loss: 1.0021\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1743 - val_loss: 0.9637\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1614 - val_loss: 0.9668\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1658 - val_loss: 0.9546\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1602 - val_loss: 0.9570\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1494 - val_loss: 0.9230\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1478 - val_loss: 0.9469\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1450 - val_loss: 0.9326\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1420 - val_loss: 0.9482\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.1390 - val_loss: 0.9163\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1399 - val_loss: 0.9048\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1326 - val_loss: 0.9824\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1398 - val_loss: 0.9184\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1334 - val_loss: 0.9098\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1452 - val_loss: 0.8698\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1258 - val_loss: 0.8642\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1283 - val_loss: 0.8889\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1284 - val_loss: 0.8676\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1232 - val_loss: 0.8609\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1218 - val_loss: 0.8281\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1207 - val_loss: 0.9334\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1180 - val_loss: 1.0075\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1207 - val_loss: 0.8353\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1140 - val_loss: 0.8254\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1131 - val_loss: 1.1174\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1101 - val_loss: 0.8242\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1114 - val_loss: 0.8670\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1131 - val_loss: 0.8755\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1303 - val_loss: 0.8382\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1108 - val_loss: 0.8483\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1080 - val_loss: 0.8221\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1093 - val_loss: 0.8024\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0992 - val_loss: 0.8040\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0988 - val_loss: 0.7896\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0975 - val_loss: 0.7848\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1058 - val_loss: 0.8027\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1034 - val_loss: 0.7767\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.1016 - val_loss: 0.8293\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.1007 - val_loss: 0.7925\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0943 - val_loss: 0.7645\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0922 - val_loss: 0.7918\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0907 - val_loss: 0.7862\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0888 - val_loss: 0.7739\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0897 - val_loss: 0.7858\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1007 - val_loss: 0.7704\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0979 - val_loss: 0.7493\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0872 - val_loss: 0.7397\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0893 - val_loss: 0.7498\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0902 - val_loss: 0.7490\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0844 - val_loss: 0.7893\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0863 - val_loss: 0.7408\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0853 - val_loss: 0.7541\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0889 - val_loss: 0.7754\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0895 - val_loss: 0.7518\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0839 - val_loss: 0.7172\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0814 - val_loss: 0.7222\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0859 - val_loss: 0.7275\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0827 - val_loss: 0.7263\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0800 - val_loss: 0.7259\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0787 - val_loss: 0.7109\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0768 - val_loss: 0.7132\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0773 - val_loss: 0.7070\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0787 - val_loss: 0.6929\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0756 - val_loss: 0.7288\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0821 - val_loss: 0.7013\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0744 - val_loss: 0.6873\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0751 - val_loss: 0.7087\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0731 - val_loss: 0.6879\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0736 - val_loss: 0.7226\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0744 - val_loss: 0.6765\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0746 - val_loss: 0.7361\n",
      "Epoch 114/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0716 - val_loss: 0.6903\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0718 - val_loss: 0.6997\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0695 - val_loss: 0.6644\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0702 - val_loss: 0.6671\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0668 - val_loss: 0.6585\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0737 - val_loss: 0.6597\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0747 - val_loss: 0.6880\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0763 - val_loss: 0.6957\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0724 - val_loss: 0.6734\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0680 - val_loss: 0.6628\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0658 - val_loss: 0.6561\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0671 - val_loss: 0.6458\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0648 - val_loss: 0.6767\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0699 - val_loss: 0.6716\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0629 - val_loss: 0.6898\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0646 - val_loss: 0.6526\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0661 - val_loss: 0.6971\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0882 - val_loss: 0.6355\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0651 - val_loss: 0.7065\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0646 - val_loss: 0.6824\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0681 - val_loss: 0.7941\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0682 - val_loss: 0.6367\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0689 - val_loss: 0.6515\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0613 - val_loss: 0.6821\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0607 - val_loss: 0.6323\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0618 - val_loss: 0.6218\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0739 - val_loss: 0.6416\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0629 - val_loss: 0.6373\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0721 - val_loss: 0.6728\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0952 - val_loss: 0.6455\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0945 - val_loss: 0.7406\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0803 - val_loss: 0.6963\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0803 - val_loss: 0.6938\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0582 - val_loss: 0.6459\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0557 - val_loss: 0.6644\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0569 - val_loss: 0.6481\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0572 - val_loss: 0.6405\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0551 - val_loss: 0.6968\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0560 - val_loss: 0.6410\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0550 - val_loss: 0.6688\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0550 - val_loss: 0.6620\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0543 - val_loss: 0.7526\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0581 - val_loss: 0.6774\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0558 - val_loss: 0.6471\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0528 - val_loss: 0.6567\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0552 - val_loss: 0.6488\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0518 - val_loss: 0.6901\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0526 - val_loss: 0.6263\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0541 - val_loss: 0.6173\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 154us/step - loss: 0.0530 - val_loss: 0.6316\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0555 - val_loss: 0.6314\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0530 - val_loss: 0.6102\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0539 - val_loss: 0.6270\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0513 - val_loss: 0.6296\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0516 - val_loss: 0.6196\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0505 - val_loss: 0.6139\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0543 - val_loss: 0.6398\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0539 - val_loss: 0.6453\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0570 - val_loss: 0.6536\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0498 - val_loss: 0.6158\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0500 - val_loss: 0.6152\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0497 - val_loss: 0.6240\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0491 - val_loss: 0.6860\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0624 - val_loss: 0.6199\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0490 - val_loss: 0.6164\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0549 - val_loss: 5.5546\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0929 - val_loss: 0.6289\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0496 - val_loss: 0.6135\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0511 - val_loss: 0.6172\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0485 - val_loss: 0.6219\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0526 - val_loss: 0.6296\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0512 - val_loss: 0.6364\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0572 - val_loss: 0.6329\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0491 - val_loss: 0.6188\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0468 - val_loss: 0.6145\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0487 - val_loss: 0.6151\n",
      "Epoch 190/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0667 - val_loss: 0.6382\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0494 - val_loss: 0.6335\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0518 - val_loss: 0.6084\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0586 - val_loss: 0.6155\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0563 - val_loss: 0.6092\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0524 - val_loss: 0.6038\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0486 - val_loss: 0.6342\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0485 - val_loss: 0.6157\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0445 - val_loss: 0.6301\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0494 - val_loss: 0.6063\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0436 - val_loss: 0.6020\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0438 - val_loss: 0.6042\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0454 - val_loss: 0.6250\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0436 - val_loss: 0.6077\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0443 - val_loss: 0.6158\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0438 - val_loss: 0.5945\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0494 - val_loss: 0.5974\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0698 - val_loss: 0.6694\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0541 - val_loss: 0.6056\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0427 - val_loss: 0.6112\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0465 - val_loss: 0.5928\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0436 - val_loss: 0.6104\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0739 - val_loss: 0.5977\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0615 - val_loss: 0.5836\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0529 - val_loss: 0.6137\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0508 - val_loss: 0.6051\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0551 - val_loss: 0.6055\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0618 - val_loss: 0.6067\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0444 - val_loss: 0.6166\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0430 - val_loss: 0.6049\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0539 - val_loss: 0.6291\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0412 - val_loss: 0.6149\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0407 - val_loss: 0.6046\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0438 - val_loss: 0.6081\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0403 - val_loss: 0.6062\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0436 - val_loss: 0.5926\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0436 - val_loss: 0.6193\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0435 - val_loss: 0.6414\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0571 - val_loss: 0.5979\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0477 - val_loss: 0.5882\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0480 - val_loss: 0.5893\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0490 - val_loss: 0.5924\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0684 - val_loss: 0.6139\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0421 - val_loss: 0.5903\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0442 - val_loss: 0.5968\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0409 - val_loss: 0.5871\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0389 - val_loss: 0.6002\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0409 - val_loss: 0.6512\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0401 - val_loss: 0.5872\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0410 - val_loss: 0.6054\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0402 - val_loss: 0.5840\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0420 - val_loss: 0.5949\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0424 - val_loss: 0.6307\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0499 - val_loss: 0.5998\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0569 - val_loss: 0.6957\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0722 - val_loss: 0.6000\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0881 - val_loss: 0.6026\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0460 - val_loss: 0.6025\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0629 - val_loss: 0.5967\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1599 - val_loss: 0.6102\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0507 - val_loss: 0.5979\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 451us/step - loss: 15.0037 - val_loss: 4.7292\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 2.2265 - val_loss: 3.0791\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 1.3726 - val_loss: 2.4257\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.9205 - val_loss: 1.9964\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.7672 - val_loss: 2.0599\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.6738 - val_loss: 1.7915\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.5940 - val_loss: 1.6622\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.5372 - val_loss: 1.6083\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.5012 - val_loss: 1.6171\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.4585 - val_loss: 1.4826\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.4203 - val_loss: 1.4696\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.3863 - val_loss: 1.4726\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.3745 - val_loss: 1.3248\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3637 - val_loss: 1.3272\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.3334 - val_loss: 1.3164\n",
      "Epoch 16/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.3152 - val_loss: 1.2983\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2946 - val_loss: 1.2595\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2835 - val_loss: 1.3035\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2673 - val_loss: 1.1863\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2532 - val_loss: 1.1317\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.2435 - val_loss: 1.1865\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.2369 - val_loss: 1.1603\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2254 - val_loss: 1.0849\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2186 - val_loss: 1.0256\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2071 - val_loss: 1.2596\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.2033 - val_loss: 0.9741\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2009 - val_loss: 1.1341\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1914 - val_loss: 0.9991\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1883 - val_loss: 0.9976\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1891 - val_loss: 0.9174\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1740 - val_loss: 0.9744\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1702 - val_loss: 0.9211\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1824 - val_loss: 0.8805\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1911 - val_loss: 0.8908\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1634 - val_loss: 0.8692\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1612 - val_loss: 0.8748\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1511 - val_loss: 0.8738\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1490 - val_loss: 0.8186\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1492 - val_loss: 0.8353\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1552 - val_loss: 0.7960\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1445 - val_loss: 0.7934\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1422 - val_loss: 0.8105\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1336 - val_loss: 0.9702\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1316 - val_loss: 0.8334\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1277 - val_loss: 0.7817\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1303 - val_loss: 0.8172\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1252 - val_loss: 0.8279\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1252 - val_loss: 0.7192\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1226 - val_loss: 0.7879\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1186 - val_loss: 0.7383\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1221 - val_loss: 0.7361\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1200 - val_loss: 0.7489\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1138 - val_loss: 0.7388\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1216 - val_loss: 0.7200\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1410 - val_loss: 0.7375\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1203 - val_loss: 0.7499\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1120 - val_loss: 0.7635\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1088 - val_loss: 0.7046\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1053 - val_loss: 0.7150\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1051 - val_loss: 0.6784\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1035 - val_loss: 0.6917\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1153 - val_loss: 0.6422\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1101 - val_loss: 0.6629\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0972 - val_loss: 0.7144\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0987 - val_loss: 0.6881\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0967 - val_loss: 0.6868\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1073 - val_loss: 0.6646\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0933 - val_loss: 0.6915\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0961 - val_loss: 0.6493\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0944 - val_loss: 0.6524\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0918 - val_loss: 0.6586\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1233 - val_loss: 0.6782\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0959 - val_loss: 0.6573\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1271 - val_loss: 0.6503\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0951 - val_loss: 0.6484\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0939 - val_loss: 0.6824\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1011 - val_loss: 0.6474\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0886 - val_loss: 0.6099\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0820 - val_loss: 0.6867\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0847 - val_loss: 0.6244\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0814 - val_loss: 0.6345\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0862 - val_loss: 0.6448\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1125 - val_loss: 0.6815\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0823 - val_loss: 0.6092\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0776 - val_loss: 0.6420\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0759 - val_loss: 0.6365\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0829 - val_loss: 0.6139\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0838 - val_loss: 0.6278\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0902 - val_loss: 0.6387\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0727 - val_loss: 0.6506\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0738 - val_loss: 0.6178\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0773 - val_loss: 0.6559\n",
      "Epoch 93/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0750 - val_loss: 0.5887\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0720 - val_loss: 0.6008\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0746 - val_loss: 0.6391\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0736 - val_loss: 0.6542\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0686 - val_loss: 0.5897\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0689 - val_loss: 0.6248\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0673 - val_loss: 0.6047\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0653 - val_loss: 0.5955\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0680 - val_loss: 0.5885\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0647 - val_loss: 0.5905\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0692 - val_loss: 0.5826\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0709 - val_loss: 0.6265\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0658 - val_loss: 0.6473\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0632 - val_loss: 0.5888\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0670 - val_loss: 0.5835\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0643 - val_loss: 0.5999\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0671 - val_loss: 0.5622\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0635 - val_loss: 0.5769\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0640 - val_loss: 0.5509\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0648 - val_loss: 0.5796\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0616 - val_loss: 0.5845\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0619 - val_loss: 0.5736\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0883 - val_loss: 0.8282\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0731 - val_loss: 0.5444\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0754 - val_loss: 0.6734\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0690 - val_loss: 0.5594\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0615 - val_loss: 0.5703\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0709 - val_loss: 0.5579\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0601 - val_loss: 0.5542\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0677 - val_loss: 1.0682\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0954 - val_loss: 0.5660\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0887 - val_loss: 0.5526\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0610 - val_loss: 0.5476\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0580 - val_loss: 0.5416\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0572 - val_loss: 0.5527\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 155us/step - loss: 0.0546 - val_loss: 0.5626\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0553 - val_loss: 0.5632\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0586 - val_loss: 0.6197\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0576 - val_loss: 0.5429\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0540 - val_loss: 0.5405\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0557 - val_loss: 0.5310\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0530 - val_loss: 0.5181\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0531 - val_loss: 0.5337\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0633 - val_loss: 0.5477\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0522 - val_loss: 0.5686\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0512 - val_loss: 0.5304\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0510 - val_loss: 0.5385\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0514 - val_loss: 0.5404\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0504 - val_loss: 0.5307\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0500 - val_loss: 0.5905\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0507 - val_loss: 0.5410\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0501 - val_loss: 0.5246\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0544 - val_loss: 0.5332\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0949 - val_loss: 0.7616\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1017 - val_loss: 0.5702\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0552 - val_loss: 0.5361\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0500 - val_loss: 0.5268\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0514 - val_loss: 0.5416\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0526 - val_loss: 0.5443\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0842 - val_loss: 0.5325\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1304 - val_loss: 0.6221\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0968 - val_loss: 0.5581\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.1093 - val_loss: 0.5902\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0572 - val_loss: 0.5318\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0485 - val_loss: 0.5481\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0513 - val_loss: 0.5366\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0490 - val_loss: 0.5349\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0479 - val_loss: 0.5576\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0462 - val_loss: 0.5458\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0489 - val_loss: 0.5720\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0494 - val_loss: 0.5667\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0453 - val_loss: 0.5448\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0462 - val_loss: 0.5597\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0459 - val_loss: 0.5332\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0461 - val_loss: 0.5354\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0481 - val_loss: 0.5418\n",
      "Epoch 169/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0432 - val_loss: 0.5540\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0456 - val_loss: 0.5540\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0746 - val_loss: 0.5270\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0605 - val_loss: 0.5615\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0717 - val_loss: 0.5550\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0492 - val_loss: 0.5567\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0445 - val_loss: 0.5409\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0455 - val_loss: 0.5599\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0419 - val_loss: 0.5408\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0418 - val_loss: 0.5184\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0433 - val_loss: 0.5316\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0418 - val_loss: 0.5215\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0435 - val_loss: 0.5220\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0407 - val_loss: 0.5349\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0437 - val_loss: 0.5067\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0402 - val_loss: 0.5327\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0396 - val_loss: 0.5217\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0421 - val_loss: 0.5354\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0408 - val_loss: 0.5399\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0402 - val_loss: 0.5296\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0413 - val_loss: 0.5275\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0482 - val_loss: 0.5495\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0430 - val_loss: 0.5172\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0427 - val_loss: 0.5422\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0386 - val_loss: 0.5583\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0420 - val_loss: 0.5303\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0405 - val_loss: 0.5265\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0387 - val_loss: 0.5412\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0402 - val_loss: 0.5294\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0391 - val_loss: 0.5254\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0386 - val_loss: 0.5291\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0373 - val_loss: 0.5334\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0377 - val_loss: 0.5274\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0370 - val_loss: 0.5262\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0362 - val_loss: 0.5151\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0408 - val_loss: 0.5071\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0399 - val_loss: 0.5230\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0365 - val_loss: 0.5180\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0400 - val_loss: 0.5150\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0381 - val_loss: 0.5360\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0394 - val_loss: 0.5252\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0411 - val_loss: 0.5260\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0392 - val_loss: 0.5238\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0403 - val_loss: 0.5191\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0377 - val_loss: 0.5347\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0363 - val_loss: 0.5222\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0378 - val_loss: 0.5206\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0363 - val_loss: 0.5270\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0384 - val_loss: 0.5493\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0358 - val_loss: 0.7310\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0543 - val_loss: 0.5094\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0381 - val_loss: 0.5090\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0370 - val_loss: 0.5360\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0346 - val_loss: 0.5185\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0417 - val_loss: 0.5213\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0385 - val_loss: 0.5149\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0355 - val_loss: 0.5457\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0357 - val_loss: 0.5281\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0357 - val_loss: 0.5067\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0340 - val_loss: 0.5133\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0348 - val_loss: 0.5362\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0362 - val_loss: 0.5431\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0356 - val_loss: 0.5288\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0344 - val_loss: 0.4997\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0339 - val_loss: 0.5171\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0330 - val_loss: 0.5327\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0339 - val_loss: 0.5020\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0362 - val_loss: 0.5321\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0351 - val_loss: 0.5157\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0360 - val_loss: 0.5007\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0339 - val_loss: 0.5286\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0335 - val_loss: 0.5035\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0328 - val_loss: 0.5002\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0339 - val_loss: 0.5134\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0343 - val_loss: 0.5132\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0400 - val_loss: 0.5210\n",
      "Epoch 245/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0326 - val_loss: 0.5145\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0418 - val_loss: 0.5372\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0349 - val_loss: 0.5483\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0413 - val_loss: 0.5250\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0425 - val_loss: 0.5159\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0352 - val_loss: 0.5364\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 450us/step - loss: 13.4365 - val_loss: 3.7906\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 1.9157 - val_loss: 4.3813\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 1.0389 - val_loss: 2.2067\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.7919 - val_loss: 2.1858\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.6716 - val_loss: 2.1458\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.5701 - val_loss: 1.7917\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.5077 - val_loss: 1.6275\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.4720 - val_loss: 1.6013\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.4269 - val_loss: 1.6450\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3965 - val_loss: 1.4453\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.3670 - val_loss: 1.4842\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.3413 - val_loss: 1.4369\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.3224 - val_loss: 1.3392\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.2995 - val_loss: 1.2708\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2769 - val_loss: 1.2460\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.2756 - val_loss: 1.2101\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2542 - val_loss: 1.1632\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2479 - val_loss: 1.1220\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2333 - val_loss: 1.1696\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2299 - val_loss: 1.2587\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.2177 - val_loss: 1.0819\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2183 - val_loss: 1.0322\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.2066 - val_loss: 1.0737\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1961 - val_loss: 1.0074\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1818 - val_loss: 0.9395\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1846 - val_loss: 0.9630\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1863 - val_loss: 0.9443\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.2050 - val_loss: 0.9008\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1645 - val_loss: 0.9752\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1666 - val_loss: 0.9213\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1510 - val_loss: 0.8361\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1518 - val_loss: 0.8351\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1447 - val_loss: 0.8302\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1467 - val_loss: 0.8986\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1716 - val_loss: 0.7418\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1739 - val_loss: 0.8267\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1380 - val_loss: 0.7889\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1310 - val_loss: 0.7446\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1516 - val_loss: 0.8165\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2051 - val_loss: 0.7421\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1235 - val_loss: 0.7617\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1296 - val_loss: 0.7296\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1173 - val_loss: 0.6838\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1204 - val_loss: 0.7063\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1239 - val_loss: 0.7042\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1107 - val_loss: 0.7224\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1150 - val_loss: 0.7021\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1249 - val_loss: 0.6745\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1118 - val_loss: 0.7115\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1110 - val_loss: 0.6576\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1021 - val_loss: 0.6793\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1054 - val_loss: 0.6626\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1042 - val_loss: 0.6191\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1011 - val_loss: 0.6445\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0997 - val_loss: 0.6443\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1110 - val_loss: 0.6264\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0953 - val_loss: 0.6032\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1014 - val_loss: 0.6458\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0979 - val_loss: 0.6169\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1319 - val_loss: 0.6737\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1040 - val_loss: 0.6205\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0945 - val_loss: 0.6603\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0914 - val_loss: 0.6062\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0877 - val_loss: 0.6384\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0830 - val_loss: 0.6024\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0850 - val_loss: 0.5908\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0841 - val_loss: 0.5917\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0870 - val_loss: 0.6210\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0849 - val_loss: 0.5719\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0832 - val_loss: 0.5707\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0782 - val_loss: 0.5531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0800 - val_loss: 0.5682\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0772 - val_loss: 0.5490\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0802 - val_loss: 0.5754\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0798 - val_loss: 0.6125\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0769 - val_loss: 0.5571\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0768 - val_loss: 0.5795\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0720 - val_loss: 0.5632\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0763 - val_loss: 0.5490\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0713 - val_loss: 0.7151\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0786 - val_loss: 0.5553\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0764 - val_loss: 0.5395\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0692 - val_loss: 0.5432\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0773 - val_loss: 0.5785\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0709 - val_loss: 0.5370\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0669 - val_loss: 0.5498\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0684 - val_loss: 0.8418\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0691 - val_loss: 0.5316\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0759 - val_loss: 0.5632\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0677 - val_loss: 0.5466\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0625 - val_loss: 0.5372\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0628 - val_loss: 0.5232\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0724 - val_loss: 0.5408\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0658 - val_loss: 0.5232\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0661 - val_loss: 0.5357\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0653 - val_loss: 0.5193\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0570 - val_loss: 0.5452\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0597 - val_loss: 0.5172\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0678 - val_loss: 0.5681\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0616 - val_loss: 0.5238\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0587 - val_loss: 0.5393\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0568 - val_loss: 0.5468\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0581 - val_loss: 0.5452\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0792 - val_loss: 0.6252\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0787 - val_loss: 0.5195\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0626 - val_loss: 0.5372\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0557 - val_loss: 0.5011\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0622 - val_loss: 0.5401\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0636 - val_loss: 0.5101\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0576 - val_loss: 0.5142\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3876 - val_loss: 0.5609\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0847 - val_loss: 0.5626\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0618 - val_loss: 0.5645\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0696 - val_loss: 0.5268\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0648 - val_loss: 0.5378\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0588 - val_loss: 0.5436\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0567 - val_loss: 0.5447\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0549 - val_loss: 0.4951\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0540 - val_loss: 0.5032\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0527 - val_loss: 0.5376\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0519 - val_loss: 0.5068\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0521 - val_loss: 0.5183\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0528 - val_loss: 0.5256\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0513 - val_loss: 0.5284\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0505 - val_loss: 0.5086\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0511 - val_loss: 0.4972\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0538 - val_loss: 0.4933\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0543 - val_loss: 0.5146\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0504 - val_loss: 0.5043\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0484 - val_loss: 0.4967\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0477 - val_loss: 0.5075\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0472 - val_loss: 0.4990\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0511 - val_loss: 0.5371\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0510 - val_loss: 0.4992\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0627 - val_loss: 0.5166\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0577 - val_loss: 0.5410\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0471 - val_loss: 0.5137\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0486 - val_loss: 0.5396\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0458 - val_loss: 0.5247\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0464 - val_loss: 0.4857\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0475 - val_loss: 0.5384\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0517 - val_loss: 0.5114\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0449 - val_loss: 0.5199\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0450 - val_loss: 0.5102\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0463 - val_loss: 0.5037\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0442 - val_loss: 0.4833\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0450 - val_loss: 0.4915\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0438 - val_loss: 0.4961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0449 - val_loss: 0.4817\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0452 - val_loss: 0.4960\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0448 - val_loss: 0.5003\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0425 - val_loss: 0.5025\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0420 - val_loss: 0.4863\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0449 - val_loss: 0.5014\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0425 - val_loss: 0.5134\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0412 - val_loss: 0.4968\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0440 - val_loss: 0.4891\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0466 - val_loss: 0.5202\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0448 - val_loss: 0.5057\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0421 - val_loss: 0.4972\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0407 - val_loss: 0.5249\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0419 - val_loss: 0.4878\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0446 - val_loss: 0.4967\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0408 - val_loss: 0.5102\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0401 - val_loss: 0.5403\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0400 - val_loss: 0.5025\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0406 - val_loss: 0.5217\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0397 - val_loss: 0.4840\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0380 - val_loss: 0.5206\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0381 - val_loss: 0.5063\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0403 - val_loss: 0.4893\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0403 - val_loss: 0.5030\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0378 - val_loss: 0.4995\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0376 - val_loss: 0.5064\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0385 - val_loss: 0.4841\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0383 - val_loss: 0.5055\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0389 - val_loss: 0.4848\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0387 - val_loss: 0.4999\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0394 - val_loss: 0.4891\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0662 - val_loss: 0.5122\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0410 - val_loss: 0.5196\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0426 - val_loss: 0.5043\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0640 - val_loss: 0.5035\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0402 - val_loss: 0.5288\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0404 - val_loss: 0.5317\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0367 - val_loss: 0.6172\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0391 - val_loss: 0.5005\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0359 - val_loss: 0.5058\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0373 - val_loss: 0.4990\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0352 - val_loss: 0.5197\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0369 - val_loss: 0.4936\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0394 - val_loss: 0.4935\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0378 - val_loss: 0.4955\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0477 - val_loss: 0.5268\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0359 - val_loss: 0.4970\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0350 - val_loss: 0.4995\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0344 - val_loss: 0.5105\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0344 - val_loss: 0.5112\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0354 - val_loss: 0.5005\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0342 - val_loss: 0.4910\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0337 - val_loss: 0.5126\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0329 - val_loss: 0.4790\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0330 - val_loss: 0.5071\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0355 - val_loss: 0.4718\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0369 - val_loss: 0.4930\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0351 - val_loss: 0.5071\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0323 - val_loss: 0.4986\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0330 - val_loss: 0.4904\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0338 - val_loss: 0.4908\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0330 - val_loss: 0.4987\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0337 - val_loss: 0.5146\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0349 - val_loss: 0.5032\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0468 - val_loss: 0.5093\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0520 - val_loss: 0.5056\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0332 - val_loss: 0.4979\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0322 - val_loss: 0.5002\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0319 - val_loss: 0.5661\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0317 - val_loss: 0.5689\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0314 - val_loss: 0.5084\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0307 - val_loss: 0.6231\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0413 - val_loss: 0.5296\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0314 - val_loss: 0.4886\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0308 - val_loss: 0.5263\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0308 - val_loss: 0.5011\n",
      "Epoch 225/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0308 - val_loss: 0.4958\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0295 - val_loss: 0.5003\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0316 - val_loss: 0.5087\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0310 - val_loss: 0.5068\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0402 - val_loss: 0.5065\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0308 - val_loss: 0.4866\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0312 - val_loss: 0.4985\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0308 - val_loss: 0.4942\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0301 - val_loss: 0.4904\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0300 - val_loss: 0.5278\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0314 - val_loss: 0.4935\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0293 - val_loss: 0.5124\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0297 - val_loss: 0.5057\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0326 - val_loss: 0.5331\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0320 - val_loss: 0.5047\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0289 - val_loss: 0.4957\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0291 - val_loss: 0.5154\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0297 - val_loss: 0.5062\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0289 - val_loss: 0.4920\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0300 - val_loss: 0.4952\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0294 - val_loss: 0.5191\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0298 - val_loss: 0.4878\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0288 - val_loss: 0.5101\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0280 - val_loss: 0.5252\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0282 - val_loss: 0.5112\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0277 - val_loss: 0.5115\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 457us/step - loss: 11.1813 - val_loss: 3.9283\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 1.7435 - val_loss: 2.1826\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 1.0118 - val_loss: 1.9550\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.7883 - val_loss: 1.7072\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.6261 - val_loss: 1.6337\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.6145 - val_loss: 1.6694\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.5161 - val_loss: 3.5234\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.6066 - val_loss: 1.2328\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.3895 - val_loss: 1.5748\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.3576 - val_loss: 1.1675\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.3295 - val_loss: 1.0814\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.3039 - val_loss: 1.2034\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.2887 - val_loss: 1.1107\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.3141 - val_loss: 1.0123\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.2473 - val_loss: 0.9879\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2414 - val_loss: 0.9411\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2220 - val_loss: 0.9030\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2087 - val_loss: 0.8551\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.2094 - val_loss: 0.8319\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2011 - val_loss: 0.7811\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1945 - val_loss: 0.8072\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2058 - val_loss: 0.7680\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.2145 - val_loss: 0.7655\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1954 - val_loss: 0.9506\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1876 - val_loss: 0.7562\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.1560 - val_loss: 1.0211\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1603 - val_loss: 0.7145\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1763 - val_loss: 0.7205\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1451 - val_loss: 0.7386\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1407 - val_loss: 0.8886\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1620 - val_loss: 0.7840\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1416 - val_loss: 0.6537\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1346 - val_loss: 0.6564\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1238 - val_loss: 0.6334\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1235 - val_loss: 0.6482\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1207 - val_loss: 0.6066\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1137 - val_loss: 0.5807\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.1137 - val_loss: 0.6008\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1105 - val_loss: 0.6009\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1190 - val_loss: 0.5759\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1130 - val_loss: 0.5940\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1104 - val_loss: 0.6642\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1164 - val_loss: 0.5643\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1150 - val_loss: 0.5559\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1059 - val_loss: 0.5535\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1021 - val_loss: 0.6515\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.1049 - val_loss: 0.5878\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0978 - val_loss: 0.5503\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0985 - val_loss: 0.4886\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0975 - val_loss: 0.5285\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0983 - val_loss: 0.5220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0985 - val_loss: 0.5595\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0978 - val_loss: 0.4932\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0954 - val_loss: 0.5024\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0874 - val_loss: 0.5002\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0853 - val_loss: 0.5757\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0825 - val_loss: 0.4924\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0820 - val_loss: 0.4758\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0823 - val_loss: 0.4924\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0945 - val_loss: 0.4842\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0790 - val_loss: 0.4588\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0785 - val_loss: 0.5046\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0769 - val_loss: 0.4658\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0816 - val_loss: 0.4968\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0936 - val_loss: 0.4705\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0792 - val_loss: 0.4528\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.1014 - val_loss: 0.4619\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0804 - val_loss: 0.4437\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0706 - val_loss: 0.4415\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0781 - val_loss: 0.4241\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0737 - val_loss: 0.4651\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0698 - val_loss: 0.4769\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0784 - val_loss: 0.4474\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0685 - val_loss: 0.4278\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0668 - val_loss: 0.4468\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0666 - val_loss: 0.4548\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0678 - val_loss: 0.4548\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0638 - val_loss: 0.4160\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0625 - val_loss: 0.4257\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0630 - val_loss: 0.4071\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0920 - val_loss: 0.4307\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0951 - val_loss: 0.4572\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0741 - val_loss: 0.4933\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0617 - val_loss: 0.4151\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0664 - val_loss: 0.4099\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0598 - val_loss: 0.4331\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0601 - val_loss: 0.4265\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0797 - val_loss: 0.4641\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0651 - val_loss: 0.4140\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0646 - val_loss: 0.4105\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0656 - val_loss: 0.4233\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0604 - val_loss: 0.4238\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0609 - val_loss: 0.4173\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0571 - val_loss: 0.4085\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0544 - val_loss: 0.4094\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0559 - val_loss: 0.4004\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0551 - val_loss: 0.4402\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0544 - val_loss: 0.4146\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0537 - val_loss: 0.4482\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0609 - val_loss: 0.4072\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0516 - val_loss: 0.4241\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0515 - val_loss: 0.4093\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0517 - val_loss: 0.4076\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0505 - val_loss: 0.4123\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0569 - val_loss: 0.4352\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0563 - val_loss: 0.3959\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0513 - val_loss: 0.4045\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0497 - val_loss: 0.4033\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0480 - val_loss: 0.3987\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0548 - val_loss: 0.4019\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0495 - val_loss: 0.4073\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0476 - val_loss: 0.3839\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0473 - val_loss: 0.4034\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0474 - val_loss: 0.3832\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0463 - val_loss: 0.4112\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0465 - val_loss: 0.4115\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0515 - val_loss: 0.4611\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0485 - val_loss: 0.4110\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0449 - val_loss: 0.4108\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0446 - val_loss: 0.4038\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0444 - val_loss: 0.4003\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0452 - val_loss: 0.4080\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0473 - val_loss: 0.4093\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0444 - val_loss: 0.4079\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0449 - val_loss: 0.3981\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0437 - val_loss: 0.3952\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0443 - val_loss: 0.4001\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0425 - val_loss: 0.3979\n",
      "Epoch 129/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0452 - val_loss: 0.3993\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0515 - val_loss: 0.4225\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0407 - val_loss: 0.3965\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0409 - val_loss: 0.4023\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0409 - val_loss: 0.4554\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0423 - val_loss: 0.3964\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0423 - val_loss: 0.4054\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0410 - val_loss: 0.4003\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0410 - val_loss: 0.4471\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0440 - val_loss: 0.4352\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0437 - val_loss: 0.4206\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0409 - val_loss: 0.4058\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0421 - val_loss: 0.4070\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0417 - val_loss: 0.4771\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0402 - val_loss: 0.4278\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0434 - val_loss: 0.4428\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0415 - val_loss: 0.4035\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0412 - val_loss: 0.4418\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0417 - val_loss: 0.3971\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0381 - val_loss: 0.4101\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0381 - val_loss: 0.4184\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0387 - val_loss: 0.4060\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0378 - val_loss: 0.4031\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.0434 - val_loss: 0.4200\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0361 - val_loss: 0.4211\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0564 - val_loss: 0.4373\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0543 - val_loss: 0.4343\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0395 - val_loss: 0.4265\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0384 - val_loss: 0.4961\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0375 - val_loss: 0.4417\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0373 - val_loss: 0.4078\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0368 - val_loss: 0.4194\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0359 - val_loss: 0.4178\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0386 - val_loss: 0.4044\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0363 - val_loss: 0.4138\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0356 - val_loss: 0.4222\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0350 - val_loss: 0.4094\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0350 - val_loss: 0.4177\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0360 - val_loss: 0.4200\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0373 - val_loss: 0.4124\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0354 - val_loss: 0.4427\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0379 - val_loss: 0.4125\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0440 - val_loss: 0.4266\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0370 - val_loss: 0.4094\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0336 - val_loss: 0.4255\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0341 - val_loss: 0.4654\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0413 - val_loss: 0.4354\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0375 - val_loss: 0.4079\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0322 - val_loss: 0.4328\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0328 - val_loss: 0.4311\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0324 - val_loss: 0.4234\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0332 - val_loss: 0.4071\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0379 - val_loss: 0.4190\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0348 - val_loss: 0.4798\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0453 - val_loss: 0.4206\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0656 - val_loss: 0.4448\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0501 - val_loss: 0.4460\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0373 - val_loss: 0.4291\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0358 - val_loss: 0.4465\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0368 - val_loss: 0.4470\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0416 - val_loss: 0.4559\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0338 - val_loss: 0.4368\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0312 - val_loss: 0.4501\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0357 - val_loss: 0.4363\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0322 - val_loss: 0.4602\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0308 - val_loss: 0.4413\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0315 - val_loss: 0.4228\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0311 - val_loss: 0.4235\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0320 - val_loss: 0.4340\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0297 - val_loss: 0.4287\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0311 - val_loss: 0.4416\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0300 - val_loss: 0.4481\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0295 - val_loss: 0.4386\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0312 - val_loss: 0.5115\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0346 - val_loss: 0.4405\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0297 - val_loss: 0.4421\n",
      "Epoch 205/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0300 - val_loss: 0.4351\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0293 - val_loss: 0.4281\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0306 - val_loss: 0.4344\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0287 - val_loss: 0.4504\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0301 - val_loss: 0.4997\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0371 - val_loss: 0.4562\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0360 - val_loss: 0.4503\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0300 - val_loss: 0.4475\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0275 - val_loss: 0.4396\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0299 - val_loss: 0.4305\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0295 - val_loss: 0.4482\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0289 - val_loss: 0.4440\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0281 - val_loss: 0.4587\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0278 - val_loss: 0.4369\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0289 - val_loss: 0.4562\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0280 - val_loss: 0.4514\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0298 - val_loss: 0.4814\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0307 - val_loss: 0.4578\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0296 - val_loss: 0.4664\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0274 - val_loss: 0.4348\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0279 - val_loss: 0.4428\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0285 - val_loss: 0.4349\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0277 - val_loss: 0.4526\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0283 - val_loss: 0.4519\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0280 - val_loss: 0.4527\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0272 - val_loss: 0.4619\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0278 - val_loss: 0.4445\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0264 - val_loss: 0.4591\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0286 - val_loss: 0.4416\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0276 - val_loss: 0.4565\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0278 - val_loss: 0.4639\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0280 - val_loss: 0.4462\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0279 - val_loss: 0.4777\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0277 - val_loss: 0.4507\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0261 - val_loss: 0.4589\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0266 - val_loss: 0.4917\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0269 - val_loss: 0.4575\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 157us/step - loss: 0.0268 - val_loss: 0.4512\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0266 - val_loss: 0.4356\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0261 - val_loss: 0.4480\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0264 - val_loss: 0.4565\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 159us/step - loss: 0.0257 - val_loss: 0.4536\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0376 - val_loss: 0.4524\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0280 - val_loss: 0.4721\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0298 - val_loss: 0.5495\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 158us/step - loss: 0.0280 - val_loss: 0.4737\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 462us/step - loss: 10.3345 - val_loss: 4.6738\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 1.8222 - val_loss: 2.4906\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.9706 - val_loss: 2.2045\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.7345 - val_loss: 1.7298\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 156us/step - loss: 0.6389 - val_loss: 1.6845\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.5425 - val_loss: 1.5719\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.6026 - val_loss: 1.5244\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.4425 - val_loss: 1.3674\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.3793 - val_loss: 1.2607\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.3557 - val_loss: 1.4120\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.3277 - val_loss: 1.1457\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.3148 - val_loss: 1.5951\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.2934 - val_loss: 1.0469\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.2846 - val_loss: 1.0577\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2736 - val_loss: 1.0195\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.2229 - val_loss: 0.9867\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2177 - val_loss: 0.9675\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1998 - val_loss: 0.9623\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1920 - val_loss: 0.9146\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1890 - val_loss: 0.8712\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1883 - val_loss: 0.8565\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1738 - val_loss: 0.8190\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1868 - val_loss: 0.8651\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.2002 - val_loss: 0.8691\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1663 - val_loss: 0.8425\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1737 - val_loss: 0.8038\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1387 - val_loss: 0.7491\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1392 - val_loss: 0.8137\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1389 - val_loss: 0.9705\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1468 - val_loss: 0.6914\n",
      "Epoch 31/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1363 - val_loss: 0.7033\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1723 - val_loss: 0.6927\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1417 - val_loss: 0.7506\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1209 - val_loss: 0.7713\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1236 - val_loss: 2.2385\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1256 - val_loss: 0.6504\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1114 - val_loss: 0.6952\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1219 - val_loss: 0.6515\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1096 - val_loss: 0.6741\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1086 - val_loss: 0.5979\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1181 - val_loss: 0.6514\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1083 - val_loss: 0.6192\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1072 - val_loss: 0.6165\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1014 - val_loss: 0.6321\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0921 - val_loss: 0.6363\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0897 - val_loss: 0.6186\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0977 - val_loss: 0.6211\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0842 - val_loss: 0.5906\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0888 - val_loss: 0.5909\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0952 - val_loss: 0.5724\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.1331 - val_loss: 0.5783\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.1025 - val_loss: 0.6124\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1209 - val_loss: 0.6222\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0915 - val_loss: 0.5520\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0823 - val_loss: 0.5713\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0833 - val_loss: 0.5605\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0800 - val_loss: 0.6283\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0779 - val_loss: 0.5586\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0756 - val_loss: 0.5206\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0733 - val_loss: 0.5217\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0733 - val_loss: 0.5284\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0755 - val_loss: 0.5397\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0791 - val_loss: 0.5616\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0693 - val_loss: 0.4989\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0729 - val_loss: 0.5387\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0709 - val_loss: 0.6117\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0683 - val_loss: 0.5104\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0687 - val_loss: 0.5307\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0673 - val_loss: 0.5144\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0741 - val_loss: 0.5061\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0635 - val_loss: 0.4894\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0652 - val_loss: 0.5042\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0621 - val_loss: 0.8092\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0640 - val_loss: 0.4858\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0615 - val_loss: 0.4891\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0644 - val_loss: 0.5006\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0656 - val_loss: 0.4753\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0698 - val_loss: 0.4844\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0585 - val_loss: 0.4790\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0596 - val_loss: 0.4708\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0579 - val_loss: 0.5314\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0585 - val_loss: 0.4596\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0577 - val_loss: 0.4763\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0635 - val_loss: 0.5052\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0561 - val_loss: 0.4710\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0559 - val_loss: 0.4746\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0630 - val_loss: 0.4374\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0610 - val_loss: 0.4780\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0532 - val_loss: 0.4516\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0529 - val_loss: 0.4857\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0622 - val_loss: 0.4978\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0538 - val_loss: 0.4529\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0550 - val_loss: 0.4768\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0525 - val_loss: 0.4647\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0515 - val_loss: 0.4441\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0530 - val_loss: 0.4747\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0508 - val_loss: 0.4433\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0527 - val_loss: 0.4550\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0527 - val_loss: 0.4298\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0497 - val_loss: 0.4416\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0499 - val_loss: 0.4470\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0474 - val_loss: 0.4416\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0471 - val_loss: 0.5077\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0461 - val_loss: 0.4392\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0454 - val_loss: 0.4575\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0461 - val_loss: 0.4377\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0457 - val_loss: 0.4232\n",
      "Epoch 108/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0457 - val_loss: 0.4210\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0458 - val_loss: 0.4251\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0444 - val_loss: 0.4389\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0447 - val_loss: 0.4166\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0443 - val_loss: 0.4465\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0454 - val_loss: 0.4410\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0458 - val_loss: 0.4230\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0426 - val_loss: 0.4408\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0412 - val_loss: 0.4148\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0420 - val_loss: 0.4094\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0430 - val_loss: 0.4418\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0440 - val_loss: 0.4452\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0427 - val_loss: 0.4335\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0454 - val_loss: 0.4651\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0437 - val_loss: 0.4227\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0403 - val_loss: 0.4247\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0405 - val_loss: 0.4270\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0414 - val_loss: 0.4139\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0387 - val_loss: 0.4240\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0400 - val_loss: 0.4248\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0421 - val_loss: 0.4436\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0404 - val_loss: 0.4146\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0407 - val_loss: 0.4187\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0402 - val_loss: 0.4255\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0421 - val_loss: 0.4547\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0417 - val_loss: 0.4078\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0390 - val_loss: 0.4299\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0394 - val_loss: 0.4116\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0395 - val_loss: 0.4447\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0385 - val_loss: 0.4123\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0376 - val_loss: 0.4318\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0400 - val_loss: 0.4298\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0410 - val_loss: 0.4184\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0378 - val_loss: 0.4275\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0359 - val_loss: 0.4242\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0369 - val_loss: 0.4150\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0353 - val_loss: 0.4287\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0365 - val_loss: 0.4126\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0357 - val_loss: 0.3981\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0336 - val_loss: 0.4130\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0364 - val_loss: 0.3986\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0440 - val_loss: 0.4262\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0381 - val_loss: 0.4057\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0341 - val_loss: 0.3962\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0344 - val_loss: 0.4190\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0353 - val_loss: 0.4290\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0356 - val_loss: 0.4384\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0340 - val_loss: 0.4002\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0339 - val_loss: 0.4245\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0340 - val_loss: 0.4453\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0338 - val_loss: 0.4948\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0341 - val_loss: 0.3988\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0349 - val_loss: 0.4199\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0339 - val_loss: 0.4028\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0327 - val_loss: 0.4130\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0334 - val_loss: 0.4192\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0352 - val_loss: 0.3998\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0323 - val_loss: 0.4003\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0346 - val_loss: 0.4032\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0318 - val_loss: 0.4009\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0352 - val_loss: 0.3998\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0306 - val_loss: 0.4566\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0330 - val_loss: 0.4105\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0315 - val_loss: 0.3978\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0300 - val_loss: 0.4228\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0346 - val_loss: 2.6563\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0422 - val_loss: 0.4250\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0423 - val_loss: 0.4255\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0633 - val_loss: 0.4381\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0393 - val_loss: 0.4112\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0352 - val_loss: 0.4113\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0334 - val_loss: 0.4045\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0305 - val_loss: 0.4241\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0310 - val_loss: 0.4012\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.0306 - val_loss: 0.4055\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0304 - val_loss: 0.4113\n",
      "Epoch 184/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0296 - val_loss: 0.4004\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0302 - val_loss: 0.4002\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0305 - val_loss: 0.4068\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0302 - val_loss: 0.4073\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0292 - val_loss: 0.4009\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0290 - val_loss: 0.4036\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0290 - val_loss: 0.4126\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0297 - val_loss: 0.4172\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0288 - val_loss: 0.4023\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0308 - val_loss: 0.4154\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0329 - val_loss: 0.4042\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0358 - val_loss: 0.4246\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0326 - val_loss: 0.4089\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0304 - val_loss: 0.4418\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0295 - val_loss: 0.4193\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0288 - val_loss: 0.4134\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0275 - val_loss: 0.4112\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0292 - val_loss: 0.4108\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0273 - val_loss: 0.3859\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0291 - val_loss: 0.4187\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0335 - val_loss: 0.4266\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0398 - val_loss: 0.4285\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0298 - val_loss: 0.4024\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0275 - val_loss: 0.3998\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0271 - val_loss: 0.4031\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0268 - val_loss: 0.4140\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0263 - val_loss: 0.4046\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0266 - val_loss: 0.4140\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0271 - val_loss: 0.4026\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0286 - val_loss: 0.4195\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0263 - val_loss: 0.4023\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0257 - val_loss: 0.4181\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0257 - val_loss: 0.4113\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0257 - val_loss: 0.4049\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0257 - val_loss: 0.4062\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0284 - val_loss: 0.4054\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0287 - val_loss: 0.4055\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0261 - val_loss: 0.3979\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0259 - val_loss: 0.4173\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0263 - val_loss: 0.4001\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0273 - val_loss: 0.4194\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0275 - val_loss: 0.4125\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0255 - val_loss: 0.4178\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0260 - val_loss: 0.4116\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0259 - val_loss: 0.4215\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0272 - val_loss: 0.4556\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0302 - val_loss: 0.4021\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0280 - val_loss: 0.4240\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0275 - val_loss: 0.4152\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0271 - val_loss: 0.6298\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0274 - val_loss: 0.4143\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0242 - val_loss: 0.4253\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0268 - val_loss: 0.4114\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0249 - val_loss: 0.4026\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0269 - val_loss: 0.4030\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0265 - val_loss: 0.3985\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0249 - val_loss: 0.4243\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0277 - val_loss: 0.5611\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0280 - val_loss: 0.3978\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0241 - val_loss: 0.3987\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0241 - val_loss: 0.3978\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0253 - val_loss: 0.4214\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0239 - val_loss: 0.4042\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0242 - val_loss: 0.4119\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0259 - val_loss: 0.4088\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0242 - val_loss: 0.4182\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0252 - val_loss: 0.4097\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 460us/step - loss: 10.8958 - val_loss: 3.4177\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 1.3865 - val_loss: 1.9087\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.8947 - val_loss: 1.7361\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.6567 - val_loss: 1.6122\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.5827 - val_loss: 1.3796\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.4775 - val_loss: 1.2124\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 1.2253 - val_loss: 1.5283\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.4714 - val_loss: 1.1676\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.3911 - val_loss: 1.1253\n",
      "Epoch 10/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.3970 - val_loss: 1.1180\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.3395 - val_loss: 1.1252\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2961 - val_loss: 1.0393\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2601 - val_loss: 0.9361\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.2321 - val_loss: 0.8815\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2340 - val_loss: 0.9285\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2075 - val_loss: 0.8166\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1972 - val_loss: 0.7748\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1826 - val_loss: 0.7749\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1734 - val_loss: 0.7477\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1771 - val_loss: 0.7080\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1925 - val_loss: 0.6954\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1773 - val_loss: 0.6478\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1469 - val_loss: 0.6523\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1397 - val_loss: 0.6597\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1469 - val_loss: 0.6225\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1368 - val_loss: 0.6476\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1244 - val_loss: 0.5953\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1217 - val_loss: 0.6611\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1204 - val_loss: 0.6150\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1250 - val_loss: 0.5530\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1247 - val_loss: 0.5574\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1143 - val_loss: 0.7038\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1088 - val_loss: 0.9037\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1233 - val_loss: 0.5394\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1154 - val_loss: 0.5393\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1148 - val_loss: 0.5252\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1027 - val_loss: 0.5573\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0990 - val_loss: 0.5317\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0995 - val_loss: 0.5650\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1039 - val_loss: 0.5536\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1087 - val_loss: 1.0382\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1136 - val_loss: 0.4880\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1006 - val_loss: 0.4810\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0922 - val_loss: 0.5129\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0867 - val_loss: 0.4800\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0902 - val_loss: 0.4585\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0866 - val_loss: 0.4763\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0986 - val_loss: 0.4702\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0965 - val_loss: 0.5011\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1052 - val_loss: 0.5193\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0897 - val_loss: 0.4695\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0786 - val_loss: 0.5638\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0764 - val_loss: 0.4579\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0732 - val_loss: 0.4519\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0748 - val_loss: 0.4663\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0741 - val_loss: 0.6030\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0772 - val_loss: 0.4743\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0794 - val_loss: 0.4567\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0727 - val_loss: 0.4493\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0710 - val_loss: 0.5306\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0738 - val_loss: 0.6395\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0692 - val_loss: 0.4265\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0687 - val_loss: 0.4351\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0671 - val_loss: 0.4497\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0637 - val_loss: 0.4023\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0638 - val_loss: 0.4214\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0629 - val_loss: 0.4335\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0623 - val_loss: 0.4108\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0673 - val_loss: 0.4187\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0682 - val_loss: 0.4120\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0676 - val_loss: 0.3995\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1079 - val_loss: 0.4159\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0724 - val_loss: 0.4295\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0605 - val_loss: 0.4329\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0595 - val_loss: 0.3883\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0632 - val_loss: 0.3989\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0707 - val_loss: 0.4095\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0597 - val_loss: 0.6966\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0601 - val_loss: 0.3809\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0569 - val_loss: 0.4178\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0537 - val_loss: 0.3921\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0563 - val_loss: 0.3701\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0542 - val_loss: 0.3894\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0517 - val_loss: 0.3929\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0585 - val_loss: 0.3966\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0540 - val_loss: 0.3815\n",
      "Epoch 87/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0540 - val_loss: 0.4288\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0522 - val_loss: 0.4093\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0505 - val_loss: 0.3942\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0530 - val_loss: 0.3605\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0506 - val_loss: 0.4125\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0487 - val_loss: 0.3760\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0479 - val_loss: 0.4142\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0492 - val_loss: 0.3649\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0488 - val_loss: 0.3645\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0457 - val_loss: 0.3501\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0455 - val_loss: 0.3647\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0451 - val_loss: 0.3629\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0453 - val_loss: 0.3581\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0455 - val_loss: 0.3634\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0450 - val_loss: 0.3577\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0555 - val_loss: 0.3617\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0494 - val_loss: 0.4671\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0436 - val_loss: 0.3870\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0438 - val_loss: 0.3897\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0434 - val_loss: 0.3832\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0426 - val_loss: 0.3458\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0417 - val_loss: 0.3523\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0473 - val_loss: 0.3699\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0414 - val_loss: 0.3920\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0465 - val_loss: 0.3365\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0417 - val_loss: 0.3555\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0420 - val_loss: 0.3490\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0422 - val_loss: 0.3572\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0411 - val_loss: 0.3609\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0420 - val_loss: 0.3474\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0392 - val_loss: 0.3534\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0411 - val_loss: 0.3491\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0393 - val_loss: 0.3604\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0379 - val_loss: 0.3534\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0370 - val_loss: 0.3468\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0372 - val_loss: 0.3352\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0372 - val_loss: 0.3640\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0501 - val_loss: 0.4421\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0595 - val_loss: 0.3562\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0389 - val_loss: 0.3673\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0410 - val_loss: 0.3786\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0394 - val_loss: 0.3603\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0397 - val_loss: 0.3800\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0391 - val_loss: 0.3635\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0406 - val_loss: 0.3746\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0373 - val_loss: 0.3680\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0371 - val_loss: 0.3683\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0378 - val_loss: 0.3777\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0363 - val_loss: 0.3575\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0364 - val_loss: 0.3930\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0373 - val_loss: 0.3516\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0382 - val_loss: 0.4101\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0486 - val_loss: 0.3600\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0367 - val_loss: 0.3802\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0388 - val_loss: 0.3695\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0361 - val_loss: 0.3473\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0360 - val_loss: 0.3603\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0335 - val_loss: 0.3628\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0335 - val_loss: 0.3602\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0338 - val_loss: 0.3486\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0320 - val_loss: 0.3648\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0335 - val_loss: 0.3453\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0336 - val_loss: 0.3908\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0394 - val_loss: 0.3592\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0375 - val_loss: 0.3923\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0331 - val_loss: 0.3672\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0314 - val_loss: 0.3719\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0339 - val_loss: 0.3533\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0358 - val_loss: 0.3393\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0328 - val_loss: 0.3339\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0317 - val_loss: 0.3496\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0309 - val_loss: 0.3497\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0310 - val_loss: 0.3371\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0312 - val_loss: 0.3404\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0340 - val_loss: 0.3572\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0371 - val_loss: 0.3727\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0331 - val_loss: 0.3550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0365 - val_loss: 0.3762\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0324 - val_loss: 0.3854\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0319 - val_loss: 0.3511\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0295 - val_loss: 0.3562\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0315 - val_loss: 0.3571\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0313 - val_loss: 0.3577\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0308 - val_loss: 0.3559\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0292 - val_loss: 0.4026\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0298 - val_loss: 0.3567\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0297 - val_loss: 0.3708\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0314 - val_loss: 0.3728\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0290 - val_loss: 0.3599\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0279 - val_loss: 0.3456\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0300 - val_loss: 0.3652\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0289 - val_loss: 0.3564\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0291 - val_loss: 0.3627\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0307 - val_loss: 0.3668\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0277 - val_loss: 0.3956\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0301 - val_loss: 0.3580\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0294 - val_loss: 0.3694\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0289 - val_loss: 0.3600\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0274 - val_loss: 0.3573\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0287 - val_loss: 0.3589\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0271 - val_loss: 0.3541\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0276 - val_loss: 0.3521\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0273 - val_loss: 0.3653\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0267 - val_loss: 0.3695\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0268 - val_loss: 0.3638\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0271 - val_loss: 0.3681\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0272 - val_loss: 0.3744\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0273 - val_loss: 0.3484\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0271 - val_loss: 0.3842\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0270 - val_loss: 0.3686\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0288 - val_loss: 0.3774\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0256 - val_loss: 0.3688\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0276 - val_loss: 0.3619\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0258 - val_loss: 0.3869\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0264 - val_loss: 0.3605\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0294 - val_loss: 0.3593\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0280 - val_loss: 0.3714\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0313 - val_loss: 0.3738\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0302 - val_loss: 0.3628\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0256 - val_loss: 0.3714\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0281 - val_loss: 0.3667\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0270 - val_loss: 0.3790\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0256 - val_loss: 0.3830\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0246 - val_loss: 0.3691\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0265 - val_loss: 0.3733\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0250 - val_loss: 0.3708\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0265 - val_loss: 0.3717\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0267 - val_loss: 0.3846\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0269 - val_loss: 0.3661\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0334 - val_loss: 0.3706\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0300 - val_loss: 0.3738\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0279 - val_loss: 0.3678\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0390 - val_loss: 0.3723\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0269 - val_loss: 0.3686\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0260 - val_loss: 0.3653\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0247 - val_loss: 0.3631\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0238 - val_loss: 0.3652\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0241 - val_loss: 0.3769\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0244 - val_loss: 0.3745\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0236 - val_loss: 0.3739\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0232 - val_loss: 0.3670\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0235 - val_loss: 0.3804\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0233 - val_loss: 0.3954\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0257 - val_loss: 0.3800\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0240 - val_loss: 0.3785\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0231 - val_loss: 0.3715\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0242 - val_loss: 0.3731\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0237 - val_loss: 0.3686\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0230 - val_loss: 0.3785\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0232 - val_loss: 0.3907\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0231 - val_loss: 0.3735\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0232 - val_loss: 0.3621\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0245 - val_loss: 0.3782\n",
      "Epoch 240/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0249 - val_loss: 0.3725\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0230 - val_loss: 0.3966\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0237 - val_loss: 0.3721\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0225 - val_loss: 0.3793\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0222 - val_loss: 0.3725\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0224 - val_loss: 0.3804\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0231 - val_loss: 0.3896\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0270 - val_loss: 0.3735\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0235 - val_loss: 0.3746\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0236 - val_loss: 0.3905\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0218 - val_loss: 0.3721\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 459us/step - loss: 9.2184 - val_loss: 3.1975\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 1.6878 - val_loss: 1.8034\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.8518 - val_loss: 1.3963\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.6379 - val_loss: 1.3648\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.5177 - val_loss: 1.2315\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.4592 - val_loss: 1.1938\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.4171 - val_loss: 1.0813\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.3620 - val_loss: 0.9764\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.3177 - val_loss: 0.9225\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.2936 - val_loss: 0.9268\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2691 - val_loss: 0.8768\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.2608 - val_loss: 0.8419\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.2520 - val_loss: 0.8422\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.2662 - val_loss: 0.8248\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.2521 - val_loss: 0.7422\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.2351 - val_loss: 0.7235\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.2141 - val_loss: 0.6352\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2096 - val_loss: 0.7881\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1676 - val_loss: 0.7037\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1770 - val_loss: 0.9038\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1710 - val_loss: 0.7245\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1716 - val_loss: 0.5199\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1507 - val_loss: 0.5971\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1509 - val_loss: 0.7823\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1416 - val_loss: 0.5955\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1373 - val_loss: 0.4845\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1269 - val_loss: 0.5453\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1238 - val_loss: 0.5430\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1211 - val_loss: 0.4754\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1211 - val_loss: 0.5043\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1923 - val_loss: 0.5492\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1363 - val_loss: 0.5368\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1101 - val_loss: 0.4788\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1159 - val_loss: 0.4713\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1151 - val_loss: 0.4527\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1039 - val_loss: 0.4801\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1009 - val_loss: 0.5433\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0969 - val_loss: 0.4564\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1300 - val_loss: 0.4812\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1413 - val_loss: 0.4808\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0963 - val_loss: 0.4337\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0894 - val_loss: 0.4613\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0892 - val_loss: 0.4228\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0876 - val_loss: 0.4235\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0849 - val_loss: 0.4058\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0883 - val_loss: 0.4125\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0820 - val_loss: 0.4006\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0830 - val_loss: 0.3891\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0798 - val_loss: 0.4668\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0812 - val_loss: 0.3652\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0754 - val_loss: 0.3776\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0777 - val_loss: 0.3744\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0789 - val_loss: 0.4122\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0764 - val_loss: 0.3927\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0740 - val_loss: 0.3709\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0717 - val_loss: 0.3385\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0700 - val_loss: 0.3303\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0691 - val_loss: 0.3646\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0781 - val_loss: 0.3491\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0670 - val_loss: 0.3451\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0651 - val_loss: 0.3529\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0907 - val_loss: 0.4343\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0924 - val_loss: 0.3317\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0669 - val_loss: 0.3378\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0678 - val_loss: 0.3313\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0640 - val_loss: 0.3217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0641 - val_loss: 0.3342\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0586 - val_loss: 0.3385\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0589 - val_loss: 0.3331\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0587 - val_loss: 0.3370\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0561 - val_loss: 0.3370\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0550 - val_loss: 0.3078\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0572 - val_loss: 0.3079\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0551 - val_loss: 0.3067\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0546 - val_loss: 0.3233\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0545 - val_loss: 0.3010\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0558 - val_loss: 0.3491\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0541 - val_loss: 0.2778\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0620 - val_loss: 0.3019\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0598 - val_loss: 0.2891\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0611 - val_loss: 0.3178\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0534 - val_loss: 0.2990\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0549 - val_loss: 0.2906\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0517 - val_loss: 0.3122\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0503 - val_loss: 0.2953\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0515 - val_loss: 0.3041\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0482 - val_loss: 0.3070\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0476 - val_loss: 0.3034\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0516 - val_loss: 0.2731\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0484 - val_loss: 0.3059\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0465 - val_loss: 0.3247\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0442 - val_loss: 0.2997\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0452 - val_loss: 0.2721\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0477 - val_loss: 0.2704\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0439 - val_loss: 0.2930\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0446 - val_loss: 0.2693\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0433 - val_loss: 0.2982\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0445 - val_loss: 0.2756\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0425 - val_loss: 0.2836\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0478 - val_loss: 0.2845\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0471 - val_loss: 0.2698\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0436 - val_loss: 0.2667\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0425 - val_loss: 0.2781\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0434 - val_loss: 0.2730\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0396 - val_loss: 0.2954\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0466 - val_loss: 0.2851\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0396 - val_loss: 0.2662\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0393 - val_loss: 0.2661\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0434 - val_loss: 0.3045\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0442 - val_loss: 0.2739\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0459 - val_loss: 0.2782\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0598 - val_loss: 0.3323\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0477 - val_loss: 0.2808\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0407 - val_loss: 0.2539\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0426 - val_loss: 0.2762\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0508 - val_loss: 0.2670\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0463 - val_loss: 0.3096\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0393 - val_loss: 0.2689\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0381 - val_loss: 0.2990\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0364 - val_loss: 0.2597\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0408 - val_loss: 0.2776\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0389 - val_loss: 0.2690\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0368 - val_loss: 0.2546\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0353 - val_loss: 0.2666\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0359 - val_loss: 0.2681\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0351 - val_loss: 0.2692\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0352 - val_loss: 0.2705\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0342 - val_loss: 0.3181\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0528 - val_loss: 0.2889\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0434 - val_loss: 0.2849\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0386 - val_loss: 0.2544\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0388 - val_loss: 0.2747\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0350 - val_loss: 0.2555\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0376 - val_loss: 0.2690\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0364 - val_loss: 0.3102\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0355 - val_loss: 0.2568\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0330 - val_loss: 0.2655\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0339 - val_loss: 0.2648\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0329 - val_loss: 0.2861\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0316 - val_loss: 0.2784\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0355 - val_loss: 0.2638\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0383 - val_loss: 0.3229\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0358 - val_loss: 0.2894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0397 - val_loss: 0.2631\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0412 - val_loss: 0.2589\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0420 - val_loss: 0.2914\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0376 - val_loss: 0.2858\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0417 - val_loss: 0.2779\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0339 - val_loss: 0.2838\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0359 - val_loss: 0.2748\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0310 - val_loss: 0.3016\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0312 - val_loss: 0.2909\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0389 - val_loss: 0.2880\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0333 - val_loss: 0.2671\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0350 - val_loss: 0.2649\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0324 - val_loss: 0.3273\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0359 - val_loss: 0.2871\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0308 - val_loss: 0.2786\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0326 - val_loss: 0.2648\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0303 - val_loss: 0.2908\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0308 - val_loss: 0.2722\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0305 - val_loss: 0.2702\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0300 - val_loss: 0.2637\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0282 - val_loss: 0.3019\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0315 - val_loss: 0.2583\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0285 - val_loss: 0.2721\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0305 - val_loss: 0.2664\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0322 - val_loss: 0.2570\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0304 - val_loss: 0.2685\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0308 - val_loss: 0.2931\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0300 - val_loss: 0.2649\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0287 - val_loss: 0.2715\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0278 - val_loss: 0.2858\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0277 - val_loss: 0.2732\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0278 - val_loss: 0.2742\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0309 - val_loss: 0.2783\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0284 - val_loss: 0.2719\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0300 - val_loss: 0.3086\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0291 - val_loss: 0.2850\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0284 - val_loss: 0.2744\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0274 - val_loss: 0.2715\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0274 - val_loss: 0.2775\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0277 - val_loss: 0.2872\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0274 - val_loss: 0.2749\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0258 - val_loss: 0.2656\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0283 - val_loss: 0.2613\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0276 - val_loss: 0.2728\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0268 - val_loss: 0.2747\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0268 - val_loss: 0.2635\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0288 - val_loss: 0.2726\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0272 - val_loss: 0.2782\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0269 - val_loss: 0.2832\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0256 - val_loss: 0.2745\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0255 - val_loss: 0.3265\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0260 - val_loss: 0.2890\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0260 - val_loss: 0.2773\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0255 - val_loss: 0.2804\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0290 - val_loss: 0.2928\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0274 - val_loss: 0.2673\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0261 - val_loss: 0.2904\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0265 - val_loss: 0.2885\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0262 - val_loss: 0.2755\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0280 - val_loss: 0.2826\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0265 - val_loss: 0.2733\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0251 - val_loss: 0.2693\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0241 - val_loss: 0.2716\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0243 - val_loss: 0.2837\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0252 - val_loss: 0.2806\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0249 - val_loss: 0.3112\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0250 - val_loss: 0.2779\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0237 - val_loss: 0.2830\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0240 - val_loss: 0.2776\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0236 - val_loss: 0.2768\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0239 - val_loss: 0.2758\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0256 - val_loss: 0.3220\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0235 - val_loss: 0.3023\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0251 - val_loss: 0.2717\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0240 - val_loss: 0.2735\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0293 - val_loss: 0.2724\n",
      "Epoch 220/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0235 - val_loss: 0.2810\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0232 - val_loss: 0.2781\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0238 - val_loss: 0.2903\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0232 - val_loss: 0.2776\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0229 - val_loss: 0.2896\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0232 - val_loss: 0.2816\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0229 - val_loss: 0.2774\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0237 - val_loss: 0.2743\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0232 - val_loss: 0.2840\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0232 - val_loss: 0.2901\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0232 - val_loss: 0.2845\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0240 - val_loss: 0.3266\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0304 - val_loss: 0.2828\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0261 - val_loss: 0.2922\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0262 - val_loss: 0.2828\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0237 - val_loss: 0.2962\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0230 - val_loss: 0.2814\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0222 - val_loss: 0.2731\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0223 - val_loss: 0.2773\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0216 - val_loss: 0.2815\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0222 - val_loss: 0.3001\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0252 - val_loss: 0.2720\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0245 - val_loss: 0.3361\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0228 - val_loss: 0.3027\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0224 - val_loss: 0.2787\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0219 - val_loss: 0.2875\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0224 - val_loss: 0.2840\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0214 - val_loss: 0.3044\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0234 - val_loss: 0.2829\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0214 - val_loss: 0.2943\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0206 - val_loss: 0.2729\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 472us/step - loss: 7.6787 - val_loss: 3.5497\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 2.4323 - val_loss: 2.8143\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 1.3795 - val_loss: 1.8595\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.7102 - val_loss: 1.5288\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.6172 - val_loss: 1.4272\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.4995 - val_loss: 1.6989\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.4316 - val_loss: 1.2562\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.3493 - val_loss: 1.0790\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.3097 - val_loss: 1.1380\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2861 - val_loss: 1.3264\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.2511 - val_loss: 0.9158\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.2493 - val_loss: 0.8597\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2097 - val_loss: 0.9324\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2025 - val_loss: 0.9044\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1843 - val_loss: 0.8395\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1808 - val_loss: 0.7737\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1695 - val_loss: 0.7241\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1715 - val_loss: 0.7848\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1574 - val_loss: 0.7376\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1644 - val_loss: 0.7434\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1462 - val_loss: 1.0564\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1506 - val_loss: 0.6434\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1381 - val_loss: 0.6973\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1361 - val_loss: 0.6859\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1346 - val_loss: 0.6610\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1206 - val_loss: 0.7193\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1136 - val_loss: 0.6512\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1293 - val_loss: 0.7943\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1836 - val_loss: 0.7480\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1240 - val_loss: 0.5949\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1092 - val_loss: 0.7011\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1031 - val_loss: 0.5865\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1001 - val_loss: 0.5564\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0925 - val_loss: 0.5409\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0931 - val_loss: 0.5461\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0889 - val_loss: 0.5653\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0859 - val_loss: 0.5930\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0892 - val_loss: 0.5575\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0927 - val_loss: 0.5871\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0845 - val_loss: 0.5516\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0830 - val_loss: 0.5586\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0815 - val_loss: 0.5103\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0832 - val_loss: 0.5172\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0822 - val_loss: 0.5419\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0824 - val_loss: 0.5211\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0748 - val_loss: 0.4971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0776 - val_loss: 0.4929\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0729 - val_loss: 0.4759\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0741 - val_loss: 0.5020\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0701 - val_loss: 0.5006\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0691 - val_loss: 0.4876\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0702 - val_loss: 0.4711\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0680 - val_loss: 0.5273\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0714 - val_loss: 0.5087\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0783 - val_loss: 0.5080\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0709 - val_loss: 0.4934\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0683 - val_loss: 0.4626\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0665 - val_loss: 0.5451\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0633 - val_loss: 0.4571\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0626 - val_loss: 0.4438\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0605 - val_loss: 0.4505\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0586 - val_loss: 0.5793\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0624 - val_loss: 0.4654\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0599 - val_loss: 0.4457\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0607 - val_loss: 0.4498\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0628 - val_loss: 0.4404\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0579 - val_loss: 0.4596\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0562 - val_loss: 0.4534\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0523 - val_loss: 0.7212\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0590 - val_loss: 0.4658\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0516 - val_loss: 0.4334\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0522 - val_loss: 0.4411\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0539 - val_loss: 0.4808\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0508 - val_loss: 0.4306\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0576 - val_loss: 0.4309\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0570 - val_loss: 0.4334\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0524 - val_loss: 0.4752\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0511 - val_loss: 0.4242\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0526 - val_loss: 0.4217\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0507 - val_loss: 0.4529\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0504 - val_loss: 0.4164\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0477 - val_loss: 0.4954\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0500 - val_loss: 0.4566\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0478 - val_loss: 0.4119\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0465 - val_loss: 0.4137\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0562 - val_loss: 0.4096\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0510 - val_loss: 0.4116\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0532 - val_loss: 0.4006\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0469 - val_loss: 0.4247\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0462 - val_loss: 0.4006\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0452 - val_loss: 0.4072\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0502 - val_loss: 0.3992\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0467 - val_loss: 0.4016\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0432 - val_loss: 0.4130\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0437 - val_loss: 0.4228\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0440 - val_loss: 0.4380\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0421 - val_loss: 0.4228\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0432 - val_loss: 0.4044\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0434 - val_loss: 0.3889\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0425 - val_loss: 0.3936\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0416 - val_loss: 0.3950\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0394 - val_loss: 0.3959\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0407 - val_loss: 0.3908\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0412 - val_loss: 0.3991\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0423 - val_loss: 0.4125\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0397 - val_loss: 0.3886\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0393 - val_loss: 0.4435\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0417 - val_loss: 0.3910\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0384 - val_loss: 0.3982\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0373 - val_loss: 0.3990\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0369 - val_loss: 0.3931\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0367 - val_loss: 0.4312\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0366 - val_loss: 0.3798\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0381 - val_loss: 0.4079\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0376 - val_loss: 0.3792\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0377 - val_loss: 0.3772\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0354 - val_loss: 0.3867\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0357 - val_loss: 0.3933\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0381 - val_loss: 0.4119\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0378 - val_loss: 0.3948\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0383 - val_loss: 0.3855\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0375 - val_loss: 0.4123\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0387 - val_loss: 0.3997\n",
      "Epoch 124/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0358 - val_loss: 0.3809\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0330 - val_loss: 0.3862\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0346 - val_loss: 0.3855\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0341 - val_loss: 0.3746\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0342 - val_loss: 0.3891\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0358 - val_loss: 0.3821\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0330 - val_loss: 0.3888\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0342 - val_loss: 0.3977\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0332 - val_loss: 0.4035\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0323 - val_loss: 0.3820\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0347 - val_loss: 0.3820\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0343 - val_loss: 0.3809\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0324 - val_loss: 0.3849\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0315 - val_loss: 0.3891\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0301 - val_loss: 0.3997\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0307 - val_loss: 0.3791\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0325 - val_loss: 0.3912\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0310 - val_loss: 0.4054\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0299 - val_loss: 0.4130\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0370 - val_loss: 0.3750\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0321 - val_loss: 0.3962\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0306 - val_loss: 0.3824\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0326 - val_loss: 0.3996\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0304 - val_loss: 0.3800\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0298 - val_loss: 0.3942\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0299 - val_loss: 0.3851\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0301 - val_loss: 0.3919\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0290 - val_loss: 0.3916\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0287 - val_loss: 0.4240\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0310 - val_loss: 0.4070\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0302 - val_loss: 0.4289\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0306 - val_loss: 0.4024\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0305 - val_loss: 0.3960\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0298 - val_loss: 0.3910\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0305 - val_loss: 0.5000\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0305 - val_loss: 0.4082\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0287 - val_loss: 0.3938\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0289 - val_loss: 0.3888\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0289 - val_loss: 0.3768\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0281 - val_loss: 0.4029\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0293 - val_loss: 0.4006\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0296 - val_loss: 0.4579\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0279 - val_loss: 0.3886\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0282 - val_loss: 0.4017\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0276 - val_loss: 0.4095\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0278 - val_loss: 0.3999\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0265 - val_loss: 0.3935\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0287 - val_loss: 0.4358\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0326 - val_loss: 0.4307\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0368 - val_loss: 0.4036\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0341 - val_loss: 0.4446\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0338 - val_loss: 0.4051\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0279 - val_loss: 0.3955\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0268 - val_loss: 0.4776\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0274 - val_loss: 0.3992\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0275 - val_loss: 0.4140\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0272 - val_loss: 0.4010\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0262 - val_loss: 0.4133\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0278 - val_loss: 0.4009\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0280 - val_loss: 0.3985\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0264 - val_loss: 0.4033\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0264 - val_loss: 0.4274\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0265 - val_loss: 0.4076\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0279 - val_loss: 0.4100\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0262 - val_loss: 0.4221\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0259 - val_loss: 0.4096\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0254 - val_loss: 0.4140\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0259 - val_loss: 0.4045\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0241 - val_loss: 0.4457\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0247 - val_loss: 0.4147\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0244 - val_loss: 0.4241\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0232 - val_loss: 0.4091\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0251 - val_loss: 0.4165\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0239 - val_loss: 0.4222\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0242 - val_loss: 0.4279\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0247 - val_loss: 0.4034\n",
      "Epoch 200/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0239 - val_loss: 0.4646\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0244 - val_loss: 0.4097\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0246 - val_loss: 0.4043\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0236 - val_loss: 0.4150\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0293 - val_loss: 0.4125\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0252 - val_loss: 0.4021\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0250 - val_loss: 0.4504\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0264 - val_loss: 0.4361\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0252 - val_loss: 0.4210\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0252 - val_loss: 0.4096\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0242 - val_loss: 0.4385\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0249 - val_loss: 0.4107\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0247 - val_loss: 0.4354\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0228 - val_loss: 0.4222\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0241 - val_loss: 0.4258\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0221 - val_loss: 0.4406\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0239 - val_loss: 0.4883\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0237 - val_loss: 0.4236\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0298 - val_loss: 0.4445\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0316 - val_loss: 0.4221\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0394 - val_loss: 0.4523\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0254 - val_loss: 0.4023\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0238 - val_loss: 0.4850\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0242 - val_loss: 0.4277\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0223 - val_loss: 0.4130\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0237 - val_loss: 0.4368\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0222 - val_loss: 0.4281\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0227 - val_loss: 0.4199\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0227 - val_loss: 0.4390\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0224 - val_loss: 0.4062\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0231 - val_loss: 0.4197\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0225 - val_loss: 0.4217\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0215 - val_loss: 0.4449\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0208 - val_loss: 0.4291\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0222 - val_loss: 0.4598\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0217 - val_loss: 0.4177\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0226 - val_loss: 0.4410\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 160us/step - loss: 0.0216 - val_loss: 0.4470\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0232 - val_loss: 0.4280\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0217 - val_loss: 0.4286\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0215 - val_loss: 0.4414\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0222 - val_loss: 0.4231\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0209 - val_loss: 0.4131\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0233 - val_loss: 0.4121\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0211 - val_loss: 0.4511\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.0210 - val_loss: 0.4404\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0205 - val_loss: 0.4501\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0232 - val_loss: 0.4432\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0210 - val_loss: 0.4301\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0218 - val_loss: 0.4278\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0215 - val_loss: 0.4319\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 469us/step - loss: 7.9917 - val_loss: 2.5915\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 1.0897 - val_loss: 1.8315\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.8021 - val_loss: 1.5222\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.5904 - val_loss: 1.4004\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.5419 - val_loss: 1.4390\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.4185 - val_loss: 1.7466\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.4488 - val_loss: 1.0650\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.3219 - val_loss: 1.1053\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.2973 - val_loss: 1.0374\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.2668 - val_loss: 0.9550\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2763 - val_loss: 1.0280\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.2460 - val_loss: 0.8800\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.2034 - val_loss: 0.7899\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1923 - val_loss: 0.7962\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.2352 - val_loss: 0.7190\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1724 - val_loss: 0.7540\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.2009 - val_loss: 0.9390\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.2055 - val_loss: 0.8376\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1677 - val_loss: 1.2281\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1582 - val_loss: 0.6799\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1590 - val_loss: 0.7750\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1520 - val_loss: 0.7784\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1433 - val_loss: 0.5955\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1232 - val_loss: 0.5875\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1265 - val_loss: 0.7161\n",
      "Epoch 26/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1300 - val_loss: 0.5445\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1283 - val_loss: 0.7019\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1278 - val_loss: 0.5728\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.1248 - val_loss: 0.5393\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1078 - val_loss: 0.5613\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1071 - val_loss: 0.5307\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1031 - val_loss: 0.5219\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1157 - val_loss: 0.5686\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1214 - val_loss: 0.5276\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1076 - val_loss: 0.6949\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 161us/step - loss: 0.1017 - val_loss: 0.5436\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0979 - val_loss: 0.5531\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0946 - val_loss: 0.5673\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0980 - val_loss: 0.5278\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0960 - val_loss: 0.5596\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0824 - val_loss: 0.4713\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0876 - val_loss: 0.5810\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0835 - val_loss: 0.4778\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0968 - val_loss: 0.4854\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0808 - val_loss: 0.4702\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0871 - val_loss: 0.4603\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0932 - val_loss: 0.4758\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1128 - val_loss: 0.6583\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0878 - val_loss: 0.4839\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0794 - val_loss: 0.4995\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0753 - val_loss: 0.4702\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0759 - val_loss: 0.4898\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0744 - val_loss: 0.4324\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0836 - val_loss: 0.4577\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0749 - val_loss: 0.4464\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0689 - val_loss: 0.4993\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0694 - val_loss: 0.4663\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0627 - val_loss: 0.4227\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0613 - val_loss: 0.4345\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0637 - val_loss: 0.4349\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0623 - val_loss: 0.4041\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0648 - val_loss: 0.4048\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0604 - val_loss: 0.4088\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0670 - val_loss: 0.4398\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0650 - val_loss: 0.3991\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0629 - val_loss: 0.4058\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0593 - val_loss: 0.3889\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0597 - val_loss: 0.4062\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0556 - val_loss: 0.3957\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0594 - val_loss: 0.5748\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0908 - val_loss: 0.3954\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0581 - val_loss: 0.3817\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0627 - val_loss: 0.3924\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0620 - val_loss: 0.3942\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0604 - val_loss: 0.3720\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0724 - val_loss: 0.4121\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0630 - val_loss: 0.3773\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0551 - val_loss: 0.3872\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0524 - val_loss: 0.3894\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0506 - val_loss: 0.3932\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0525 - val_loss: 0.3652\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0507 - val_loss: 0.3848\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0481 - val_loss: 0.3997\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0466 - val_loss: 0.3776\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0463 - val_loss: 0.3459\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0470 - val_loss: 0.3577\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0522 - val_loss: 0.3753\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0503 - val_loss: 0.3547\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0489 - val_loss: 0.3620\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0440 - val_loss: 0.5006\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0438 - val_loss: 0.3620\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0449 - val_loss: 0.3694\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0440 - val_loss: 0.3518\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0432 - val_loss: 0.3477\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0558 - val_loss: 0.4086\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0792 - val_loss: 0.3827\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0507 - val_loss: 0.4071\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0433 - val_loss: 0.3707\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0465 - val_loss: 0.3844\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0524 - val_loss: 0.3684\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0459 - val_loss: 0.3492\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0401 - val_loss: 0.3594\n",
      "Epoch 103/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0450 - val_loss: 0.3602\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0487 - val_loss: 0.3659\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0404 - val_loss: 0.3747\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0399 - val_loss: 0.3611\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0405 - val_loss: 0.3800\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0417 - val_loss: 0.3594\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0396 - val_loss: 0.3533\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0391 - val_loss: 0.3717\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0398 - val_loss: 0.3587\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0388 - val_loss: 0.4012\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0401 - val_loss: 0.3690\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0374 - val_loss: 0.3487\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0348 - val_loss: 0.5188\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0401 - val_loss: 0.3646\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0392 - val_loss: 0.3444\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0405 - val_loss: 0.3715\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0374 - val_loss: 0.3628\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0363 - val_loss: 0.3391\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0414 - val_loss: 0.3578\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0408 - val_loss: 0.3713\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0404 - val_loss: 0.3615\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0369 - val_loss: 0.3921\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0353 - val_loss: 0.3702\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0349 - val_loss: 0.3600\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0351 - val_loss: 0.3873\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0366 - val_loss: 0.3423\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0367 - val_loss: 0.3787\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0340 - val_loss: 0.3546\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0338 - val_loss: 0.3791\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0352 - val_loss: 0.4443\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0351 - val_loss: 0.3543\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0363 - val_loss: 0.3620\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0322 - val_loss: 0.3608\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0344 - val_loss: 0.3614\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0360 - val_loss: 0.3474\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0348 - val_loss: 0.3988\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0448 - val_loss: 0.4116\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0353 - val_loss: 0.3646\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0331 - val_loss: 0.3793\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0399 - val_loss: 0.3960\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0341 - val_loss: 0.3999\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0335 - val_loss: 0.3662\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0316 - val_loss: 0.3740\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0303 - val_loss: 0.4030\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0317 - val_loss: 0.3621\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0320 - val_loss: 0.3729\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0321 - val_loss: 0.3593\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0313 - val_loss: 0.4245\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0447 - val_loss: 0.3720\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0317 - val_loss: 0.3964\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0321 - val_loss: 0.3646\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0372 - val_loss: 0.3913\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0320 - val_loss: 0.3663\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0290 - val_loss: 0.3603\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0319 - val_loss: 0.3553\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0323 - val_loss: 0.4229\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0325 - val_loss: 0.3627\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0271 - val_loss: 0.3623\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0283 - val_loss: 0.3732\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0273 - val_loss: 0.3579\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0306 - val_loss: 0.3695\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0287 - val_loss: 0.3550\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0286 - val_loss: 0.3529\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0289 - val_loss: 0.3719\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0280 - val_loss: 0.4082\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0297 - val_loss: 0.3934\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0282 - val_loss: 0.3577\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0286 - val_loss: 0.3532\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0281 - val_loss: 0.3575\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0287 - val_loss: 0.3703\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0288 - val_loss: 0.3978\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0301 - val_loss: 0.3517\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0264 - val_loss: 0.3575\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0273 - val_loss: 0.3890\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0259 - val_loss: 0.3701\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0267 - val_loss: 0.3579\n",
      "Epoch 179/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0284 - val_loss: 0.3508\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0257 - val_loss: 0.3535\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0262 - val_loss: 0.3593\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0275 - val_loss: 0.3784\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0275 - val_loss: 0.3702\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0268 - val_loss: 0.4003\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0266 - val_loss: 0.3711\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0260 - val_loss: 0.3676\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0274 - val_loss: 0.3569\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0268 - val_loss: 0.3859\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0260 - val_loss: 0.3681\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0276 - val_loss: 0.4271\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0284 - val_loss: 0.3603\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0284 - val_loss: 0.3628\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0256 - val_loss: 0.3674\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0263 - val_loss: 0.3629\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0331 - val_loss: 0.3859\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0292 - val_loss: 0.4080\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0269 - val_loss: 0.3636\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0263 - val_loss: 0.3782\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0272 - val_loss: 0.3704\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0238 - val_loss: 0.3719\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0254 - val_loss: 0.3683\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0236 - val_loss: 0.3611\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0237 - val_loss: 0.3938\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0253 - val_loss: 0.3638\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0253 - val_loss: 0.3627\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0257 - val_loss: 0.3569\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0246 - val_loss: 0.3701\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0247 - val_loss: 0.3661\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0263 - val_loss: 0.3681\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0248 - val_loss: 0.3789\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0251 - val_loss: 0.3741\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0240 - val_loss: 0.3601\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0244 - val_loss: 0.3737\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0239 - val_loss: 0.3672\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0261 - val_loss: 0.3657\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0262 - val_loss: 0.3829\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0238 - val_loss: 0.4504\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0247 - val_loss: 0.3728\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0230 - val_loss: 0.3768\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0274 - val_loss: 0.3772\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0252 - val_loss: 0.4146\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0258 - val_loss: 0.3799\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0241 - val_loss: 0.4342\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0236 - val_loss: 0.3999\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0232 - val_loss: 0.3830\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0251 - val_loss: 0.3913\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0251 - val_loss: 0.3761\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0224 - val_loss: 0.3831\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0236 - val_loss: 0.3765\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0218 - val_loss: 0.4051\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0230 - val_loss: 0.3805\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0224 - val_loss: 0.3803\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0230 - val_loss: 0.3694\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0222 - val_loss: 0.3804\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0228 - val_loss: 0.3965\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0243 - val_loss: 0.3742\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0220 - val_loss: 0.3787\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0244 - val_loss: 0.3656\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0244 - val_loss: 0.3759\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0248 - val_loss: 0.3986\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0223 - val_loss: 0.3900\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0214 - val_loss: 0.3822\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0226 - val_loss: 0.4161\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0220 - val_loss: 0.3823\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0222 - val_loss: 0.4321\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0252 - val_loss: 0.3804\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0229 - val_loss: 0.4223\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0221 - val_loss: 0.4046\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0226 - val_loss: 0.3914\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0220 - val_loss: 0.3886\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 467us/step - loss: 8.6247 - val_loss: 2.7791\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 1.4659 - val_loss: 2.2561\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 1.0786 - val_loss: 1.6379\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.7398 - val_loss: 1.2531\n",
      "Epoch 5/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.5365 - val_loss: 1.0762\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.6831 - val_loss: 1.2078\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.4710 - val_loss: 0.9364\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.3631 - val_loss: 0.8561\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.3469 - val_loss: 1.6543\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.2879 - val_loss: 1.2368\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.2680 - val_loss: 0.7608\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.2312 - val_loss: 0.6813\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.2286 - val_loss: 0.6969\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1997 - val_loss: 0.5705\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1969 - val_loss: 0.5538\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1996 - val_loss: 0.5034\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1747 - val_loss: 0.5247\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1902 - val_loss: 0.5182\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1790 - val_loss: 0.5249\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1876 - val_loss: 0.5001\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1420 - val_loss: 0.5708\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1578 - val_loss: 0.4808\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1556 - val_loss: 0.4431\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1493 - val_loss: 0.4148\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1404 - val_loss: 0.4590\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1303 - val_loss: 0.4350\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1314 - val_loss: 0.4609\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1222 - val_loss: 0.4419\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1129 - val_loss: 0.4850\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1282 - val_loss: 0.4204\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1079 - val_loss: 0.4104\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1165 - val_loss: 0.3802\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0974 - val_loss: 0.4433\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0993 - val_loss: 0.3715\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0928 - val_loss: 0.3544\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0985 - val_loss: 0.3522\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1130 - val_loss: 0.3841\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1296 - val_loss: 0.4164\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0976 - val_loss: 0.3727\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0859 - val_loss: 0.4192\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0850 - val_loss: 0.3437\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0870 - val_loss: 0.3583\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0910 - val_loss: 0.4773\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0813 - val_loss: 0.4439\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0807 - val_loss: 0.3348\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0792 - val_loss: 0.3475\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0752 - val_loss: 0.3448\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0766 - val_loss: 0.3263\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0731 - val_loss: 0.4088\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0701 - val_loss: 0.3272\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0729 - val_loss: 0.3522\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0676 - val_loss: 0.3187\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0700 - val_loss: 0.3243\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0688 - val_loss: 0.3433\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0873 - val_loss: 0.3470\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0652 - val_loss: 0.3339\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0640 - val_loss: 0.3105\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0670 - val_loss: 0.3289\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0675 - val_loss: 0.3847\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0679 - val_loss: 0.3596\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0630 - val_loss: 0.3649\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0625 - val_loss: 0.3427\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0637 - val_loss: 0.3477\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0611 - val_loss: 0.3078\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0641 - val_loss: 0.3288\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0583 - val_loss: 0.3391\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0581 - val_loss: 0.3370\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0550 - val_loss: 0.3103\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0629 - val_loss: 0.3490\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0598 - val_loss: 0.3163\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0597 - val_loss: 0.3782\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0544 - val_loss: 0.5339\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0533 - val_loss: 0.4606\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0588 - val_loss: 0.3174\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0568 - val_loss: 0.3569\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0516 - val_loss: 0.3195\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0553 - val_loss: 0.5396\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0651 - val_loss: 0.5541\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0552 - val_loss: 0.3854\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0499 - val_loss: 0.3400\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0485 - val_loss: 0.3226\n",
      "Epoch 82/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0506 - val_loss: 0.3237\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0541 - val_loss: 0.3268\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0468 - val_loss: 0.3259\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0544 - val_loss: 0.3275\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0501 - val_loss: 0.3316\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0495 - val_loss: 0.3146\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0448 - val_loss: 0.3188\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0448 - val_loss: 0.3269\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0496 - val_loss: 0.3085\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0478 - val_loss: 0.3024\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0573 - val_loss: 0.3133\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0466 - val_loss: 0.3154\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0437 - val_loss: 0.3163\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0487 - val_loss: 0.3404\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0419 - val_loss: 0.3127\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0437 - val_loss: 0.3140\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0445 - val_loss: 0.2980\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0524 - val_loss: 0.3019\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0427 - val_loss: 0.2974\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0448 - val_loss: 0.3068\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0428 - val_loss: 0.3740\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0452 - val_loss: 0.3053\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0506 - val_loss: 0.3228\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0410 - val_loss: 0.3220\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0439 - val_loss: 0.3457\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0391 - val_loss: 0.3605\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0376 - val_loss: 0.3120\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0387 - val_loss: 0.3245\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0399 - val_loss: 0.3141\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0393 - val_loss: 0.3097\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0388 - val_loss: 0.3098\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0446 - val_loss: 0.3716\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 183us/step - loss: 0.0462 - val_loss: 0.3029\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0444 - val_loss: 0.3165\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0417 - val_loss: 0.3334\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0386 - val_loss: 0.3167\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0384 - val_loss: 0.3097\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0375 - val_loss: 0.3075\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0351 - val_loss: 0.3170\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0365 - val_loss: 0.3014\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0360 - val_loss: 0.3023\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0371 - val_loss: 0.3447\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0395 - val_loss: 0.3091\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0364 - val_loss: 0.3103\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0349 - val_loss: 0.3332\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0361 - val_loss: 0.3154\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0347 - val_loss: 0.3383\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0337 - val_loss: 0.3070\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0379 - val_loss: 0.3051\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0431 - val_loss: 0.3270\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0339 - val_loss: 0.3311\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0359 - val_loss: 0.3024\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0349 - val_loss: 0.3916\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0345 - val_loss: 0.3171\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0348 - val_loss: 0.3258\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0391 - val_loss: 0.3328\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0349 - val_loss: 0.3086\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0352 - val_loss: 0.3222\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0321 - val_loss: 0.3199\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0314 - val_loss: 0.3197\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0384 - val_loss: 0.3199\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0331 - val_loss: 0.3023\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0339 - val_loss: 0.3940\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0322 - val_loss: 0.3188\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0346 - val_loss: 0.3242\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 190us/step - loss: 0.0420 - val_loss: 0.3386\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0384 - val_loss: 0.3047\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0331 - val_loss: 0.3166\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0297 - val_loss: 0.3199\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0305 - val_loss: 0.3107\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0324 - val_loss: 0.3062\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0298 - val_loss: 0.4254\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0315 - val_loss: 0.3091\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0349 - val_loss: 0.3433\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0328 - val_loss: 0.3165\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0312 - val_loss: 0.3223\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0295 - val_loss: 0.3364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0296 - val_loss: 0.3228\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0311 - val_loss: 0.3220\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0290 - val_loss: 0.3213\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0346 - val_loss: 0.3183\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0299 - val_loss: 0.3114\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0298 - val_loss: 0.3322\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0303 - val_loss: 0.2991\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0304 - val_loss: 0.3159\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0363 - val_loss: 0.3212\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0326 - val_loss: 0.3390\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0308 - val_loss: 0.3099\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0299 - val_loss: 0.3241\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0293 - val_loss: 0.3188\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0319 - val_loss: 0.3297\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0301 - val_loss: 0.3002\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0290 - val_loss: 0.3508\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0292 - val_loss: 0.3088\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0272 - val_loss: 0.3330\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0271 - val_loss: 0.3488\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0287 - val_loss: 0.3217\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0261 - val_loss: 0.3335\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0269 - val_loss: 0.3143\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0284 - val_loss: 0.3132\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0257 - val_loss: 0.3368\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0267 - val_loss: 0.3224\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0294 - val_loss: 0.3227\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0275 - val_loss: 0.3249\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0275 - val_loss: 0.3232\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0348 - val_loss: 0.3659\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0291 - val_loss: 0.3507\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0267 - val_loss: 0.3163\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0305 - val_loss: 0.3612\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0273 - val_loss: 0.3285\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0278 - val_loss: 0.3270\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0271 - val_loss: 0.3294\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0251 - val_loss: 0.3189\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0241 - val_loss: 0.3190\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0257 - val_loss: 0.3323\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0260 - val_loss: 0.3130\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0250 - val_loss: 0.3369\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0273 - val_loss: 0.3368\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0269 - val_loss: 0.3317\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0254 - val_loss: 0.3320\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0266 - val_loss: 0.3254\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0259 - val_loss: 0.3478\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0254 - val_loss: 0.3220\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0250 - val_loss: 0.3311\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0256 - val_loss: 0.3436\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0268 - val_loss: 0.3223\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0253 - val_loss: 0.3300\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0247 - val_loss: 0.3376\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0253 - val_loss: 0.3231\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0302 - val_loss: 0.3293\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0254 - val_loss: 0.3310\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0275 - val_loss: 0.3365\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0238 - val_loss: 0.3352\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0268 - val_loss: 0.3380\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0265 - val_loss: 0.3166\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0269 - val_loss: 0.3540\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0268 - val_loss: 0.3387\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0240 - val_loss: 0.3561\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0251 - val_loss: 0.3314\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0266 - val_loss: 0.3358\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0248 - val_loss: 0.4404\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0240 - val_loss: 0.3378\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0252 - val_loss: 0.3324\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0241 - val_loss: 0.3186\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0244 - val_loss: 0.3438\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0234 - val_loss: 0.3302\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0233 - val_loss: 0.3370\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0229 - val_loss: 0.3228\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0232 - val_loss: 0.3373\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0227 - val_loss: 0.4454\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0274 - val_loss: 0.3233\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0265 - val_loss: 0.3397\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0252 - val_loss: 0.3347\n",
      "Epoch 235/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0228 - val_loss: 0.3376\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0249 - val_loss: 0.3402\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0288 - val_loss: 0.3398\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0266 - val_loss: 0.3350\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0258 - val_loss: 0.3655\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0313 - val_loss: 0.5416\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0300 - val_loss: 0.3344\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0243 - val_loss: 0.3275\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0226 - val_loss: 0.3818\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0218 - val_loss: 0.3302\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0232 - val_loss: 0.3575\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0265 - val_loss: 0.3516\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0226 - val_loss: 0.3403\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0215 - val_loss: 0.3387\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0232 - val_loss: 0.3562\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0237 - val_loss: 0.3260\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 471us/step - loss: 8.1307 - val_loss: 3.9541\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 1.5582 - val_loss: 1.9874\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.8802 - val_loss: 1.6643\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.6857 - val_loss: 1.5949\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.5775 - val_loss: 1.5967\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.4482 - val_loss: 1.4405\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.4609 - val_loss: 1.2114\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.3776 - val_loss: 1.3573\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.3652 - val_loss: 2.1997\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.3782 - val_loss: 0.9456\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.2481 - val_loss: 0.9208\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.2425 - val_loss: 0.8359\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.2031 - val_loss: 0.8800\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.2004 - val_loss: 1.0199\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1994 - val_loss: 0.9687\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1808 - val_loss: 0.7899\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1668 - val_loss: 0.7107\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1835 - val_loss: 0.7541\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1647 - val_loss: 0.7112\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1644 - val_loss: 0.7622\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1371 - val_loss: 0.8461\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1295 - val_loss: 0.7207\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1447 - val_loss: 0.6898\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1287 - val_loss: 0.7268\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1208 - val_loss: 0.7631\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1143 - val_loss: 0.7126\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1377 - val_loss: 0.7049\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1212 - val_loss: 0.7475\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1074 - val_loss: 0.6472\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1126 - val_loss: 0.6721\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1132 - val_loss: 0.6324\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1121 - val_loss: 0.7557\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1056 - val_loss: 0.7318\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.1069 - val_loss: 0.6372\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1007 - val_loss: 0.6863\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0891 - val_loss: 0.8261\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0951 - val_loss: 0.6540\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0982 - val_loss: 0.5941\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0862 - val_loss: 0.6189\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0856 - val_loss: 0.6135\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0847 - val_loss: 0.8058\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0833 - val_loss: 0.6265\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0772 - val_loss: 0.6181\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0839 - val_loss: 0.6091\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0756 - val_loss: 0.7380\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0724 - val_loss: 0.6604\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0969 - val_loss: 0.7323\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1300 - val_loss: 0.6579\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0716 - val_loss: 0.7691\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0727 - val_loss: 0.6321\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0729 - val_loss: 0.6178\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0713 - val_loss: 0.5976\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0660 - val_loss: 0.6052\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0719 - val_loss: 0.5853\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0741 - val_loss: 0.5916\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0773 - val_loss: 0.5914\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0642 - val_loss: 0.6665\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0672 - val_loss: 0.5982\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0612 - val_loss: 0.5681\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0606 - val_loss: 0.5726\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0578 - val_loss: 0.5934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0606 - val_loss: 0.6584\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0655 - val_loss: 0.5988\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0621 - val_loss: 0.5912\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0556 - val_loss: 0.5672\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0656 - val_loss: 0.5892\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0626 - val_loss: 0.9564\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0592 - val_loss: 0.5959\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0544 - val_loss: 0.5531\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0501 - val_loss: 0.5504\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0572 - val_loss: 0.6598\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0557 - val_loss: 0.5511\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0586 - val_loss: 0.6161\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0681 - val_loss: 0.5680\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0506 - val_loss: 0.5401\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0496 - val_loss: 0.6587\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0510 - val_loss: 0.5433\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0487 - val_loss: 0.6158\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0627 - val_loss: 0.6058\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0583 - val_loss: 0.5413\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0501 - val_loss: 0.5386\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0464 - val_loss: 0.5630\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0490 - val_loss: 0.5477\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0485 - val_loss: 0.5991\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0505 - val_loss: 0.5253\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0490 - val_loss: 0.5354\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0445 - val_loss: 0.5604\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0453 - val_loss: 0.5695\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0479 - val_loss: 0.7488\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0491 - val_loss: 0.5237\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0552 - val_loss: 0.6062\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0501 - val_loss: 0.5430\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0437 - val_loss: 0.5446\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0473 - val_loss: 0.5136\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0421 - val_loss: 0.5308\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0485 - val_loss: 0.5283\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0429 - val_loss: 0.5244\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0505 - val_loss: 0.5749\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0441 - val_loss: 0.5374\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0402 - val_loss: 0.5198\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0399 - val_loss: 0.5226\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0396 - val_loss: 0.5190\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0410 - val_loss: 0.5426\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0442 - val_loss: 0.5310\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0407 - val_loss: 0.5242\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0411 - val_loss: 0.5301\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0375 - val_loss: 0.5019\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0436 - val_loss: 0.5187\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0378 - val_loss: 0.5143\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0404 - val_loss: 0.5169\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0365 - val_loss: 0.5362\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0432 - val_loss: 0.5510\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0409 - val_loss: 0.5342\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0556 - val_loss: 0.5576\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0340 - val_loss: 0.5167\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0382 - val_loss: 0.5187\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0435 - val_loss: 0.5218\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0386 - val_loss: 0.7818\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0419 - val_loss: 0.5467\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0387 - val_loss: 0.5246\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0395 - val_loss: 0.5132\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0446 - val_loss: 0.5611\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0402 - val_loss: 0.5223\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0393 - val_loss: 0.5201\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0407 - val_loss: 0.6058\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0419 - val_loss: 0.5657\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0419 - val_loss: 0.5234\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0398 - val_loss: 0.5497\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0397 - val_loss: 0.5510\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0415 - val_loss: 0.5715\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0334 - val_loss: 0.6036\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0344 - val_loss: 0.5413\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0323 - val_loss: 0.5119\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0322 - val_loss: 0.4979\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0359 - val_loss: 0.5429\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0318 - val_loss: 0.5161\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0337 - val_loss: 0.5142\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0341 - val_loss: 0.5758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0468 - val_loss: 0.5464\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0414 - val_loss: 0.5404\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0500 - val_loss: 0.5401\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0419 - val_loss: 0.5413\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0356 - val_loss: 0.5133\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0293 - val_loss: 0.5292\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0300 - val_loss: 0.5262\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0333 - val_loss: 0.5226\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0358 - val_loss: 0.6109\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0303 - val_loss: 0.5207\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0355 - val_loss: 0.5728\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0315 - val_loss: 0.5383\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0312 - val_loss: 0.6422\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0337 - val_loss: 0.5516\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0361 - val_loss: 0.5293\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0345 - val_loss: 0.5592\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0348 - val_loss: 0.5704\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0345 - val_loss: 0.5245\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0528 - val_loss: 0.5679\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0333 - val_loss: 0.5331\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0292 - val_loss: 0.5340\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0290 - val_loss: 0.5184\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0318 - val_loss: 0.5349\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0280 - val_loss: 0.5192\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0281 - val_loss: 0.5186\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0277 - val_loss: 0.5340\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0340 - val_loss: 0.5251\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0272 - val_loss: 0.5363\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0291 - val_loss: 0.5515\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0283 - val_loss: 0.6525\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0323 - val_loss: 0.5508\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0337 - val_loss: 0.5283\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0271 - val_loss: 0.5838\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0262 - val_loss: 0.5216\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0286 - val_loss: 0.5297\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0317 - val_loss: 0.5206\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0285 - val_loss: 0.5162\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0275 - val_loss: 0.5608\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0267 - val_loss: 0.5180\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0274 - val_loss: 0.5397\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0274 - val_loss: 0.5726\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0293 - val_loss: 0.5575\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0278 - val_loss: 0.5992\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0309 - val_loss: 0.5316\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0249 - val_loss: 0.5398\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0272 - val_loss: 0.5252\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0264 - val_loss: 0.5480\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0317 - val_loss: 0.5354\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 162us/step - loss: 0.0260 - val_loss: 0.5702\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0279 - val_loss: 0.5310\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0286 - val_loss: 0.5262\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0252 - val_loss: 0.5305\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0248 - val_loss: 0.5553\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0245 - val_loss: 0.5415\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0267 - val_loss: 0.5365\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0272 - val_loss: 0.5274\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0263 - val_loss: 0.5538\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0278 - val_loss: 0.5322\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0296 - val_loss: 0.5307\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0236 - val_loss: 0.5533\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0250 - val_loss: 0.5380\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0269 - val_loss: 0.5374\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0263 - val_loss: 0.5441\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0265 - val_loss: 0.5323\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0251 - val_loss: 0.6009\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0324 - val_loss: 0.5537\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0276 - val_loss: 0.5500\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0260 - val_loss: 0.5592\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0254 - val_loss: 0.5388\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0267 - val_loss: 0.5607\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0269 - val_loss: 0.5488\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0262 - val_loss: 0.5268\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0249 - val_loss: 0.5375\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0260 - val_loss: 0.5318\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0254 - val_loss: 0.5596\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0275 - val_loss: 0.5383\n",
      "Epoch 215/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0261 - val_loss: 0.5689\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0259 - val_loss: 0.5373\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0258 - val_loss: 0.5558\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0250 - val_loss: 0.5608\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0292 - val_loss: 0.5143\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0266 - val_loss: 0.5699\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0276 - val_loss: 0.5331\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0259 - val_loss: 0.5543\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0247 - val_loss: 0.5583\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0256 - val_loss: 0.5213\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0242 - val_loss: 0.5514\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0234 - val_loss: 0.5549\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0241 - val_loss: 0.5146\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0218 - val_loss: 0.5333\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0230 - val_loss: 0.5349\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0229 - val_loss: 0.5459\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0238 - val_loss: 0.5212\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0268 - val_loss: 0.5391\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0224 - val_loss: 0.5237\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0232 - val_loss: 0.5305\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0238 - val_loss: 0.5309\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0224 - val_loss: 0.5674\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0247 - val_loss: 0.5246\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0280 - val_loss: 0.5468\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0270 - val_loss: 0.6029\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0232 - val_loss: 0.5414\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0217 - val_loss: 0.5483\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0206 - val_loss: 0.5450\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0215 - val_loss: 0.5282\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0217 - val_loss: 0.5440\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0223 - val_loss: 0.5335\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0216 - val_loss: 0.5340\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0214 - val_loss: 0.5195\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0263 - val_loss: 0.5210\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0214 - val_loss: 0.5273\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0217 - val_loss: 0.5388\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 471us/step - loss: 7.9982 - val_loss: 3.1800\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 1.8930 - val_loss: 21.2997\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 1.5818 - val_loss: 1.7401\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.8542 - val_loss: 1.4479\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.6192 - val_loss: 1.0914\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.4246 - val_loss: 1.0094\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.3709 - val_loss: 1.2427\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.3129 - val_loss: 1.0163\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.4184 - val_loss: 1.2297\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.4706 - val_loss: 1.1002\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.3282 - val_loss: 0.9884\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.2864 - val_loss: 0.7587\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.2895 - val_loss: 0.8502\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.2364 - val_loss: 0.7226\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2035 - val_loss: 0.7116\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1877 - val_loss: 0.7517\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1593 - val_loss: 0.7457\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1648 - val_loss: 0.6886\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1851 - val_loss: 0.6524\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1675 - val_loss: 0.7668\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1463 - val_loss: 0.7531\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1383 - val_loss: 0.7847\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1415 - val_loss: 0.6760\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1540 - val_loss: 0.6881\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1321 - val_loss: 0.6909\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1138 - val_loss: 0.5986\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1160 - val_loss: 0.6378\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1202 - val_loss: 0.7525\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1160 - val_loss: 0.6963\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1138 - val_loss: 0.9401\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1058 - val_loss: 0.6060\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1022 - val_loss: 0.6155\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1047 - val_loss: 0.6355\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0983 - val_loss: 0.6264\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0998 - val_loss: 0.5880\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0965 - val_loss: 0.5204\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1121 - val_loss: 0.5084\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0971 - val_loss: 0.6380\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0894 - val_loss: 0.5242\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0874 - val_loss: 0.5288\n",
      "Epoch 41/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0952 - val_loss: 0.5473\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.1120 - val_loss: 0.5183\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0982 - val_loss: 0.5196\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0779 - val_loss: 0.6029\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0827 - val_loss: 0.5070\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0884 - val_loss: 0.5591\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0731 - val_loss: 0.5773\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0719 - val_loss: 0.5238\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0780 - val_loss: 0.4941\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0732 - val_loss: 0.5166\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0694 - val_loss: 0.4903\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0666 - val_loss: 1.9378\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1152 - val_loss: 0.4856\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0755 - val_loss: 0.5008\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0741 - val_loss: 0.5341\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1280 - val_loss: 0.5610\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0603 - val_loss: 0.5095\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0673 - val_loss: 0.5111\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0618 - val_loss: 0.4897\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0630 - val_loss: 0.4747\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0574 - val_loss: 0.4541\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0595 - val_loss: 0.4718\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0578 - val_loss: 0.4918\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0623 - val_loss: 0.4728\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0580 - val_loss: 0.5988\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0649 - val_loss: 0.5221\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0897 - val_loss: 0.4806\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0748 - val_loss: 0.4899\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0928 - val_loss: 0.4596\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0643 - val_loss: 0.4990\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0565 - val_loss: 0.4576\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0538 - val_loss: 0.5173\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0548 - val_loss: 0.4909\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0532 - val_loss: 0.4464\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0538 - val_loss: 0.4826\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0521 - val_loss: 0.4698\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0532 - val_loss: 0.4636\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0503 - val_loss: 0.4838\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0480 - val_loss: 0.4597\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0520 - val_loss: 0.5617\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0497 - val_loss: 0.4609\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0487 - val_loss: 0.4715\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0482 - val_loss: 0.4584\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0501 - val_loss: 0.4611\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0588 - val_loss: 0.4392\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0474 - val_loss: 0.5000\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0477 - val_loss: 0.4527\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0543 - val_loss: 0.4688\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0468 - val_loss: 0.4730\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0472 - val_loss: 0.4628\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0455 - val_loss: 0.5021\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0448 - val_loss: 0.4607\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0427 - val_loss: 0.4621\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0419 - val_loss: 0.4693\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0441 - val_loss: 0.5649\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0428 - val_loss: 0.5157\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0418 - val_loss: 0.4998\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0419 - val_loss: 0.5972\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0446 - val_loss: 0.4644\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0388 - val_loss: 0.5422\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0397 - val_loss: 0.4579\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0414 - val_loss: 0.4391\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0402 - val_loss: 0.4659\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0432 - val_loss: 0.4803\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0454 - val_loss: 0.4520\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0399 - val_loss: 0.4677\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0424 - val_loss: 0.4392\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0448 - val_loss: 0.4516\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0372 - val_loss: 0.4531\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0394 - val_loss: 0.5468\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0364 - val_loss: 0.4831\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0398 - val_loss: 0.4654\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0404 - val_loss: 0.4343\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0359 - val_loss: 0.4577\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0446 - val_loss: 0.4658\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0397 - val_loss: 0.4627\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0357 - val_loss: 0.5144\n",
      "Epoch 118/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0373 - val_loss: 0.4677\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0342 - val_loss: 0.4609\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0384 - val_loss: 0.4474\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0369 - val_loss: 0.4622\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0364 - val_loss: 0.4986\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0341 - val_loss: 0.4979\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0476 - val_loss: 0.4721\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0426 - val_loss: 0.4631\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0449 - val_loss: 0.4615\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0331 - val_loss: 0.4605\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0345 - val_loss: 0.4673\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0341 - val_loss: 0.4632\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0310 - val_loss: 0.4761\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0453 - val_loss: 0.4720\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0546 - val_loss: 0.5870\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0392 - val_loss: 0.5076\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0625 - val_loss: 0.5324\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0428 - val_loss: 0.4852\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0327 - val_loss: 0.4544\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0348 - val_loss: 0.4774\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0348 - val_loss: 0.4834\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0336 - val_loss: 0.5208\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0381 - val_loss: 0.4948\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0377 - val_loss: 0.5923\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0365 - val_loss: 0.4637\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0322 - val_loss: 0.4583\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0341 - val_loss: 0.5186\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0339 - val_loss: 0.4787\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0325 - val_loss: 0.5134\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0361 - val_loss: 0.4669\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0290 - val_loss: 0.4759\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0316 - val_loss: 0.5135\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0389 - val_loss: 0.4822\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0299 - val_loss: 0.5545\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0342 - val_loss: 0.5210\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0309 - val_loss: 0.4803\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0396 - val_loss: 0.4703\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0384 - val_loss: 0.4782\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0319 - val_loss: 0.4910\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0296 - val_loss: 0.4599\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0302 - val_loss: 0.4809\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0299 - val_loss: 0.4643\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0293 - val_loss: 0.4698\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0292 - val_loss: 0.4659\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0340 - val_loss: 0.4873\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0296 - val_loss: 0.4750\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0268 - val_loss: 0.4702\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0274 - val_loss: 0.4688\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0282 - val_loss: 0.4768\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0324 - val_loss: 0.4813\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0459 - val_loss: 0.4695\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0273 - val_loss: 0.5456\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0304 - val_loss: 0.4924\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0310 - val_loss: 0.5031\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0307 - val_loss: 0.6105\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0294 - val_loss: 0.4825\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0290 - val_loss: 0.4946\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0315 - val_loss: 0.4927\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0277 - val_loss: 0.4990\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0271 - val_loss: 0.4890\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0283 - val_loss: 0.4847\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0290 - val_loss: 0.4859\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0282 - val_loss: 0.5092\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0276 - val_loss: 0.4664\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0261 - val_loss: 0.5576\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0260 - val_loss: 0.4911\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0279 - val_loss: 0.5315\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0277 - val_loss: 0.4800\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0278 - val_loss: 0.4882\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0279 - val_loss: 0.5048\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0283 - val_loss: 0.4940\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0268 - val_loss: 0.4905\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0277 - val_loss: 0.5013\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0260 - val_loss: 0.4942\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0285 - val_loss: 0.4714\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0261 - val_loss: 0.4741\n",
      "Epoch 194/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0288 - val_loss: 0.4922\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0273 - val_loss: 0.5141\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0351 - val_loss: 0.4919\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0272 - val_loss: 0.5034\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0306 - val_loss: 0.4852\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0277 - val_loss: 0.5123\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0268 - val_loss: 0.5107\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0234 - val_loss: 0.5058\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0266 - val_loss: 0.4683\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0292 - val_loss: 0.4882\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0271 - val_loss: 0.4858\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0247 - val_loss: 0.4735\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0245 - val_loss: 0.4740\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0258 - val_loss: 0.4835\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0346 - val_loss: 0.4990\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0416 - val_loss: 0.4937\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0270 - val_loss: 0.4928\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0244 - val_loss: 0.4942\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0238 - val_loss: 0.4865\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0237 - val_loss: 0.5203\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0238 - val_loss: 0.4869\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0247 - val_loss: 0.5074\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0256 - val_loss: 0.4915\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0266 - val_loss: 0.4852\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0236 - val_loss: 0.4796\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0254 - val_loss: 0.5075\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0223 - val_loss: 0.4903\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0214 - val_loss: 0.4928\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0236 - val_loss: 0.4896\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0225 - val_loss: 0.4903\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0272 - val_loss: 0.4881\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0265 - val_loss: 0.5033\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0264 - val_loss: 0.5430\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0254 - val_loss: 0.4874\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0248 - val_loss: 0.4955\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0249 - val_loss: 0.4847\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0238 - val_loss: 0.4991\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0241 - val_loss: 0.4964\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0252 - val_loss: 0.4867\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0237 - val_loss: 0.4864\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0224 - val_loss: 0.4901\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0244 - val_loss: 0.5051\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0234 - val_loss: 0.4984\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0233 - val_loss: 0.4846\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0232 - val_loss: 0.5458\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0241 - val_loss: 0.4785\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0240 - val_loss: 0.4982\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0261 - val_loss: 0.4881\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0286 - val_loss: 0.5086\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0213 - val_loss: 0.5062\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0212 - val_loss: 0.5123\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0247 - val_loss: 0.4919\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0328 - val_loss: 0.5090\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0247 - val_loss: 0.5015\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0245 - val_loss: 0.4953\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0219 - val_loss: 0.4870\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0201 - val_loss: 0.5025\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 476us/step - loss: 7.3519 - val_loss: 3.0485\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 2.0390 - val_loss: 2.0403\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 1.1095 - val_loss: 1.8033\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.6235 - val_loss: 1.3041\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.6072 - val_loss: 0.9611\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.7393 - val_loss: 1.1441\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.5562 - val_loss: 1.1307\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.3797 - val_loss: 1.0941\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.3888 - val_loss: 0.9174\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.3144 - val_loss: 0.8514\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2594 - val_loss: 0.7099\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 181us/step - loss: 0.2563 - val_loss: 0.7105\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.2820 - val_loss: 0.6938\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2551 - val_loss: 0.6778\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1740 - val_loss: 0.5805\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1796 - val_loss: 0.7274\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1795 - val_loss: 1.2730\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1462 - val_loss: 0.5917\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1547 - val_loss: 0.5674\n",
      "Epoch 20/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1486 - val_loss: 0.5132\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1718 - val_loss: 0.5689\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1325 - val_loss: 0.5520\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1182 - val_loss: 0.4852\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1240 - val_loss: 0.5125\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1665 - val_loss: 0.5564\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1311 - val_loss: 0.5732\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1270 - val_loss: 0.4850\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1220 - val_loss: 0.5046\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1387 - val_loss: 0.5577\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1310 - val_loss: 0.4851\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1146 - val_loss: 0.4544\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1069 - val_loss: 0.4805\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0967 - val_loss: 0.4418\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1058 - val_loss: 0.4347\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0959 - val_loss: 0.6441\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1024 - val_loss: 0.5972\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0983 - val_loss: 0.4683\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0989 - val_loss: 0.4680\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0857 - val_loss: 0.4882\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0932 - val_loss: 0.4290\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0927 - val_loss: 0.4617\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0893 - val_loss: 0.4307\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0790 - val_loss: 0.4184\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0944 - val_loss: 0.4149\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0738 - val_loss: 0.4049\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0754 - val_loss: 0.4178\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0763 - val_loss: 0.3957\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0740 - val_loss: 0.4672\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0760 - val_loss: 0.5304\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0917 - val_loss: 0.5648\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0807 - val_loss: 0.4221\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0668 - val_loss: 0.4252\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0880 - val_loss: 0.4040\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0764 - val_loss: 0.4126\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0710 - val_loss: 0.4525\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0691 - val_loss: 0.3818\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0727 - val_loss: 0.3908\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0721 - val_loss: 0.3723\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0646 - val_loss: 0.4471\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0648 - val_loss: 0.3978\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0684 - val_loss: 0.4152\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0636 - val_loss: 0.3975\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0692 - val_loss: 0.4493\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0785 - val_loss: 0.4484\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0617 - val_loss: 0.4164\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0545 - val_loss: 0.4911\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0571 - val_loss: 0.5554\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0614 - val_loss: 0.3963\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0552 - val_loss: 0.8093\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0555 - val_loss: 0.3628\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0551 - val_loss: 0.3974\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0594 - val_loss: 0.3698\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0534 - val_loss: 0.3857\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0627 - val_loss: 0.3525\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0564 - val_loss: 0.4199\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0570 - val_loss: 0.4070\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0526 - val_loss: 0.3824\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0518 - val_loss: 0.4266\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0475 - val_loss: 0.3850\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0552 - val_loss: 0.4059\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0532 - val_loss: 0.3808\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0482 - val_loss: 0.3711\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0620 - val_loss: 0.4257\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0633 - val_loss: 0.3939\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0556 - val_loss: 0.7023\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0515 - val_loss: 0.4057\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0619 - val_loss: 0.3807\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0591 - val_loss: 0.5679\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0464 - val_loss: 0.3957\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0456 - val_loss: 0.3748\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0460 - val_loss: 0.4921\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0430 - val_loss: 0.4434\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0486 - val_loss: 0.3660\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0467 - val_loss: 0.3702\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0544 - val_loss: 0.3821\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0442 - val_loss: 0.3939\n",
      "Epoch 97/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0519 - val_loss: 0.3781\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0453 - val_loss: 0.3765\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0465 - val_loss: 0.3741\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0459 - val_loss: 0.3949\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0436 - val_loss: 0.4180\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0462 - val_loss: 0.4367\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0433 - val_loss: 0.3458\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0419 - val_loss: 0.4160\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0424 - val_loss: 0.4112\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0500 - val_loss: 0.3765\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0429 - val_loss: 0.3658\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0456 - val_loss: 0.4720\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0496 - val_loss: 0.3656\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0438 - val_loss: 0.3573\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0388 - val_loss: 0.3464\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0385 - val_loss: 0.3702\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0394 - val_loss: 0.3582\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0419 - val_loss: 0.3604\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0437 - val_loss: 0.3483\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0400 - val_loss: 0.3708\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0381 - val_loss: 0.3575\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0488 - val_loss: 0.4320\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0414 - val_loss: 0.3756\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0422 - val_loss: 0.3551\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0368 - val_loss: 0.3608\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0349 - val_loss: 0.3596\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0355 - val_loss: 0.3824\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0434 - val_loss: 0.3542\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0365 - val_loss: 0.3942\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0398 - val_loss: 0.3660\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0444 - val_loss: 0.3751\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0576 - val_loss: 0.3710\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0389 - val_loss: 0.3710\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0342 - val_loss: 0.3641\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0349 - val_loss: 0.4886\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0404 - val_loss: 0.4788\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0331 - val_loss: 0.3796\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0354 - val_loss: 0.3485\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0340 - val_loss: 0.3710\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0335 - val_loss: 0.3946\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0387 - val_loss: 0.3717\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0372 - val_loss: 0.4484\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0365 - val_loss: 0.3838\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0374 - val_loss: 0.3563\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0329 - val_loss: 0.3552\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0409 - val_loss: 0.3776\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0341 - val_loss: 0.3597\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0342 - val_loss: 0.3871\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0413 - val_loss: 0.3658\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0359 - val_loss: 0.3630\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0313 - val_loss: 0.3719\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0402 - val_loss: 0.3721\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0506 - val_loss: 0.3645\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0344 - val_loss: 0.3569\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0321 - val_loss: 0.3669\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0375 - val_loss: 0.3480\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0331 - val_loss: 0.5083\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0438 - val_loss: 0.3718\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0365 - val_loss: 0.3827\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0362 - val_loss: 0.3574\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0354 - val_loss: 0.3467\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0321 - val_loss: 0.4416\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0369 - val_loss: 0.3619\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0294 - val_loss: 0.3549\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0344 - val_loss: 0.3570\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0398 - val_loss: 0.3801\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0312 - val_loss: 0.3738\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0318 - val_loss: 0.3741\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0348 - val_loss: 0.3869\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0301 - val_loss: 0.3704\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0322 - val_loss: 0.3889\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0411 - val_loss: 0.3723\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0326 - val_loss: 0.5207\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0322 - val_loss: 0.4064\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0315 - val_loss: 0.3703\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0325 - val_loss: 0.3753\n",
      "Epoch 173/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0275 - val_loss: 0.3787\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0332 - val_loss: 0.3928\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0288 - val_loss: 0.3977\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0307 - val_loss: 0.3762\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0322 - val_loss: 0.3897\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0280 - val_loss: 0.4379\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0356 - val_loss: 0.4252\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0313 - val_loss: 0.3840\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0312 - val_loss: 0.3776\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0293 - val_loss: 0.3730\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0284 - val_loss: 0.3664\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0326 - val_loss: 0.4092\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0307 - val_loss: 0.3661\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0316 - val_loss: 0.3639\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0309 - val_loss: 0.3714\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0289 - val_loss: 0.3909\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0268 - val_loss: 0.3622\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0287 - val_loss: 0.3592\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0266 - val_loss: 0.3814\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0272 - val_loss: 0.4283\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0304 - val_loss: 0.3687\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0324 - val_loss: 0.3763\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0262 - val_loss: 0.4078\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0281 - val_loss: 0.3772\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0310 - val_loss: 0.3848\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0256 - val_loss: 0.3806\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0320 - val_loss: 0.3591\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0300 - val_loss: 0.3839\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0268 - val_loss: 0.3650\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0297 - val_loss: 0.3863\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0284 - val_loss: 0.3766\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0311 - val_loss: 0.3626\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0275 - val_loss: 0.3629\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0292 - val_loss: 0.3914\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0286 - val_loss: 0.3846\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0280 - val_loss: 0.3741\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0257 - val_loss: 0.3562\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0246 - val_loss: 0.4480\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0250 - val_loss: 0.3656\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0251 - val_loss: 0.4321\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0272 - val_loss: 0.4374\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0309 - val_loss: 0.3807\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0294 - val_loss: 0.4035\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0251 - val_loss: 0.3662\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0272 - val_loss: 0.3777\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0256 - val_loss: 0.3843\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0369 - val_loss: 0.3595\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0262 - val_loss: 0.3753\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0248 - val_loss: 0.3685\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0255 - val_loss: 0.3794\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0289 - val_loss: 0.3891\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0290 - val_loss: 0.3717\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0249 - val_loss: 0.4227\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0267 - val_loss: 0.3785\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0247 - val_loss: 0.3681\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0257 - val_loss: 0.3980\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0245 - val_loss: 0.3808\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0221 - val_loss: 0.3764\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0325 - val_loss: 0.3806\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0229 - val_loss: 0.3674\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0273 - val_loss: 0.4005\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0369 - val_loss: 0.3819\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0224 - val_loss: 0.3743\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0238 - val_loss: 0.3744\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0230 - val_loss: 0.3603\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0248 - val_loss: 0.3924\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0226 - val_loss: 0.3733\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0263 - val_loss: 0.3836\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0229 - val_loss: 0.3695\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0314 - val_loss: 0.4014\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0264 - val_loss: 0.4089\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0264 - val_loss: 0.3950\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0244 - val_loss: 0.4117\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0218 - val_loss: 0.4165\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0224 - val_loss: 0.5105\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0287 - val_loss: 0.3638\n",
      "Epoch 249/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0225 - val_loss: 0.3836\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0236 - val_loss: 0.3696\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 475us/step - loss: 9.0726 - val_loss: 20.7568\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 3.2203 - val_loss: 2.3166\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 1.4096 - val_loss: 1.1842\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.8093 - val_loss: 2.1909\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.6470 - val_loss: 1.3085\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.4722 - val_loss: 1.0868\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.4097 - val_loss: 1.7041\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.4715 - val_loss: 0.8567\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.3457 - val_loss: 0.8007\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.3179 - val_loss: 1.0712\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.3234 - val_loss: 0.7530\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.2978 - val_loss: 0.8017\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.2651 - val_loss: 0.5733\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2787 - val_loss: 0.7298\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2608 - val_loss: 0.6604\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1893 - val_loss: 0.5139\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.2020 - val_loss: 0.5431\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.2014 - val_loss: 0.5110\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1771 - val_loss: 0.5372\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1847 - val_loss: 0.8053\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1552 - val_loss: 0.4429\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1613 - val_loss: 0.4355\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1620 - val_loss: 0.5895\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1607 - val_loss: 0.5447\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1803 - val_loss: 0.4690\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1645 - val_loss: 0.4412\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1283 - val_loss: 0.4301\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1165 - val_loss: 0.3821\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1421 - val_loss: 0.4223\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1212 - val_loss: 0.3863\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.1099 - val_loss: 0.3844\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1125 - val_loss: 0.4255\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1045 - val_loss: 0.3918\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1046 - val_loss: 0.3423\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1199 - val_loss: 0.3554\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1059 - val_loss: 0.3934\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1269 - val_loss: 0.3658\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0983 - val_loss: 0.3568\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0900 - val_loss: 0.6641\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1078 - val_loss: 0.4618\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0902 - val_loss: 0.4124\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0928 - val_loss: 0.3368\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0909 - val_loss: 0.3575\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0868 - val_loss: 0.3939\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1043 - val_loss: 0.3409\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1127 - val_loss: 0.3565\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0861 - val_loss: 0.3185\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0844 - val_loss: 0.3367\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0781 - val_loss: 0.5481\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0864 - val_loss: 0.3692\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0780 - val_loss: 0.3578\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0721 - val_loss: 0.3451\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0788 - val_loss: 0.3527\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0794 - val_loss: 0.4020\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0796 - val_loss: 0.4114\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0736 - val_loss: 0.3234\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0779 - val_loss: 0.3159\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0675 - val_loss: 0.3480\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0701 - val_loss: 0.3201\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0674 - val_loss: 0.3266\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0630 - val_loss: 0.2948\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0681 - val_loss: 0.3414\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0642 - val_loss: 0.2909\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0687 - val_loss: 0.2993\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0672 - val_loss: 0.3212\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0980 - val_loss: 0.3580\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1190 - val_loss: 0.3504\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0720 - val_loss: 0.5697\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0627 - val_loss: 0.3752\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0620 - val_loss: 0.3357\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0662 - val_loss: 0.3245\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0589 - val_loss: 0.3056\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0649 - val_loss: 0.2909\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0749 - val_loss: 0.3608\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0600 - val_loss: 0.2923\n",
      "Epoch 76/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0543 - val_loss: 0.3100\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0628 - val_loss: 0.4011\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0712 - val_loss: 0.3337\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0579 - val_loss: 0.2895\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0588 - val_loss: 0.3073\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0584 - val_loss: 0.2937\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0546 - val_loss: 0.3298\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0488 - val_loss: 0.2770\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0528 - val_loss: 0.2860\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0519 - val_loss: 0.2847\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0493 - val_loss: 0.2898\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0492 - val_loss: 0.4042\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0568 - val_loss: 0.2859\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0510 - val_loss: 0.3200\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0546 - val_loss: 0.2953\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0520 - val_loss: 0.2757\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0583 - val_loss: 0.2941\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0473 - val_loss: 0.2876\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0479 - val_loss: 0.2788\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0475 - val_loss: 0.2828\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0497 - val_loss: 0.2871\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0440 - val_loss: 0.2653\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0451 - val_loss: 0.2828\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0433 - val_loss: 0.2889\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0452 - val_loss: 0.2708\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0491 - val_loss: 0.2996\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0436 - val_loss: 0.2729\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0441 - val_loss: 0.2722\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0469 - val_loss: 0.3322\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0599 - val_loss: 0.3300\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0509 - val_loss: 0.2664\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0515 - val_loss: 0.2811\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0430 - val_loss: 0.2855\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0414 - val_loss: 0.2920\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0451 - val_loss: 0.2802\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0460 - val_loss: 0.2757\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0408 - val_loss: 0.3855\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0439 - val_loss: 0.3124\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0518 - val_loss: 0.3006\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0428 - val_loss: 0.3160\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0488 - val_loss: 0.3080\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0403 - val_loss: 0.2927\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0455 - val_loss: 0.2858\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0380 - val_loss: 0.2785\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0389 - val_loss: 0.3881\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0416 - val_loss: 0.2762\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0466 - val_loss: 0.2841\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0421 - val_loss: 0.2885\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0356 - val_loss: 0.3143\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0362 - val_loss: 0.2824\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0405 - val_loss: 0.2687\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0419 - val_loss: 0.2921\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0392 - val_loss: 0.2667\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0345 - val_loss: 0.2733\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0366 - val_loss: 0.2526\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0443 - val_loss: 0.2831\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0372 - val_loss: 0.2725\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0353 - val_loss: 0.2584\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0352 - val_loss: 0.2806\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0385 - val_loss: 0.2694\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0486 - val_loss: 0.2969\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0390 - val_loss: 0.2716\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0360 - val_loss: 0.2966\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0360 - val_loss: 0.2778\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0377 - val_loss: 0.3703\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0385 - val_loss: 0.2709\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0338 - val_loss: 0.2731\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0345 - val_loss: 0.3353\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0455 - val_loss: 0.2981\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0337 - val_loss: 0.2790\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0333 - val_loss: 0.2766\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0390 - val_loss: 0.3180\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0370 - val_loss: 0.4179\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0378 - val_loss: 0.3805\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0385 - val_loss: 0.2755\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0360 - val_loss: 0.3162\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0332 - val_loss: 0.3430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0348 - val_loss: 0.3335\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0390 - val_loss: 0.2866\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0344 - val_loss: 0.2673\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0318 - val_loss: 0.2768\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0391 - val_loss: 0.7467\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0363 - val_loss: 0.3458\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 163us/step - loss: 0.0336 - val_loss: 0.2723\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0319 - val_loss: 0.2718\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0320 - val_loss: 0.2704\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0361 - val_loss: 0.3243\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0311 - val_loss: 0.2743\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0320 - val_loss: 0.2717\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0348 - val_loss: 0.2786\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0365 - val_loss: 0.2743\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0298 - val_loss: 0.2773\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0328 - val_loss: 0.3491\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0335 - val_loss: 0.2803\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0358 - val_loss: 0.2687\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0286 - val_loss: 0.2630\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0316 - val_loss: 0.2655\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0297 - val_loss: 0.2832\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0293 - val_loss: 0.2968\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0286 - val_loss: 0.2966\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0349 - val_loss: 0.2786\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0313 - val_loss: 0.2827\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0292 - val_loss: 0.2653\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0303 - val_loss: 0.2627\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0301 - val_loss: 0.2680\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0381 - val_loss: 0.2855\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0332 - val_loss: 0.2811\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0293 - val_loss: 0.2707\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0297 - val_loss: 0.2666\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0311 - val_loss: 0.2751\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0283 - val_loss: 0.2615\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0313 - val_loss: 0.2871\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0307 - val_loss: 0.3216\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0277 - val_loss: 0.4051\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0328 - val_loss: 0.2727\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0279 - val_loss: 0.2717\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0301 - val_loss: 0.3669\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0291 - val_loss: 0.2810\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0262 - val_loss: 0.2959\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0269 - val_loss: 0.2832\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0289 - val_loss: 0.2723\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0284 - val_loss: 0.2875\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0274 - val_loss: 0.2716\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0311 - val_loss: 0.3580\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0343 - val_loss: 0.2625\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0283 - val_loss: 0.2724\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0271 - val_loss: 0.2664\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0243 - val_loss: 0.3703\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0292 - val_loss: 0.2796\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0252 - val_loss: 0.2835\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0299 - val_loss: 0.2745\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0268 - val_loss: 0.2713\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0285 - val_loss: 0.2898\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0293 - val_loss: 0.2801\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0292 - val_loss: 0.2699\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0289 - val_loss: 0.2702\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0272 - val_loss: 0.4422\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0283 - val_loss: 0.2929\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0289 - val_loss: 0.2867\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0265 - val_loss: 0.2782\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0257 - val_loss: 0.2802\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0241 - val_loss: 0.2670\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0293 - val_loss: 0.2812\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0260 - val_loss: 0.3040\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0242 - val_loss: 0.2858\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0278 - val_loss: 0.2879\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0238 - val_loss: 0.2987\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0240 - val_loss: 0.4328\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0262 - val_loss: 0.2734\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0316 - val_loss: 0.2827\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0300 - val_loss: 0.2750\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0350 - val_loss: 0.2714\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0256 - val_loss: 0.2793\n",
      "Epoch 229/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0335 - val_loss: 0.2833\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0245 - val_loss: 0.2874\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0224 - val_loss: 0.2717\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0251 - val_loss: 0.2862\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0291 - val_loss: 0.2711\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0295 - val_loss: 0.2932\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0297 - val_loss: 0.3042\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0273 - val_loss: 0.2758\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0283 - val_loss: 0.3086\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0260 - val_loss: 0.2714\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0220 - val_loss: 0.2832\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0288 - val_loss: 0.2792\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0241 - val_loss: 0.3355\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0227 - val_loss: 0.2775\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0217 - val_loss: 0.2689\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0228 - val_loss: 0.2818\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0298 - val_loss: 0.2858\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0235 - val_loss: 0.2845\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0214 - val_loss: 0.2810\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0273 - val_loss: 0.3005\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0233 - val_loss: 0.3046\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0248 - val_loss: 0.2826\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 481us/step - loss: 7.5630 - val_loss: 3.7642\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 2.3725 - val_loss: 2.4496\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 1.1539 - val_loss: 1.2589\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.9362 - val_loss: 1.4271\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 1.5806 - val_loss: 2.3973\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.9520 - val_loss: 1.6818\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.5988 - val_loss: 1.4265\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.5615 - val_loss: 1.0272\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.3709 - val_loss: 1.0781\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.5350 - val_loss: 0.9280\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.3693 - val_loss: 1.6344\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.2885 - val_loss: 1.0055\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2904 - val_loss: 0.8491\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2479 - val_loss: 0.7153\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2582 - val_loss: 0.7256\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2419 - val_loss: 0.7108\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.2237 - val_loss: 0.6155\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.2543 - val_loss: 0.7495\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1851 - val_loss: 0.5990\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1769 - val_loss: 0.7704\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.1927 - val_loss: 0.6441\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1594 - val_loss: 0.8861\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1480 - val_loss: 0.9467\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1753 - val_loss: 0.6745\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1597 - val_loss: 0.5384\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1520 - val_loss: 0.5655\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1204 - val_loss: 0.6313\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1413 - val_loss: 0.4938\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1333 - val_loss: 0.5211\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1337 - val_loss: 0.5301\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1572 - val_loss: 0.4458\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1202 - val_loss: 0.4930\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1193 - val_loss: 0.5406\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1165 - val_loss: 0.7347\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1082 - val_loss: 0.4348\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1429 - val_loss: 0.4777\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1248 - val_loss: 0.4070\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1252 - val_loss: 0.4214\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1058 - val_loss: 0.4157\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1067 - val_loss: 0.5182\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1252 - val_loss: 0.4239\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1052 - val_loss: 0.6443\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0942 - val_loss: 0.3895\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1009 - val_loss: 0.4579\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0968 - val_loss: 1.2302\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0884 - val_loss: 0.4158\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0893 - val_loss: 0.3894\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0849 - val_loss: 0.4602\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0846 - val_loss: 0.5134\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0781 - val_loss: 0.3542\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0847 - val_loss: 0.3816\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0756 - val_loss: 0.4687\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0845 - val_loss: 0.3444\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0805 - val_loss: 0.3824\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0695 - val_loss: 0.4794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0816 - val_loss: 0.3701\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0704 - val_loss: 0.3874\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0736 - val_loss: 0.4185\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0713 - val_loss: 0.3510\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0723 - val_loss: 0.3311\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0706 - val_loss: 0.3407\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0681 - val_loss: 0.3471\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0819 - val_loss: 0.3384\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0675 - val_loss: 0.3575\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0629 - val_loss: 0.6612\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0761 - val_loss: 0.3411\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0659 - val_loss: 0.3568\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0639 - val_loss: 0.3483\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0710 - val_loss: 0.3135\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0679 - val_loss: 0.3633\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0609 - val_loss: 0.4569\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0782 - val_loss: 0.3702\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0720 - val_loss: 0.4379\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0668 - val_loss: 0.3330\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0598 - val_loss: 0.3413\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0870 - val_loss: 0.4210\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0676 - val_loss: 0.3385\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0592 - val_loss: 0.3620\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0684 - val_loss: 0.3757\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0595 - val_loss: 0.3206\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0753 - val_loss: 0.4444\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0715 - val_loss: 0.3252\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0657 - val_loss: 0.2963\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0564 - val_loss: 0.3117\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0511 - val_loss: 0.3057\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0574 - val_loss: 0.3373\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0516 - val_loss: 0.3743\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0509 - val_loss: 0.3177\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0542 - val_loss: 0.3198\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0512 - val_loss: 0.3206\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0515 - val_loss: 0.2934\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0672 - val_loss: 0.2980\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0574 - val_loss: 0.4208\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0513 - val_loss: 0.4777\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0583 - val_loss: 0.3233\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0549 - val_loss: 1.5014\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0635 - val_loss: 0.3099\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0551 - val_loss: 0.3106\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0545 - val_loss: 0.3117\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0543 - val_loss: 0.3339\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0728 - val_loss: 0.2986\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0657 - val_loss: 0.3131\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0521 - val_loss: 0.3832\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0495 - val_loss: 0.2911\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0487 - val_loss: 0.2953\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0458 - val_loss: 0.3482\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0484 - val_loss: 0.2981\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0457 - val_loss: 0.3319\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0461 - val_loss: 0.3083\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0481 - val_loss: 0.2917\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0498 - val_loss: 0.3379\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0456 - val_loss: 0.3026\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0424 - val_loss: 0.3183\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0465 - val_loss: 0.3154\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0456 - val_loss: 0.2827\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0412 - val_loss: 0.2820\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0476 - val_loss: 0.2790\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0455 - val_loss: 0.2819\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0395 - val_loss: 0.2812\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0436 - val_loss: 0.2744\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0424 - val_loss: 0.2778\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0413 - val_loss: 0.2898\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0436 - val_loss: 0.2792\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0403 - val_loss: 0.2884\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0402 - val_loss: 0.2971\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0412 - val_loss: 0.2982\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0415 - val_loss: 0.2994\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0450 - val_loss: 0.2868\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0446 - val_loss: 0.2930\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0415 - val_loss: 0.3135\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0396 - val_loss: 0.2817\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0404 - val_loss: 0.2805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0383 - val_loss: 0.3127\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0381 - val_loss: 0.3366\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0429 - val_loss: 0.2883\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0413 - val_loss: 0.2870\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0398 - val_loss: 0.2990\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0379 - val_loss: 0.2872\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0384 - val_loss: 0.3641\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0426 - val_loss: 0.2921\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0506 - val_loss: 0.6798\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0405 - val_loss: 0.2842\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0386 - val_loss: 0.3891\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0472 - val_loss: 0.2918\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0346 - val_loss: 0.2859\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0364 - val_loss: 0.2767\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0385 - val_loss: 0.3116\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0362 - val_loss: 0.2884\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0382 - val_loss: 0.2760\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0346 - val_loss: 0.2985\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0363 - val_loss: 0.2906\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0417 - val_loss: 0.2963\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0378 - val_loss: 0.2960\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0343 - val_loss: 0.2749\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0335 - val_loss: 0.3656\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0328 - val_loss: 0.2791\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0409 - val_loss: 0.3046\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0386 - val_loss: 0.2886\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0330 - val_loss: 0.3229\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0444 - val_loss: 0.3828\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0387 - val_loss: 0.3133\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0331 - val_loss: 0.2833\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0321 - val_loss: 0.2867\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 182us/step - loss: 0.0376 - val_loss: 0.2901\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 187us/step - loss: 0.0335 - val_loss: 0.2753\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0309 - val_loss: 0.2898\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0326 - val_loss: 0.3242\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0323 - val_loss: 0.3005\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 185us/step - loss: 0.0347 - val_loss: 0.2960\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 180us/step - loss: 0.0312 - val_loss: 0.2888\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0321 - val_loss: 0.2937\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0335 - val_loss: 0.2831\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0319 - val_loss: 0.2823\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0301 - val_loss: 0.2815\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0294 - val_loss: 0.2918\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0299 - val_loss: 0.2820\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0380 - val_loss: 0.2923\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0316 - val_loss: 0.2857\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0344 - val_loss: 0.2875\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0323 - val_loss: 0.2887\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0312 - val_loss: 0.3311\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0299 - val_loss: 0.2938\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0276 - val_loss: 0.4689\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0318 - val_loss: 0.2961\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0314 - val_loss: 0.2901\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0330 - val_loss: 0.3180\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0300 - val_loss: 0.4232\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0329 - val_loss: 0.2931\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0298 - val_loss: 0.3284\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0337 - val_loss: 0.3280\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0343 - val_loss: 0.2954\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0289 - val_loss: 0.2916\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0280 - val_loss: 0.3234\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0399 - val_loss: 0.2906\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0303 - val_loss: 0.2923\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0299 - val_loss: 0.2868\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0315 - val_loss: 0.2822\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0290 - val_loss: 0.3166\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0278 - val_loss: 0.3178\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0293 - val_loss: 0.2872\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0278 - val_loss: 0.3190\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0303 - val_loss: 0.2915\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0292 - val_loss: 0.2909\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0270 - val_loss: 0.3008\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0275 - val_loss: 0.2911\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0297 - val_loss: 0.2814\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0296 - val_loss: 0.3001\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 184us/step - loss: 0.0305 - val_loss: 0.3017\n",
      "Epoch 209/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0290 - val_loss: 0.2938\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0282 - val_loss: 0.2871\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0279 - val_loss: 0.2989\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0270 - val_loss: 0.2970\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0293 - val_loss: 0.2895\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0262 - val_loss: 0.2951\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0281 - val_loss: 0.2934\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0272 - val_loss: 0.3121\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0271 - val_loss: 0.3723\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0332 - val_loss: 0.3131\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0264 - val_loss: 0.3013\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0264 - val_loss: 0.3135\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0260 - val_loss: 0.2988\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0266 - val_loss: 0.2999\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0296 - val_loss: 0.3093\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0263 - val_loss: 0.3239\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0280 - val_loss: 0.3221\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0262 - val_loss: 0.2942\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0244 - val_loss: 0.2988\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0250 - val_loss: 0.3154\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0278 - val_loss: 0.2968\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0269 - val_loss: 0.3165\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0267 - val_loss: 0.3122\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0253 - val_loss: 0.3038\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0284 - val_loss: 0.3481\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0308 - val_loss: 0.3104\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0282 - val_loss: 0.3226\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0265 - val_loss: 0.3176\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0260 - val_loss: 0.2988\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0274 - val_loss: 0.3672\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0275 - val_loss: 0.2977\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0269 - val_loss: 0.3021\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0241 - val_loss: 0.3055\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0242 - val_loss: 0.2957\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0256 - val_loss: 0.3197\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0257 - val_loss: 0.3105\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0234 - val_loss: 0.2986\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0246 - val_loss: 0.3196\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0248 - val_loss: 0.3144\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0281 - val_loss: 0.3138\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0243 - val_loss: 0.3088\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0241 - val_loss: 0.3056\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 480us/step - loss: 7.5827 - val_loss: 3.0360\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 1.7391 - val_loss: 2.0711\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 1.0086 - val_loss: 2.3576\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.6506 - val_loss: 4.2512\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 1.1595 - val_loss: 1.0144\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.4962 - val_loss: 59.7284\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 2.6071 - val_loss: 4.1778\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 1.9138 - val_loss: 0.9459\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.7645 - val_loss: 4.0977\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.6067 - val_loss: 1.0051\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.4817 - val_loss: 0.7847\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.4254 - val_loss: 1.5855\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3204 - val_loss: 0.6060\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.3411 - val_loss: 0.6105\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.3548 - val_loss: 0.8195\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2588 - val_loss: 0.6585\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2256 - val_loss: 0.8053\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.2349 - val_loss: 0.5843\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2145 - val_loss: 0.5192\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1795 - val_loss: 0.5958\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1668 - val_loss: 0.4851\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2258 - val_loss: 0.5686\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1573 - val_loss: 1.8790\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2014 - val_loss: 0.4028\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1502 - val_loss: 0.5007\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.1925 - val_loss: 0.4126\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1905 - val_loss: 0.5835\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1655 - val_loss: 0.4629\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1293 - val_loss: 0.4247\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1202 - val_loss: 0.5668\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1349 - val_loss: 0.6792\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1165 - val_loss: 0.3774\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1206 - val_loss: 0.3936\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1106 - val_loss: 0.3911\n",
      "Epoch 35/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0994 - val_loss: 0.3712\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1070 - val_loss: 0.4529\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1112 - val_loss: 0.3463\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1165 - val_loss: 1.1380\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0970 - val_loss: 0.4457\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1321 - val_loss: 0.3572\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1179 - val_loss: 0.3576\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0966 - val_loss: 0.3298\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1046 - val_loss: 0.3614\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0856 - val_loss: 0.3129\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1036 - val_loss: 0.3477\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0868 - val_loss: 0.4561\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0867 - val_loss: 0.3995\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0938 - val_loss: 0.2974\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0896 - val_loss: 0.3199\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0821 - val_loss: 0.7406\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0876 - val_loss: 0.2940\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0729 - val_loss: 0.3731\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0691 - val_loss: 0.3253\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0937 - val_loss: 0.3112\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0745 - val_loss: 0.2909\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0719 - val_loss: 0.6061\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0870 - val_loss: 0.2940\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0762 - val_loss: 0.3196\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0696 - val_loss: 0.2943\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2082 - val_loss: 0.5682\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.1243 - val_loss: 0.3635\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1213 - val_loss: 0.2875\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0850 - val_loss: 0.3485\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0676 - val_loss: 0.2777\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0864 - val_loss: 0.2860\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0736 - val_loss: 0.3009\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0686 - val_loss: 0.2927\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0630 - val_loss: 0.4544\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0630 - val_loss: 0.4703\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0661 - val_loss: 0.3497\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0546 - val_loss: 0.2672\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0667 - val_loss: 0.2580\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0646 - val_loss: 0.2777\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0557 - val_loss: 0.3782\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0568 - val_loss: 0.2686\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0653 - val_loss: 0.2901\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0653 - val_loss: 0.3427\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0610 - val_loss: 0.2957\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0718 - val_loss: 0.3272\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0850 - val_loss: 0.4503\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0585 - val_loss: 0.3027\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0680 - val_loss: 0.2887\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0561 - val_loss: 0.2705\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0559 - val_loss: 0.4476\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0582 - val_loss: 0.2742\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0642 - val_loss: 0.2738\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0613 - val_loss: 0.2683\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0531 - val_loss: 0.2965\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0557 - val_loss: 0.2635\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0514 - val_loss: 0.3876\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0481 - val_loss: 0.2572\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0455 - val_loss: 0.2595\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0584 - val_loss: 0.2683\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0471 - val_loss: 0.3024\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0526 - val_loss: 0.2460\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0451 - val_loss: 0.2799\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0466 - val_loss: 0.2495\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0525 - val_loss: 0.2881\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0433 - val_loss: 0.2951\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0531 - val_loss: 0.2665\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0447 - val_loss: 0.2623\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0460 - val_loss: 0.2852\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0531 - val_loss: 0.2701\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0525 - val_loss: 0.2761\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0783 - val_loss: 0.2700\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0674 - val_loss: 0.3399\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0494 - val_loss: 0.2883\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0523 - val_loss: 0.2546\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0507 - val_loss: 0.2701\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0579 - val_loss: 0.3343\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0479 - val_loss: 0.2691\n",
      "Epoch 112/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0408 - val_loss: 0.2634\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0438 - val_loss: 0.2457\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0392 - val_loss: 0.2798\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0428 - val_loss: 0.2893\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0452 - val_loss: 0.2947\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0531 - val_loss: 0.2705\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0460 - val_loss: 0.3427\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0680 - val_loss: 0.2775\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0475 - val_loss: 0.3077\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0450 - val_loss: 0.2882\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0424 - val_loss: 0.2834\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0413 - val_loss: 0.2756\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0404 - val_loss: 0.2818\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0444 - val_loss: 0.3669\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0402 - val_loss: 0.2982\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0370 - val_loss: 0.2749\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0411 - val_loss: 0.2806\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0360 - val_loss: 0.2849\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0438 - val_loss: 0.3482\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0452 - val_loss: 0.3036\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0375 - val_loss: 0.2836\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0531 - val_loss: 0.4271\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0440 - val_loss: 0.2700\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0387 - val_loss: 0.2519\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0397 - val_loss: 0.2876\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0369 - val_loss: 0.2696\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0390 - val_loss: 0.2786\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0390 - val_loss: 0.2638\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0435 - val_loss: 0.3028\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0403 - val_loss: 0.2669\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0373 - val_loss: 0.2925\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0346 - val_loss: 0.2665\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0562 - val_loss: 0.2737\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0688 - val_loss: 0.3010\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0634 - val_loss: 0.3177\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0391 - val_loss: 0.3436\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0406 - val_loss: 0.2673\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0436 - val_loss: 0.2767\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0505 - val_loss: 0.2931\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0383 - val_loss: 0.2774\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0415 - val_loss: 0.2830\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0402 - val_loss: 0.3199\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0411 - val_loss: 0.2726\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0409 - val_loss: 0.2842\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0360 - val_loss: 0.2728\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0396 - val_loss: 0.3303\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0393 - val_loss: 0.2956\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0417 - val_loss: 0.2888\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0355 - val_loss: 0.2965\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0392 - val_loss: 0.2778\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0326 - val_loss: 0.2777\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0383 - val_loss: 0.2673\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0461 - val_loss: 0.2774\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0400 - val_loss: 0.5412\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0382 - val_loss: 0.2919\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0418 - val_loss: 0.2589\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0324 - val_loss: 0.3121\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0341 - val_loss: 0.2698\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0332 - val_loss: 0.3312\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0424 - val_loss: 0.2876\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0533 - val_loss: 0.3926\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0396 - val_loss: 0.2929\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0395 - val_loss: 0.2881\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0394 - val_loss: 0.2698\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0349 - val_loss: 0.2886\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0293 - val_loss: 0.3147\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0316 - val_loss: 0.2726\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0309 - val_loss: 0.3076\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0332 - val_loss: 0.2750\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0292 - val_loss: 0.2966\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0281 - val_loss: 0.2827\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0372 - val_loss: 0.2715\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0299 - val_loss: 0.2974\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0322 - val_loss: 0.2644\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0319 - val_loss: 0.2857\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0392 - val_loss: 0.2836\n",
      "Epoch 188/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0471 - val_loss: 0.2931\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0366 - val_loss: 0.3019\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0331 - val_loss: 0.2916\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0319 - val_loss: 0.2859\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0308 - val_loss: 0.2814\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0283 - val_loss: 0.2909\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0326 - val_loss: 0.2910\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0270 - val_loss: 0.2798\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0320 - val_loss: 0.2828\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0264 - val_loss: 0.2819\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0267 - val_loss: 0.2939\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0306 - val_loss: 0.2847\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0303 - val_loss: 0.2857\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0308 - val_loss: 0.2846\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0337 - val_loss: 0.2967\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0261 - val_loss: 0.2836\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0274 - val_loss: 0.2961\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0344 - val_loss: 0.2841\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0299 - val_loss: 0.2797\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0279 - val_loss: 0.3137\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0334 - val_loss: 0.2792\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0283 - val_loss: 0.2856\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0291 - val_loss: 0.3410\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0301 - val_loss: 0.3350\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0282 - val_loss: 0.2896\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0268 - val_loss: 0.3026\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0267 - val_loss: 0.2848\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0342 - val_loss: 0.2766\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0295 - val_loss: 0.2948\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0266 - val_loss: 0.2844\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0326 - val_loss: 0.3032\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0269 - val_loss: 0.3288\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0294 - val_loss: 0.2787\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0270 - val_loss: 0.2797\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0246 - val_loss: 0.2816\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0259 - val_loss: 0.2970\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0293 - val_loss: 0.3268\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0258 - val_loss: 0.2992\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0297 - val_loss: 0.2820\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0271 - val_loss: 0.2892\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0304 - val_loss: 0.2898\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0288 - val_loss: 0.2900\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0412 - val_loss: 0.3363\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0297 - val_loss: 0.4672\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0314 - val_loss: 0.3209\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0352 - val_loss: 0.2832\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0270 - val_loss: 0.3120\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0240 - val_loss: 0.3255\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0295 - val_loss: 0.3063\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0353 - val_loss: 0.4556\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0272 - val_loss: 0.3163\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0308 - val_loss: 0.3203\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0246 - val_loss: 0.2932\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0269 - val_loss: 0.3027\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0323 - val_loss: 0.3137\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0223 - val_loss: 0.2907\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0269 - val_loss: 0.3016\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0243 - val_loss: 0.3138\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0287 - val_loss: 0.3319\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0239 - val_loss: 0.3071\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0296 - val_loss: 0.3708\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0323 - val_loss: 0.3067\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0244 - val_loss: 0.3215\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 498us/step - loss: 7.8519 - val_loss: 7.5977\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 1.7727 - val_loss: 75.3294\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 4.4068 - val_loss: 2.8662\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 1.4778 - val_loss: 1.0459\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 3.7627 - val_loss: 1.8049\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 1.3610 - val_loss: 1.2588\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.7956 - val_loss: 0.7484\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.5809 - val_loss: 0.7303\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.5458 - val_loss: 0.9238\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.4403 - val_loss: 0.6363\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.4523 - val_loss: 0.9605\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.3405 - val_loss: 0.7583\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.3582 - val_loss: 0.7926\n",
      "Epoch 14/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2395 - val_loss: 0.5519\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.3697 - val_loss: 1.2323\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.2945 - val_loss: 0.4519\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2045 - val_loss: 0.5859\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2388 - val_loss: 0.8068\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.2872 - val_loss: 0.7665\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1975 - val_loss: 0.5213\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.2290 - val_loss: 0.4211\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1835 - val_loss: 0.4342\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1563 - val_loss: 0.3545\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1483 - val_loss: 0.3755\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1447 - val_loss: 0.3100\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1413 - val_loss: 0.3264\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1981 - val_loss: 0.3692\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1718 - val_loss: 0.4436\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1226 - val_loss: 0.3325\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1237 - val_loss: 0.3268\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1278 - val_loss: 0.3053\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1272 - val_loss: 0.3821\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1204 - val_loss: 0.3564\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1231 - val_loss: 0.3262\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1094 - val_loss: 0.2649\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1028 - val_loss: 0.7197\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1128 - val_loss: 0.3569\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1045 - val_loss: 0.2699\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.1011 - val_loss: 0.3043\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0900 - val_loss: 0.3062\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1056 - val_loss: 0.3022\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0941 - val_loss: 0.3470\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1126 - val_loss: 0.3422\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1099 - val_loss: 0.2466\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0938 - val_loss: 0.5581\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0899 - val_loss: 0.2481\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0847 - val_loss: 0.2341\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0846 - val_loss: 0.2720\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1031 - val_loss: 0.3283\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1082 - val_loss: 0.2522\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0945 - val_loss: 0.3339\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0821 - val_loss: 0.3100\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0805 - val_loss: 0.2644\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0846 - val_loss: 0.2378\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.1039 - val_loss: 0.2907\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1197 - val_loss: 0.2250\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0937 - val_loss: 0.2581\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0793 - val_loss: 0.2285\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0721 - val_loss: 0.2118\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0853 - val_loss: 0.2813\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0777 - val_loss: 0.2131\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0878 - val_loss: 0.2234\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0746 - val_loss: 0.2803\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0648 - val_loss: 0.2119\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0882 - val_loss: 0.2187\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0993 - val_loss: 0.2544\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0925 - val_loss: 0.2505\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0762 - val_loss: 0.2238\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0639 - val_loss: 0.1968\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0685 - val_loss: 0.2283\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0811 - val_loss: 0.2049\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0653 - val_loss: 0.3045\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0641 - val_loss: 0.1849\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0618 - val_loss: 0.3308\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0624 - val_loss: 0.1866\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0714 - val_loss: 0.2119\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0716 - val_loss: 0.2219\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0619 - val_loss: 0.1905\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0650 - val_loss: 0.1802\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0690 - val_loss: 0.1938\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0620 - val_loss: 0.3844\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0643 - val_loss: 0.2356\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0709 - val_loss: 0.2288\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0634 - val_loss: 0.2069\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0749 - val_loss: 0.2986\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0635 - val_loss: 0.2031\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0552 - val_loss: 0.2144\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0574 - val_loss: 0.2560\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0561 - val_loss: 0.2078\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0519 - val_loss: 0.2154\n",
      "Epoch 91/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0614 - val_loss: 0.7194\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0713 - val_loss: 0.1873\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0610 - val_loss: 0.2231\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0575 - val_loss: 0.2018\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0579 - val_loss: 0.2336\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0632 - val_loss: 0.2666\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0546 - val_loss: 0.2200\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0620 - val_loss: 0.2156\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0840 - val_loss: 0.2184\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1016 - val_loss: 0.2240\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0800 - val_loss: 0.2463\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0703 - val_loss: 0.2736\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0532 - val_loss: 0.2294\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0560 - val_loss: 0.2165\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0571 - val_loss: 0.3475\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0523 - val_loss: 0.1947\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0554 - val_loss: 0.2265\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0517 - val_loss: 0.2352\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0513 - val_loss: 0.2175\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0463 - val_loss: 0.2013\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0498 - val_loss: 0.2489\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0497 - val_loss: 0.2169\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0487 - val_loss: 0.2277\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0435 - val_loss: 0.1985\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0470 - val_loss: 0.2118\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0588 - val_loss: 0.2206\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0468 - val_loss: 0.2091\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0434 - val_loss: 0.2150\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0485 - val_loss: 0.1917\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0516 - val_loss: 0.2216\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0430 - val_loss: 0.1993\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0517 - val_loss: 0.2227\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0539 - val_loss: 0.2046\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0482 - val_loss: 0.1993\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0413 - val_loss: 0.2351\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0427 - val_loss: 0.2269\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0494 - val_loss: 0.2026\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0436 - val_loss: 0.2318\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0529 - val_loss: 0.2094\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0453 - val_loss: 0.1902\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0379 - val_loss: 0.1867\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0441 - val_loss: 0.2059\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0421 - val_loss: 0.2710\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0483 - val_loss: 0.3344\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0541 - val_loss: 0.2342\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0390 - val_loss: 0.2450\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0397 - val_loss: 0.2291\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0385 - val_loss: 0.2273\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0459 - val_loss: 0.2011\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0412 - val_loss: 0.2031\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0495 - val_loss: 0.2221\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0487 - val_loss: 0.2122\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0431 - val_loss: 0.2069\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0400 - val_loss: 0.2208\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0446 - val_loss: 0.2005\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0372 - val_loss: 0.2764\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0434 - val_loss: 0.2255\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0446 - val_loss: 0.2929\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0436 - val_loss: 0.3230\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0446 - val_loss: 0.2244\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0361 - val_loss: 0.2220\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0467 - val_loss: 0.2184\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0541 - val_loss: 0.2755\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0488 - val_loss: 0.2475\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0440 - val_loss: 0.2567\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0352 - val_loss: 0.2069\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0395 - val_loss: 0.2147\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0436 - val_loss: 0.3818\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0385 - val_loss: 0.2350\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0384 - val_loss: 0.2093\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0689 - val_loss: 0.2136\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0441 - val_loss: 0.2333\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0475 - val_loss: 0.2948\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0383 - val_loss: 0.2293\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0361 - val_loss: 0.2102\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0375 - val_loss: 0.1956\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0389 - val_loss: 0.2247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0377 - val_loss: 0.2499\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0589 - val_loss: 0.2809\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0376 - val_loss: 0.2831\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0356 - val_loss: 0.2289\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0387 - val_loss: 0.2094\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0380 - val_loss: 0.2387\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0572 - val_loss: 0.2078\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0381 - val_loss: 0.2203\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0359 - val_loss: 0.2139\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0665 - val_loss: 0.2412\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0371 - val_loss: 0.2152\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0337 - val_loss: 0.2130\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0369 - val_loss: 0.2054\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0341 - val_loss: 0.2156\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0327 - val_loss: 0.2125\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0345 - val_loss: 0.2148\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0343 - val_loss: 0.2163\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0335 - val_loss: 0.2119\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0377 - val_loss: 0.2251\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0378 - val_loss: 0.2229\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0517 - val_loss: 0.2242\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0497 - val_loss: 0.2285\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0315 - val_loss: 0.2391\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0330 - val_loss: 0.2115\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0306 - val_loss: 0.2961\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0334 - val_loss: 0.2120\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0344 - val_loss: 0.2218\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0344 - val_loss: 0.2207\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0336 - val_loss: 0.2150\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0339 - val_loss: 0.2136\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0358 - val_loss: 0.2171\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0399 - val_loss: 0.2381\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0395 - val_loss: 0.2125\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0371 - val_loss: 0.2438\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0338 - val_loss: 0.2244\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0310 - val_loss: 0.2155\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0321 - val_loss: 0.2315\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0364 - val_loss: 0.2185\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0357 - val_loss: 0.2163\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0336 - val_loss: 0.2400\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0340 - val_loss: 0.2352\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0300 - val_loss: 0.2299\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0327 - val_loss: 0.2297\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0354 - val_loss: 0.2426\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0333 - val_loss: 0.2271\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0317 - val_loss: 0.2523\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0312 - val_loss: 0.2431\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0318 - val_loss: 0.2428\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0282 - val_loss: 0.2269\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0319 - val_loss: 0.2179\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0276 - val_loss: 0.2229\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0319 - val_loss: 0.2313\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0362 - val_loss: 0.2280\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0372 - val_loss: 0.2332\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0289 - val_loss: 0.2253\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0311 - val_loss: 0.2299\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0332 - val_loss: 0.2494\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0461 - val_loss: 0.2543\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0622 - val_loss: 0.2795\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0360 - val_loss: 0.2365\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0274 - val_loss: 0.2242\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0289 - val_loss: 0.2556\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0267 - val_loss: 0.2350\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0312 - val_loss: 0.3053\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0358 - val_loss: 0.2340\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0311 - val_loss: 0.2670\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0720 - val_loss: 0.4736\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0381 - val_loss: 0.2670\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0543 - val_loss: 0.2757\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0342 - val_loss: 0.2358\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0424 - val_loss: 0.2529\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0361 - val_loss: 0.2442\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0303 - val_loss: 0.2304\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0346 - val_loss: 0.3103\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0286 - val_loss: 0.2642\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0333 - val_loss: 0.2255\n",
      "Epoch 244/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0324 - val_loss: 0.2413\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0303 - val_loss: 0.2469\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0313 - val_loss: 0.2671\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0277 - val_loss: 0.2229\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0343 - val_loss: 0.2319\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0381 - val_loss: 0.2297\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0298 - val_loss: 0.2310\n",
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 5s 495us/step - loss: 7.1203 - val_loss: 5.4761\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 2.5974 - val_loss: 2.3073\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 2.3377 - val_loss: 1.7196\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 1.8862 - val_loss: 1.1475\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 1.1217 - val_loss: 1.3095\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.9818 - val_loss: 1.3882\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.7096 - val_loss: 1.2326\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.6958 - val_loss: 0.7958\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.3830 - val_loss: 0.8990\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.3280 - val_loss: 0.6455\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.2825 - val_loss: 0.7382\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.3283 - val_loss: 0.6070\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.3885 - val_loss: 0.6108\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.4080 - val_loss: 0.6394\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2354 - val_loss: 0.6389\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2571 - val_loss: 0.6561\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.2365 - val_loss: 0.6740\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2396 - val_loss: 0.5298\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2310 - val_loss: 0.4679\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.2064 - val_loss: 0.4371\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.2121 - val_loss: 0.5846\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1846 - val_loss: 0.4581\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1811 - val_loss: 0.4817\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1551 - val_loss: 0.4264\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1798 - val_loss: 0.5532\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1283 - val_loss: 0.3968\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1649 - val_loss: 0.4085\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.2091 - val_loss: 0.4024\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1531 - val_loss: 0.5667\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.1910 - val_loss: 0.5814\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1336 - val_loss: 0.4488\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1633 - val_loss: 0.3764\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1693 - val_loss: 0.4167\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1363 - val_loss: 0.4096\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1357 - val_loss: 0.3634\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1420 - val_loss: 0.4950\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1438 - val_loss: 0.3899\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1221 - val_loss: 0.4622\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.1426 - val_loss: 0.4105\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1314 - val_loss: 0.4294\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1087 - val_loss: 0.3645\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0933 - val_loss: 0.8672\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0981 - val_loss: 0.4222\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1328 - val_loss: 0.8616\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1072 - val_loss: 0.4180\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.1005 - val_loss: 0.4659\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0995 - val_loss: 0.5282\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1157 - val_loss: 0.3898\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1273 - val_loss: 0.3395\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.1053 - val_loss: 0.3823\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0858 - val_loss: 0.3385\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0925 - val_loss: 0.3289\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0803 - val_loss: 0.3546\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0891 - val_loss: 0.3599\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0794 - val_loss: 0.3342\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0773 - val_loss: 0.3504\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 179us/step - loss: 0.0878 - val_loss: 0.3206\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0797 - val_loss: 0.3633\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.1095 - val_loss: 0.3801\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.1327 - val_loss: 0.3575\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.1086 - val_loss: 0.3519\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0813 - val_loss: 0.4746\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0715 - val_loss: 0.3612\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0698 - val_loss: 0.3197\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0753 - val_loss: 0.3348\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0662 - val_loss: 0.3594\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0725 - val_loss: 0.3429\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0707 - val_loss: 0.4292\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0602 - val_loss: 0.3358\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0559 - val_loss: 0.3453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0553 - val_loss: 0.3329\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0738 - val_loss: 0.3188\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0527 - val_loss: 0.3328\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0614 - val_loss: 0.3242\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0703 - val_loss: 0.3184\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0625 - val_loss: 0.3729\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0582 - val_loss: 0.3523\n",
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0604 - val_loss: 0.4248\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0605 - val_loss: 0.3116\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0943 - val_loss: 0.3768\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0595 - val_loss: 0.3994\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0720 - val_loss: 0.3884\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0569 - val_loss: 0.3391\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0574 - val_loss: 0.3304\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0623 - val_loss: 0.3221\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0682 - val_loss: 0.3329\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0621 - val_loss: 0.3438\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0595 - val_loss: 0.3471\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0542 - val_loss: 0.3650\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0633 - val_loss: 0.3675\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0479 - val_loss: 0.3134\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0537 - val_loss: 0.3311\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0507 - val_loss: 0.3167\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0533 - val_loss: 0.3867\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0664 - val_loss: 0.3436\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0531 - val_loss: 0.3526\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0522 - val_loss: 0.3369\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0828 - val_loss: 0.3079\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0509 - val_loss: 0.4517\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0530 - val_loss: 0.3290\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0509 - val_loss: 0.3465\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0491 - val_loss: 0.3871\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0983 - val_loss: 0.3184\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0583 - val_loss: 0.3265\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0525 - val_loss: 0.3667\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0504 - val_loss: 0.3158\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0509 - val_loss: 0.3394\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 173us/step - loss: 0.0501 - val_loss: 0.3237\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0486 - val_loss: 0.3247\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0450 - val_loss: 0.3318\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0465 - val_loss: 0.3472\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0481 - val_loss: 0.3204\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0516 - val_loss: 0.3357\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0469 - val_loss: 0.3365\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0489 - val_loss: 0.3248\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0437 - val_loss: 0.4977\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0492 - val_loss: 0.3594\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0579 - val_loss: 0.3555\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 174us/step - loss: 0.0558 - val_loss: 0.3324\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 2s 176us/step - loss: 0.0534 - val_loss: 0.3588\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 2s 177us/step - loss: 0.0416 - val_loss: 0.3524\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 2s 175us/step - loss: 0.0476 - val_loss: 0.4465\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0408 - val_loss: 0.3534\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0427 - val_loss: 0.3473\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0424 - val_loss: 0.3577\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0376 - val_loss: 0.3584\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0440 - val_loss: 0.3407\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0439 - val_loss: 0.3470\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0414 - val_loss: 0.3704\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0370 - val_loss: 0.3558\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0422 - val_loss: 0.3417\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0457 - val_loss: 0.4751\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.1030 - val_loss: 0.3568\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0422 - val_loss: 0.3617\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0532 - val_loss: 0.3409\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0466 - val_loss: 0.3718\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0379 - val_loss: 0.3337\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0407 - val_loss: 0.3527\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0488 - val_loss: 0.3701\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0580 - val_loss: 0.3679\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0339 - val_loss: 0.3642\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0363 - val_loss: 0.3425\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0409 - val_loss: 0.3492\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0326 - val_loss: 0.3489\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0457 - val_loss: 0.3625\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0686 - val_loss: 0.3880\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0432 - val_loss: 0.3670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0418 - val_loss: 0.3813\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0343 - val_loss: 0.3566\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0405 - val_loss: 0.3579\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0481 - val_loss: 0.3461\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0310 - val_loss: 0.3625\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0365 - val_loss: 0.3542\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0363 - val_loss: 0.3681\n",
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0353 - val_loss: 0.3563\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0381 - val_loss: 0.3791\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0356 - val_loss: 0.3892\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 172us/step - loss: 0.0437 - val_loss: 0.3537\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0325 - val_loss: 0.3659\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0347 - val_loss: 0.3582\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0467 - val_loss: 0.3957\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0392 - val_loss: 0.3990\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0401 - val_loss: 0.3747\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0362 - val_loss: 0.4183\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0367 - val_loss: 0.3823\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0382 - val_loss: 0.3875\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0375 - val_loss: 0.3953\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0402 - val_loss: 0.3667\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0325 - val_loss: 0.3494\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0369 - val_loss: 0.4401\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0453 - val_loss: 0.3789\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0565 - val_loss: 0.3665\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0462 - val_loss: 0.3737\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0330 - val_loss: 0.3759\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0306 - val_loss: 0.3837\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0441 - val_loss: 0.3641\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0345 - val_loss: 0.3538\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0334 - val_loss: 0.3593\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0278 - val_loss: 0.3713\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0334 - val_loss: 0.3485\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0345 - val_loss: 0.5050\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0322 - val_loss: 0.3567\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0313 - val_loss: 0.3677\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0291 - val_loss: 0.3793\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0317 - val_loss: 0.3593\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0363 - val_loss: 0.3686\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0301 - val_loss: 0.3994\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0384 - val_loss: 0.3584\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0347 - val_loss: 0.4062\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0300 - val_loss: 0.4073\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0329 - val_loss: 0.3737\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0331 - val_loss: 0.3677\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0311 - val_loss: 0.3676\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0274 - val_loss: 0.3653\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0359 - val_loss: 0.3709\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0380 - val_loss: 0.3754\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0316 - val_loss: 0.3717\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0332 - val_loss: 0.3708\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0447 - val_loss: 0.3792\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0323 - val_loss: 0.3652\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0323 - val_loss: 0.3675\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0262 - val_loss: 0.3834\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0289 - val_loss: 0.3695\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0291 - val_loss: 0.3894\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0280 - val_loss: 0.3732\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0286 - val_loss: 0.3790\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0254 - val_loss: 0.3727\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0259 - val_loss: 0.3735\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0311 - val_loss: 0.3783\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0344 - val_loss: 0.3669\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0341 - val_loss: 0.4382\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0300 - val_loss: 0.3947\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0312 - val_loss: 0.3599\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0305 - val_loss: 0.3888\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0304 - val_loss: 0.3705\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0301 - val_loss: 0.3873\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0308 - val_loss: 0.3889\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0241 - val_loss: 0.3803\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 171us/step - loss: 0.0289 - val_loss: 0.3840\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0329 - val_loss: 0.4052\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0279 - val_loss: 0.3895\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0372 - val_loss: 0.4380\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 170us/step - loss: 0.0286 - val_loss: 0.3724\n",
      "Epoch 224/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0266 - val_loss: 0.3830\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0298 - val_loss: 0.4333\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0312 - val_loss: 0.3862\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0306 - val_loss: 0.3865\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0306 - val_loss: 0.3784\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 164us/step - loss: 0.0282 - val_loss: 0.4061\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0254 - val_loss: 0.4117\n",
      "Epoch 231/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0286 - val_loss: 0.3839\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0238 - val_loss: 0.3826\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0292 - val_loss: 0.3966\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0270 - val_loss: 0.3866\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0241 - val_loss: 0.3996\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0286 - val_loss: 0.3968\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0266 - val_loss: 0.3865\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0294 - val_loss: 0.3972\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0270 - val_loss: 0.4088\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0285 - val_loss: 0.4002\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0252 - val_loss: 0.4020\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0258 - val_loss: 0.4475\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 2s 165us/step - loss: 0.0317 - val_loss: 0.4239\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 168us/step - loss: 0.0319 - val_loss: 0.3915\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0264 - val_loss: 0.4195\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0252 - val_loss: 0.4247\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 167us/step - loss: 0.0310 - val_loss: 0.4620\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 169us/step - loss: 0.0700 - val_loss: 0.3937\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0373 - val_loss: 0.5021\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 2s 166us/step - loss: 0.0757 - val_loss: 0.4111\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1440 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr=0.002;\n",
    "n_lr = 20;\n",
    "learn_rate = np.linspace(1/n_lr,1,n_lr)*lr; #20 puntos de (0,1]\n",
    "j=0;\n",
    "for i in learn_rate:\n",
    "    j+=1;\n",
    "    plt.subplot(5,n_lr/5,j);\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=SGD(lr=i),loss='mean_squared_error')\n",
    "    history_relu = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "    plt.title('lr='+str(i));\n",
    "    plt.plot(history_relu.history['loss'],label='train');\n",
    "    plt.plot(history_relu.history['val_loss'],label='validation');\n",
    "    plt.ylim([0,2])\n",
    "\n",
    "plt.show();\n",
    "plt.tight_layout();\n",
    "plt.rcParams[\"figure.figsize\"]=[15,20];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "> e) Entrene los modelos considerados en b) y c) usando *progressive decay*. Compare y comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "GPU sync failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: GPU sync failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f030d362ef98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 199\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: GPU sync failed"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_decay = 10;\n",
    "lear_decay = np.logspace(-6,0,n_decay);\n",
    "j=0;\n",
    "for i in lear_decay:\n",
    "    sgd = SGD(lr=0.02, decay=i);\n",
    "    j+=1;\n",
    "    plt.subplot(2,n_decay/5,j);\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "    history = model.fit(X_train_scaled, y_train, epochs=250, verbose=1, validation_data=(X_val_scaled, y_val))\n",
    "    plt.plot(history.history['loss'],label='train');\n",
    "    plt.plot(history.history['val_loss'],label='validation');\n",
    "    plt.ylabel('Loss');\n",
    "    plt.xlabel('Epoch');\n",
    "    plt.ylim([0,2]);\n",
    "    \n",
    "plt.show();\n",
    "plt.tight_layout();\n",
    "plt.rcParams[\"figure.figsize\"]=[15,20];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "j=0;\n",
    "for i in lear_decay:\n",
    "    sgd = SGD(lr=0.002, decay=i);   \n",
    "    j+=1;\n",
    "    plt.subplot(2,n_decay/5,j);\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "    history_relu = model.fit(X_train_scaled, y_train, epochs=250, verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "    plt.plot(history_relu.history['loss'],label='train');\n",
    "    plt.plot(history_relu.history['val_loss'],label='validation');\n",
    "    plt.ylabel('Loss');\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylim([0,2]);\n",
    "    \n",
    "plt.show();\n",
    "plt.tight_layout();\n",
    "plt.rcParams[\"figure.figsize\"]=[15,20];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "> f) Entrene los modelos considerados en b) y c) utilizando SGD en mini-*batches*. Experimente con diferentes tamaños del *batch*. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "n_batches = 21\n",
    "batch_sizes = np.round(np.linspace(1,X_train_scaled.shape[0],n_batches))\n",
    "#model.fit(X_train_scaled,y_train,batch_size=50,epochs=250,validation_data=(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "j=0;\n",
    "for i in batch_sizes:\n",
    "    sgd = SGD(lr=0.01/n_batches); # al parecer no normalizan el minibatch por lo que diverge facilmente\n",
    "    j+=1;\n",
    "    plt.subplot(3,n_batches/7,j);\n",
    "    #plt.subplot(1,n_batches,j);\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "    history = model.fit(X_train_scaled, y_train,batch_size=int(i), epochs=10,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "    plt.plot(history.history['loss'],label='train');\n",
    "    plt.plot(history.history['val_loss'],label='validation');\n",
    "    plt.ylabel('Loss');\n",
    "    plt.xlabel('Epoch');\n",
    "    plt.ylim([0,5]);\n",
    "    \n",
    "plt.show();\n",
    "plt.tight_layout();\n",
    "plt.rcParams[\"figure.figsize\"]=[15,20];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j=0;\n",
    "for i in batch_sizes:\n",
    "    sgd = SGD(lr=0.001/n_batches);\n",
    "    j+=1;\n",
    "    plt.subplot(3,n_batches/7,j);\n",
    "    #plt.subplot(1,n_batches,j);\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "    history_relu = model.fit(X_train_scaled, y_train,batch_size=int(i), epochs=10,verbose=0, validation_data=(X_val_scaled, y_val))\n",
    "    plt.plot(history_relu.history['loss'],label='train');\n",
    "    plt.plot(history_relu.history['val_loss'],label='validation');\n",
    "    plt.ylabel('Loss');\n",
    "    plt.xlabel('Epoch');\n",
    "    plt.ylim([0,5]);\n",
    "    \n",
    "plt.show();\n",
    "plt.tight_layout();\n",
    "plt.rcParams[\"figure.figsize\"]=[15,20];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "> g) Entrene los modelos obtenidos en b) y c) utilizando estrategias modernas para adaptar la tasa de aprendizaje. Compare los desempeños de adagrad, adadelta, RMSprop y adam. ¿Se observa en algún caso un mejor resultado final? ¿Se observa en algún caso una mayor velocidad de convergencia sobre el dataset de entrenamiento? ¿Sobre el dataset de validación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "optimizers = [SGD(lr=0.01),Adagrad(lr=0.01),Adadelta(lr=0.01),RMSprop(lr=0.01),Adam(lr=0.01)];\n",
    "optimizers_str=['sgd','Adagrad','Adadelta','RMSprop','Adam'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    sgd = optimizers[i];\n",
    "    plt.subplot(2,3,i+1);\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "    history = model.fit(X_train_scaled, y_train, epochs=250,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "    plt.plot(history.history['loss'],label='train');\n",
    "    plt.plot(history.history['val_loss'],label='validation');\n",
    "    plt.title(optimizers_str[i]);\n",
    "    plt.ylabel('Loss');\n",
    "    plt.xlabel('Epoch');\n",
    "    plt.ylim([0,2]);\n",
    "\n",
    "plt.show();\n",
    "plt.tight_layout();\n",
    "plt.rcParams[\"figure.figsize\"]=[15,20];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizers = [SGD(lr=0.001),Adagrad(lr=0.001),Adadelta(lr=0.001),RMSprop(lr=0.001),Adam(lr=0.001)];\n",
    "for i in range(5):\n",
    "    sgd = optimizers[i];\n",
    "    plt.subplot(2,3,i+1);\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "    history = model.fit(X_train_scaled, y_train, epochs=10,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "    plt.plot(history.history['loss'],label='train');\n",
    "    plt.plot(history.history['val_loss'],label='validation');\n",
    "    plt.title(optimizers_str[i]);\n",
    "    plt.ylabel('Loss');\n",
    "    plt.xlabel('Epoch');\n",
    "    plt.ylim([0,5]);\n",
    "\n",
    "plt.show();\n",
    "plt.tight_layout();\n",
    "plt.rcParams[\"figure.figsize\"]=[15,20];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que con los esquemas avanzados de learning rate no se tiene problemas de convergencia por el learning rate elegido como si sucedia con sgd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "> h) Entrene los modelos obtenidos en b) y c) utilizando regularizadores $l_1$ y $l_2$ (*weight decay*). Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente. Además evalúe el efecto de regularizar solo la primera capa *vs* la segunda, comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1,l2\n",
    "e=250;\n",
    "\n",
    "plt.subplot(1,3,1);\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=e,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "plt.plot(history.history['loss'],label='train');\n",
    "plt.plot(history.history['val_loss'],label='validation');\n",
    "plt.title('none');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylim([0,2]);\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2);\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',W_regularizer=l1(0.01)\n",
    "                ,activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',W_regularizer=l1(0.01),activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=e,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "plt.plot(history.history['loss'],label='train');\n",
    "plt.plot(history.history['val_loss'],label='validation');\n",
    "plt.title('l1');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylim([0,2]);\n",
    "\n",
    "\n",
    "plt.subplot(1,3,3);\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',W_regularizer=l2(0.01)\n",
    "                ,activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',W_regularizer=l2(0.01),activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=e,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "plt.plot(history.history['loss'],label='train');\n",
    "plt.plot(history.history['val_loss'],label='validation');\n",
    "plt.title('l2');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylim([0,2]);\n",
    "\n",
    "plt.show();\n",
    "plt.tight_layout();\n",
    "plt.rcParams[\"figure.figsize\"]=[15,20];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=250;\n",
    "\n",
    "plt.subplot(1,3,1);\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=e,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "plt.plot(history.history['loss'],label='train');\n",
    "plt.plot(history.history['val_loss'],label='validation');\n",
    "plt.title('none');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylim([0,2]);\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2);\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',W_regularizer=l1(0.01)\n",
    "                ,activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',W_regularizer=l1(0.01),activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=e,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "plt.plot(history.history['loss'],label='train');\n",
    "plt.plot(history.history['val_loss'],label='validation');\n",
    "plt.title('l1');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylim([0,2]);\n",
    "\n",
    "\n",
    "plt.subplot(1,3,3);\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',W_regularizer=l2(0.01)\n",
    "                ,activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',W_regularizer=l2(0.01),activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=e,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "plt.plot(history.history['loss'],label='train');\n",
    "plt.plot(history.history['val_loss'],label='validation');\n",
    "plt.title('l2');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylim([0,2]);\n",
    "\n",
    "plt.show();\n",
    "plt.tight_layout();\n",
    "plt.rcParams[\"figure.figsize\"]=[15,20];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "> i) Entrene los modelos obtenidos en b) y c) utilizando *Dropout*. Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "e=250;\n",
    "\n",
    "plt.subplot(2,2,1);\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=e,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "plt.plot(history.history['loss'],label='train');\n",
    "plt.plot(history.history['val_loss'],label='validation');\n",
    "plt.title('none');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylim([0,2]);\n",
    "\n",
    "\n",
    "plt.subplot(2,2,2);\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=e,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "plt.plot(history.history['loss'],label='train');\n",
    "plt.plot(history.history['val_loss'],label='validation');\n",
    "plt.title('Dropout(0.1)');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylim([0,2]);\n",
    "\n",
    "plt.subplot(2,2,3);\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=e,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "plt.plot(history.history['loss'],label='train');\n",
    "plt.plot(history.history['val_loss'],label='validation');\n",
    "plt.title('Dropout(0.2)');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylim([0,2]);\n",
    "\n",
    "plt.subplot(2,2,4);\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(X_train_scaled, y_train, epochs=e,verbose=2, validation_data=(X_val_scaled, y_val))\n",
    "plt.plot(history.history['loss'],label='train');\n",
    "plt.plot(history.history['val_loss'],label='validation');\n",
    "plt.title('Dropout(0.3)');\n",
    "plt.ylabel('Loss');\n",
    "plt.xlabel('Epoch');\n",
    "plt.ylim([0,2]);\n",
    "\n",
    "plt.show();\n",
    "plt.tight_layout();\n",
    "plt.rcParams[\"figure.figsize\"]=[15,20];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "#from keras.layers import Dropout\n",
    "#model = Sequential()\n",
    "#...\n",
    "#model.add(Dense(256,kernel_initializer='uniform'))\n",
    "#model.add(Activation('sigmoid'))\n",
    "#model.add(Dropout(0.2))\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "> j) Fijando todos los demás hiper-parámetros del modelo definido en b) y en c), utilice validación cruzada con un número de *folds* igual a *K* = 5 y *K*=10 para determinar el mejor valor correspondiente a un parámetro que usted elija (tasa de aprendizaje, número de neuronas, parámetro de regularización, etc) ¿El mejor parámetro para la red con sigmoidal es distinto que para ReLU? ¿Porqué sucede? Además mida el error real del modelo sobre el conjunto de pruebas, compare y concluya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "Xm = X_train_scaled.values\n",
    "ym = y_train\n",
    "kfold = cross_validation.KFold(len(Xm), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=Xm[train].shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "    model.fit(Xm[train], ym[train], epochs=250)\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    print(scores)\n",
    "    cvscores.append(scores)\n",
    "mse_cv = np.mean(cvscores)\n",
    "print('mean=' + str(mse_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=Xm[train].shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "    model.fit(Xm[train], ym[train], epochs=250)\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    print(scores)\n",
    "    cvscores.append(scores)\n",
    "mse_cv = np.mean(cvscores)\n",
    "print('mean=' + str(mse_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=Xm[train].shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "    model.fit(Xm[train], ym[train], epochs=250)\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    print(scores)\n",
    "    cvscores.append(scores)\n",
    "mse_cv = np.mean(cvscores)\n",
    "print('mean=' + str(mse_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=Xm[train].shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "    model.fit(Xm[train], ym[train], epochs=250)\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    print(scores)\n",
    "    cvscores.append(scores)\n",
    "mse_cv = np.mean(cvscores)\n",
    "print('mean=' + str(mse_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=Xm[train].shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "    model.fit(Xm[train], ym[train], epochs=250)\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    print(scores)\n",
    "    cvscores.append(scores)\n",
    "mse_cv = np.mean(cvscores)\n",
    "print('mean=' + str(mse_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=Xm[train].shape[1], kernel_initializer='uniform',activation=\"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "    model.compile(optimizer=SGD(lr=0.001),loss='mean_squared_error')\n",
    "    model.fit(Xm[train], ym[train], epochs=250)\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    print(scores)\n",
    "    cvscores.append(scores)\n",
    "mse_cv = np.mean(cvscores)\n",
    "print('mean=' + str(mse_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFtCUBp9Nqhb"
   },
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Deep Networks\n",
    "Las *deep network*, o lo que hoy en día se conoce como *deep learning*, hace referencia a modelos de redes neuronales estructurados con muchas capas, es decir, el cómputo de la función final es la composición una gran cantidad de funciones ( $f^{(n)} = f^{(n-1)} \\circ f^{(n-2)} \\circ \\cdots \\circ f^{(2)} \\circ f^{(1)} $ con $n \\gg 0$ ).  \n",
    "Este tipo de redes neuronales tienen una gran cantidad de parámetros, creciendo exponencialmente por capa con las redes *feed forward*, siendo bastante dificiles de entrenar comparadas con una red poco profunda, esto es debido a que requieren una gran cantidad de datos para ajustar correctamente todos esos parámetros. Pero entonces ¿Cuál es el beneficio que tienen este tipo de redes? ¿Qué ganancias trae el añadir capas a una arquitectura de una red neuronal?  \n",
    "\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz36.png\" title=\"Title text\" width=\"80%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "\n",
    "En esta sección se estudiará la complejidad de entrenar redes neuronales profundas, mediante la visualización de los gradientes de los pesos en cada capa, el cómo varía mientras se hace el *backpropagation* hacia las primeras capas de la red. \n",
    "\n",
    "> a) Se trabajará con las etiquetas escaladas uniformemente, es decir, $\\mu=0$ y $\\sigma=1$, ajuste sobre el conjunto de entrenamiento y transforme éstas además de las de validación y pruebas.\n",
    "```python\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "y_train_scaled = X_train_scaled.pop('Eat').values.reshape(-1,1)\n",
    "...#transform val and test\n",
    "```\n",
    "\n",
    "> b) Para el mismo problema definido anteriormente ([sección 1](#primero)) se entrenarán diferentes redes. En esta primera instancia se trabajará con la misma red de la pregunta b), inicializada con pesos uniforme. Visualice el gradiente de la función de pérdida (*loss*) para el conjunto de entrenamiento (promedio del gradiente de cada dato) respecto a los pesos en las distintas capas, para esto se le pedirá el cálculo del gradiente para una capa mediante la función de *gradients* (__[link](https://www.tensorflow.org/api_docs/python/tf/keras/backend/gradients)__) en el *backend* de Keras. Deberá generar un **histograma** para todos los pesos de cada capa antes y despues del entrenamiento con 250 *epochs*. Comente.\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "- ###calculate gradients\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "loss = keras.losses.mean_squared_error(model.output,y_train_scaled)\n",
    "listOfVariableTensors = model.trainable_weights \n",
    "gradients = K.gradients(loss, listOfVariableTensors) #We can now calculate the gradients.\n",
    "sess = K.get_session()\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:X_train_scaled.values})\n",
    "evaluated_gradients = [gradient/len(y_train) for gradient in evaluated_gradients]\n",
    "```\n",
    "\n",
    "> c) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento pero ahora entrenando una red mucho más profunda de 6 capas, 5 capas escondidas y 1 de salida. Utilice el inicializador de pesos *uniform* el cual inicializa mediante una distribución uniforme entre $-1/\\sqrt{N}$ y $1/\\sqrt{N}$ para cada capa, con $N$ el número de neuronas de la capa anterior. Por simplicidad visualice las 3-4 primeras capas de la red. Comente si observa el efecto del *gradiente desvaneciente* antes y/o después de entrenar.\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256,  kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "> d) Vuelva a generar los histogramas para los gradientes de los pesos de cada capa antes y después del entrenamiento, pero ahora entrenando la red profunda con el inicializador de Glorot [[1]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/(N_{in}+N_{out})}$  y $\\sqrt{6/(N_{in}+N_{out})}$ . Por simplicidad visualice las 3-4 primeras capas de la red. Comente si el efecto del *gradiente desvaneciente* se amortigua antes y/o después de entrenar.\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256,  kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(256, kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='glorot_uniform',activation='linear'))\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "> e) Vuelva a repetir la experimentación ahora cambiando la función de activación por ReLU, es decir, deberá visualizar los gradientes de los pesos de cada capa antes y después del entrenamiento, con inicialización *uniform* y comparar con la inicialización de He [[2]](#refs), es decir, una distribución uniforme entre -$\\sqrt{6/N_{in}}$ y $\\sqrt{6/N_{in}} $. Comente si ocurre el mismo fenómeno anterior (para función sigmoidal) sobre el efecto del *gradiente desvaneciente* para la función ReLU. Explique la importancia de la inicialización de los pesos dependiendo de la arquitectura.\n",
    "```python\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='uniform',activation='relu')) #uniform\n",
    "...\n",
    "or\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='he_uniform',activation='relu')) #he\n",
    "...\n",
    "```\n",
    "> f) ¿Qué es lo que sucede con la red más profunda? ¿El modelo logra convergencia en su entrenamiento? Modifique aspectos estructurales (funciones de activación, inicializadores, regularización, *momentum*, variación de tasa de aprendizaje, entre otros) de la red profunda de 6 capas definida anteriormente (no modifique la profundidad ni el número de neuronas) para lograr un error cuadrático medio (*mse*) similar o menor al de una red no profunda, como la definida en b) en esta sección, sobre el conjunto de pruebas.\n",
    "\n",
    "> g) Experimente con la utilización de una función activación auxiliar (debido a que aproxima) a '**ReLU**' y que es continua derivable (**softplus**) ¿Cuál es el beneficio de ésta con respecto ReLU? Comente.\n",
    "```python\n",
    "...\n",
    "model.add(Dense(nh, kernel_initializer='he_uniform',activation='softplus')) #softplus\n",
    "...\n",
    "```\n",
    "\n",
    "> h) Pruebe con utilizar una red *shallow* (poco profunda), es decir, sitúe todas las neuronas en una única capa ¿Qué sucede con la convergencia del algoritmo? ¿Por qué sucede este fenómeno?\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=X_train_scaled.shape[1], kernel_initializer='choose',activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='choose',activation='linear'))\n",
    "model.compile(optimizer=sgd,loss='mean_squared_error')\n",
    "model.fit(X_train_scaled.values, y_train_scaled, epochs=250, verbose=1, validation_data=(X_val_scaled.values, y_val_scaled))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLGc9XcDNqhi"
   },
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Entendimiento de imágenes de personas\n",
    "\n",
    "El problema de inferir ciertas características de una persona a través de una foto de ella puede resultar bastante dificil incluso para nosotros, como por ejemplo de qué país es, la emoción que expresa, la edad que tiene, o el género. La automatización de este proceso para que máquinas logren identificar ciertas características de una persona puede ser algo crucial para el futuro desarrollo de Inteligencia Artificial.\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/6B072GE.jpg\" width=\"60%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "En esta actividad trabajaremos con unos datos (imágenes) con la tarea de predecir la **edad** (*target value*) de la persona en la imagen. Los datos con corresponden a 3640 imágenes de Flickr de rostros de personas, pero, debido a que trabajamos con redes *feed forward*, se trabajará con representaciones de características extraídas. Para ésto necesitará descargar los datos del siguiente __[link](http://chenlab.ece.cornell.edu/people/Andy/ImagesOfGroups.html)__ en el extracto de *ageGenderClassification* o a través de la consola Unix.\n",
    "```\n",
    "wget http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/ageGenderClassification.zip\n",
    "```\n",
    "\n",
    "Se trabajará con archivos *.mat* que pueden ser cargados de la siguiente manera:\n",
    "```python\n",
    "import scipy.io as sio\n",
    "sio.loadmat(\"file.mat\")\n",
    "```\n",
    "\n",
    "Para descripción sobre las columnas están en el archivo readme a través del siguiente __[link](http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/README.txt)__ o a través de la consola Unix:\n",
    "```\n",
    "wget http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/README.txt\n",
    "```\n",
    "\n",
    "\n",
    "> a) Cargue los datos dos dataset de entrenamiento y de pruebas ¿Cuántos datos hay en cada conjunto?\n",
    "```python\n",
    "import scipy.io as sio\n",
    "mat_train = sio.loadmat(\"./eventrain.mat\")\n",
    "mat_test = sio.loadmat(\"./eventest.mat\")\n",
    "data_train= mat_train[\"trcoll\"][0][0]\n",
    "data_test= mat_test[\"tecoll\"][0][0]\n",
    "```\n",
    "\n",
    "> b) Eliga cuál representación utilizará para trabajar los datos y entregárselos como *input* al modelo neuronal denso. Además extraiga las etiquetas del problema. Describa los datos utilziados.\n",
    "```python\n",
    "genFeat = data[0]  #it can be used as representation: contextual features\n",
    "ageClass = data[1] #target\n",
    "ffcoefs = data[3]   #it can be used as representation: fisherface space\n",
    "faceGist = data[4]  #it can be used as representation\n",
    "```\n",
    "\n",
    "> c) Defina y entrene una modelo de red neuronal *feed forward* para la inferencia de la edad de la persona a través de la representación escogida. Intente llegar a un *mse* menor a 100 en el conjunto de pruebas. Recuerde que **NO** puede seleccionar modelos a través del conjunto de pruebas. Visualice sus resultados si estima conveniente.\n",
    "\n",
    "\n",
    "*Nota: Puede notar que la cantidad de edades presentes en el problema son pocas (1,  5, 10, 16, 28, 51 o 75 años), por lo que puede tratar al problema así como de regresión o clasificación (considerando cada edad como una clase)*\n",
    "\n",
    "\n",
    "#### Ayuda:\n",
    "> Para problemas de clasificación de múltiples clases es necesario transformar las etiquetas categóricas en *one hot vector*, donde cada columna del vector representará una categoría. Por ejemplo, si existen tres categorías (perro, gato, ratón), la categoría perro puede ser codificada como [1,0,0], y la categoría ratón puede ser codificada como [0,0,1]. Para ésto la librería *keras* nos ayuda:\n",
    "\n",
    "```python\n",
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train,num_classes=edades_distintas)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YAYlvfT-Nqhw"
   },
   "source": [
    "<a id=\"refs\"></a>\n",
    "## Referencias\n",
    "[1] Glorot, X., & Bengio, Y. (2010, March). *Understanding the difficulty of training deep feedforward neural networks*. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256).    \n",
    "[2]  He, K., Zhang, X., Ren, S., & Sun, J. (2015). *Delving deep into rectifiers: Surpassing human-level performance on imagenet classification*. In Proceedings of the IEEE international conference on computer vision (pp. 1026-1034).  \n",
    "[3] Gallagher, A. C., & Chen, T. (2009, June). *Understanding images of groups of people*. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (pp. 256-263). IEEE."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Enunciado.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
